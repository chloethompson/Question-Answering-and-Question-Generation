{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#download model \n",
    "#!python download.py 345M"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: dataclasses in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (0.8)\n",
      "Collecting datasets\n",
      "  Downloading datasets-1.4.1-py3-none-any.whl (186 kB)\n",
      "\u001b[K     |████████████████████████████████| 186 kB 16.1 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: requests>=2.19.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from datasets) (2.25.1)\n",
      "Requirement already satisfied: numpy>=1.17 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from datasets) (1.19.5)\n",
      "Requirement already satisfied: importlib-metadata in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from datasets) (3.7.0)\n",
      "Collecting xxhash\n",
      "  Downloading xxhash-2.0.0-cp36-cp36m-manylinux2010_x86_64.whl (242 kB)\n",
      "\u001b[K     |████████████████████████████████| 242 kB 46.5 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting multiprocess\n",
      "  Downloading multiprocess-0.70.11.1-py36-none-any.whl (101 kB)\n",
      "\u001b[K     |████████████████████████████████| 101 kB 17.7 MB/s ta 0:00:01\n",
      "\u001b[?25hCollecting tqdm<4.50.0,>=4.27\n",
      "  Downloading tqdm-4.49.0-py2.py3-none-any.whl (69 kB)\n",
      "\u001b[K     |████████████████████████████████| 69 kB 1.8 MB/s  eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: pyarrow>=0.17.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from datasets) (3.0.0)\n",
      "Requirement already satisfied: dill in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from datasets) (0.3.3)\n",
      "Requirement already satisfied: dataclasses in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from datasets) (0.8)\n",
      "Requirement already satisfied: pandas in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from datasets) (1.1.5)\n",
      "Requirement already satisfied: fsspec in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from datasets) (0.8.7)\n",
      "Collecting huggingface-hub==0.0.2\n",
      "  Downloading huggingface_hub-0.0.2-py3-none-any.whl (24 kB)\n",
      "Requirement already satisfied: filelock in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from huggingface-hub==0.0.2->datasets) (3.0.12)\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from requests>=2.19.0->datasets) (3.0.4)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from requests>=2.19.0->datasets) (2.10)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from requests>=2.19.0->datasets) (1.26.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from requests>=2.19.0->datasets) (2020.12.5)\n",
      "Requirement already satisfied: zipp>=0.5 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from importlib-metadata->datasets) (3.4.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.4 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from importlib-metadata->datasets) (3.7.4.3)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from pandas->datasets) (2.8.1)\n",
      "Requirement already satisfied: pytz>=2017.2 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from pandas->datasets) (2021.1)\n",
      "Requirement already satisfied: six>=1.5 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from python-dateutil>=2.7.3->pandas->datasets) (1.15.0)\n",
      "Installing collected packages: tqdm, xxhash, multiprocess, huggingface-hub, datasets\n",
      "Successfully installed datasets-1.4.1 huggingface-hub-0.0.2 multiprocess-0.70.11.1 tqdm-4.49.0 xxhash-2.0.0\n",
      "Collecting tensorflow-gpu==1.15.2\n",
      "  Downloading tensorflow_gpu-1.15.2-cp36-cp36m-manylinux2010_x86_64.whl (411.0 MB)\n",
      "\u001b[K     |████████████████████████████████| 411.0 MB 20 kB/s s eta 0:00:01     |████████                        | 102.2 MB 69.1 MB/s eta 0:00:05     |████████████▋                   | 162.4 MB 66.4 MB/s eta 0:00:04\n",
      "\u001b[?25hRequirement already satisfied: numpy<2.0,>=1.16.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from tensorflow-gpu==1.15.2) (1.19.5)\n",
      "Collecting keras-applications>=1.0.8\n",
      "  Downloading Keras_Applications-1.0.8-py3-none-any.whl (50 kB)\n",
      "\u001b[K     |████████████████████████████████| 50 kB 1.4 MB/s  eta 0:00:01\n",
      "\u001b[?25hCollecting opt-einsum>=2.3.2\n",
      "  Downloading opt_einsum-3.3.0-py3-none-any.whl (65 kB)\n",
      "\u001b[K     |████████████████████████████████| 65 kB 795 kB/s  eta 0:00:01\n",
      "\u001b[?25hCollecting grpcio>=1.8.6\n",
      "  Downloading grpcio-1.36.1-cp36-cp36m-manylinux2014_x86_64.whl (4.1 MB)\n",
      "\u001b[K     |████████████████████████████████| 4.1 MB 70.2 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: wheel>=0.26 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from tensorflow-gpu==1.15.2) (0.36.2)\n",
      "Requirement already satisfied: google-pasta>=0.1.6 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from tensorflow-gpu==1.15.2) (0.2.0)\n",
      "Collecting astor>=0.6.0\n",
      "  Downloading astor-0.8.1-py2.py3-none-any.whl (27 kB)\n",
      "Requirement already satisfied: protobuf>=3.6.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from tensorflow-gpu==1.15.2) (3.15.2)\n",
      "Requirement already satisfied: six>=1.10.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from tensorflow-gpu==1.15.2) (1.15.0)\n",
      "Collecting gast==0.2.2\n",
      "  Downloading gast-0.2.2.tar.gz (10 kB)\n",
      "Collecting keras-preprocessing>=1.0.5\n",
      "  Downloading Keras_Preprocessing-1.1.2-py2.py3-none-any.whl (42 kB)\n",
      "\u001b[K     |████████████████████████████████| 42 kB 2.0 MB/s  eta 0:00:01\n",
      "\u001b[?25hCollecting absl-py>=0.7.0\n",
      "  Downloading absl_py-0.12.0-py3-none-any.whl (129 kB)\n",
      "\u001b[K     |████████████████████████████████| 129 kB 79.6 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: wrapt>=1.11.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from tensorflow-gpu==1.15.2) (1.12.1)\n",
      "Collecting tensorflow-estimator==1.15.1\n",
      "  Downloading tensorflow_estimator-1.15.1-py2.py3-none-any.whl (503 kB)\n",
      "\u001b[K     |████████████████████████████████| 503 kB 78.5 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting termcolor>=1.1.0\n",
      "  Downloading termcolor-1.1.0.tar.gz (3.9 kB)\n",
      "Collecting tensorboard<1.16.0,>=1.15.0\n",
      "  Downloading tensorboard-1.15.0-py3-none-any.whl (3.8 MB)\n",
      "\u001b[K     |████████████████████████████████| 3.8 MB 35.5 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: h5py in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from keras-applications>=1.0.8->tensorflow-gpu==1.15.2) (3.1.0)\n",
      "Collecting markdown>=2.6.8\n",
      "  Downloading Markdown-3.3.4-py3-none-any.whl (97 kB)\n",
      "\u001b[K     |████████████████████████████████| 97 kB 10.7 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: setuptools>=41.0.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from tensorboard<1.16.0,>=1.15.0->tensorflow-gpu==1.15.2) (49.6.0.post20210108)\n",
      "Requirement already satisfied: werkzeug>=0.11.15 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from tensorboard<1.16.0,>=1.15.0->tensorflow-gpu==1.15.2) (1.0.1)\n",
      "Requirement already satisfied: importlib-metadata in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from markdown>=2.6.8->tensorboard<1.16.0,>=1.15.0->tensorflow-gpu==1.15.2) (3.7.0)\n",
      "Requirement already satisfied: cached-property in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from h5py->keras-applications>=1.0.8->tensorflow-gpu==1.15.2) (1.5.1)\n",
      "Requirement already satisfied: zipp>=0.5 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from importlib-metadata->markdown>=2.6.8->tensorboard<1.16.0,>=1.15.0->tensorflow-gpu==1.15.2) (3.4.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.4 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from importlib-metadata->markdown>=2.6.8->tensorboard<1.16.0,>=1.15.0->tensorflow-gpu==1.15.2) (3.7.4.3)\n",
      "Building wheels for collected packages: gast, termcolor\n",
      "  Building wheel for gast (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for gast: filename=gast-0.2.2-py3-none-any.whl size=7538 sha256=905942cf6ab3c36b9600c21b7e5e1891e2320583c979142c9167ad32bb5cb718\n",
      "  Stored in directory: /home/ec2-user/.cache/pip/wheels/19/a7/b9/0740c7a3a7d1d348f04823339274b90de25fbcd217b2ee1fbe\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Building wheel for termcolor (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for termcolor: filename=termcolor-1.1.0-py3-none-any.whl size=4829 sha256=ebb9de3ded5e3aec448d534363b9bbf00963376381415155bbc9f69015a0d478\n",
      "  Stored in directory: /home/ec2-user/.cache/pip/wheels/93/2a/eb/e58dbcbc963549ee4f065ff80a59f274cc7210b6eab962acdc\n",
      "Successfully built gast termcolor\n",
      "Installing collected packages: markdown, grpcio, absl-py, termcolor, tensorflow-estimator, tensorboard, opt-einsum, keras-preprocessing, keras-applications, gast, astor, tensorflow-gpu\n",
      "Successfully installed absl-py-0.12.0 astor-0.8.1 gast-0.2.2 grpcio-1.36.1 keras-applications-1.0.8 keras-preprocessing-1.1.2 markdown-3.3.4 opt-einsum-3.3.0 tensorboard-1.15.0 tensorflow-estimator-1.15.1 tensorflow-gpu-1.15.2 termcolor-1.1.0\n"
     ]
    }
   ],
   "source": [
    "!pip install dataclasses\n",
    "!pip install datasets \n",
    "#!pip install tensorflow==1.15.2\n",
    "!pip install tensorflow-gpu==1.15.2\n",
    "#!pip install transformers==3.5.0\n",
    "\n",
    "#!pip install git+https://github.com/huggingface/transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q gpt-2-simple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:\n",
      "The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
      "For more information, please see:\n",
      "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
      "  * https://github.com/tensorflow/addons\n",
      "  * https://github.com/tensorflow/io (for I/O related ops)\n",
      "If you depend on functionality not listed there, please file an issue.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import gpt_2_simple as gpt2\n",
    "import os\n",
    "from threading import Event"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'!python run_clm.py     --model_name_or_path models/345M/     --train_file data/squad_train.txt     --validation_file data/squad_test.txt     --do_train     --do_eval     --output_dir Results/test_squad/     --evaluate_during_training     --learning_rate 5e-5     --num_train_epochs=3.0'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"!python run_clm.py \\\n",
    "    --model_name_or_path models/345M/ \\\n",
    "    --train_file data/squad_train.txt \\\n",
    "    --validation_file data/squad_test.txt \\\n",
    "    --do_train \\\n",
    "    --do_eval \\\n",
    "    --output_dir Results/test_squad/ \\\n",
    "    --evaluate_during_training \\\n",
    "    --learning_rate 5e-5 \\\n",
    "    --num_train_epochs=3.0\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GPT2 Simple Version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"345M\"\n",
    "if not os.path.isdir(os.path.join(\"models\", model_name)):\n",
    "    print(f\"Downloading {model_name} model...\")\n",
    "    gpt2.download_gpt2(model_name=model_name)   # model is saved into current directory under /models/<size>/\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SQUAD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess = gpt2.start_tf_sess()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/gpt_2_simple/src/sample.py:17: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "WARNING:tensorflow:From /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/gpt_2_simple/src/memory_saving_gradients.py:62: get_backward_walk_ops (from tensorflow.contrib.graph_editor.select) is deprecated and will be removed after 2019-06-06.\n",
      "Instructions for updating:\n",
      "Please use tensorflow.python.ops.op_selector.get_backward_walk_ops.\n",
      "Loading checkpoint models/345M/model.ckpt\n",
      "INFO:tensorflow:Restoring parameters from models/345M/model.ckpt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [01:28<00:00, 88.84s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset has 16793911 tokens\n",
      "Training...\n",
      "[1 | 23.66] loss=1.27 avg=1.27\n",
      "[2 | 33.28] loss=1.08 avg=1.18\n",
      "[3 | 42.93] loss=1.48 avg=1.28\n",
      "[4 | 52.40] loss=1.25 avg=1.27\n",
      "[5 | 61.94] loss=1.09 avg=1.23\n",
      "[6 | 71.50] loss=1.25 avg=1.24\n",
      "[7 | 80.99] loss=1.20 avg=1.23\n",
      "[8 | 90.48] loss=1.48 avg=1.26\n",
      "[9 | 99.96] loss=0.82 avg=1.21\n",
      "[10 | 109.45] loss=0.79 avg=1.17\n",
      "[11 | 118.93] loss=0.94 avg=1.15\n",
      "[12 | 128.41] loss=0.82 avg=1.12\n",
      "[13 | 137.86] loss=1.50 avg=1.15\n",
      "[14 | 147.33] loss=0.95 avg=1.13\n",
      "[15 | 156.86] loss=1.62 avg=1.17\n",
      "[16 | 166.37] loss=1.06 avg=1.16\n",
      "[17 | 175.87] loss=0.96 avg=1.15\n",
      "[18 | 185.32] loss=1.29 avg=1.16\n",
      "[19 | 194.79] loss=1.13 avg=1.16\n",
      "[20 | 204.26] loss=0.84 avg=1.14\n",
      "[21 | 213.81] loss=1.50 avg=1.16\n",
      "[22 | 223.50] loss=1.82 avg=1.19\n",
      "[23 | 232.96] loss=1.31 avg=1.20\n",
      "[24 | 242.42] loss=0.93 avg=1.18\n",
      "[25 | 251.86] loss=1.12 avg=1.18\n",
      "[26 | 261.38] loss=1.04 avg=1.18\n",
      "[27 | 270.84] loss=0.88 avg=1.16\n",
      "[28 | 280.38] loss=1.10 avg=1.16\n",
      "[29 | 289.87] loss=0.89 avg=1.15\n",
      "[30 | 299.38] loss=0.99 avg=1.14\n",
      "[31 | 308.89] loss=1.00 avg=1.14\n",
      "[32 | 318.34] loss=1.16 avg=1.14\n",
      "[33 | 327.84] loss=0.87 avg=1.13\n",
      "[34 | 337.40] loss=0.97 avg=1.12\n",
      "[35 | 346.88] loss=1.10 avg=1.12\n",
      "[36 | 356.35] loss=0.58 avg=1.11\n",
      "[37 | 365.84] loss=1.24 avg=1.11\n",
      "[38 | 375.36] loss=1.19 avg=1.11\n",
      "[39 | 384.88] loss=1.29 avg=1.12\n",
      "[40 | 394.41] loss=1.19 avg=1.12\n",
      "[41 | 404.00] loss=0.72 avg=1.11\n",
      "[42 | 413.48] loss=1.18 avg=1.11\n",
      "[43 | 422.96] loss=1.31 avg=1.12\n",
      "[44 | 432.45] loss=1.33 avg=1.12\n",
      "[45 | 441.93] loss=1.15 avg=1.12\n",
      "[46 | 451.41] loss=0.79 avg=1.11\n",
      "[47 | 460.91] loss=1.02 avg=1.11\n",
      "[48 | 470.37] loss=1.73 avg=1.13\n",
      "[49 | 479.86] loss=1.69 avg=1.14\n",
      "[50 | 489.33] loss=1.46 avg=1.15\n",
      "[51 | 498.83] loss=1.32 avg=1.15\n",
      "[52 | 508.33] loss=1.17 avg=1.15\n",
      "[53 | 517.82] loss=1.03 avg=1.15\n",
      "[54 | 527.34] loss=1.12 avg=1.15\n",
      "[55 | 536.90] loss=0.85 avg=1.14\n",
      "[56 | 546.45] loss=1.19 avg=1.14\n",
      "[57 | 555.96] loss=1.83 avg=1.16\n",
      "[58 | 565.48] loss=0.88 avg=1.15\n",
      "[59 | 575.04] loss=1.32 avg=1.16\n",
      "[60 | 584.60] loss=1.35 avg=1.16\n",
      "[61 | 594.12] loss=1.24 avg=1.16\n",
      "[62 | 603.64] loss=1.07 avg=1.16\n",
      "[63 | 613.15] loss=1.38 avg=1.17\n",
      "[64 | 622.65] loss=1.15 avg=1.17\n",
      "[65 | 632.12] loss=0.66 avg=1.16\n",
      "[66 | 641.59] loss=0.91 avg=1.15\n",
      "[67 | 651.07] loss=0.86 avg=1.14\n",
      "[68 | 660.56] loss=1.04 avg=1.14\n",
      "[69 | 670.06] loss=0.72 avg=1.13\n",
      "[70 | 679.64] loss=1.22 avg=1.14\n",
      "[71 | 689.17] loss=1.17 avg=1.14\n",
      "[72 | 698.68] loss=2.05 avg=1.15\n",
      "[73 | 708.20] loss=0.88 avg=1.15\n",
      "[74 | 717.73] loss=1.27 avg=1.15\n",
      "[75 | 727.19] loss=1.32 avg=1.15\n",
      "[76 | 736.69] loss=1.35 avg=1.16\n",
      "[77 | 746.17] loss=0.88 avg=1.15\n",
      "[78 | 755.80] loss=0.97 avg=1.15\n",
      "[79 | 765.26] loss=0.87 avg=1.14\n",
      "[80 | 774.77] loss=1.55 avg=1.15\n",
      "[81 | 784.24] loss=1.87 avg=1.16\n",
      "[82 | 793.69] loss=1.23 avg=1.17\n",
      "[83 | 803.13] loss=0.89 avg=1.16\n",
      "[84 | 812.67] loss=0.87 avg=1.16\n",
      "[85 | 822.17] loss=0.97 avg=1.15\n",
      "[86 | 831.64] loss=1.01 avg=1.15\n",
      "[87 | 841.18] loss=1.05 avg=1.15\n",
      "[88 | 850.65] loss=0.79 avg=1.14\n",
      "[89 | 860.13] loss=0.82 avg=1.14\n",
      "[90 | 869.66] loss=1.69 avg=1.15\n",
      "[91 | 879.12] loss=0.95 avg=1.14\n",
      "[92 | 888.61] loss=1.02 avg=1.14\n",
      "[93 | 898.09] loss=0.91 avg=1.14\n",
      "[94 | 907.60] loss=1.02 avg=1.13\n",
      "[95 | 917.15] loss=1.02 avg=1.13\n",
      "[96 | 926.62] loss=1.00 avg=1.13\n",
      "[97 | 936.28] loss=1.16 avg=1.13\n",
      "[98 | 945.76] loss=1.14 avg=1.13\n",
      "[99 | 955.26] loss=0.80 avg=1.13\n",
      "[100 | 964.72] loss=1.40 avg=1.13\n",
      "Saving checkpoint/QGen_SQUAD_test/model-100\n",
      "[101 | 987.97] loss=1.65 avg=1.14\n",
      "[102 | 997.41] loss=1.20 avg=1.14\n",
      "[104 | 1016.34] loss=0.97 avg=1.13\n",
      "[105 | 1025.74] loss=0.79 avg=1.13\n",
      "[106 | 1035.18] loss=1.07 avg=1.12\n",
      "[107 | 1044.62] loss=1.21 avg=1.13\n",
      "[108 | 1054.06] loss=1.03 avg=1.12\n",
      "[109 | 1063.53] loss=0.88 avg=1.12\n",
      "[110 | 1073.00] loss=1.37 avg=1.12\n",
      "[364 | 3479.72] loss=0.46 avg=1.12\n",
      "[365 | 3489.15] loss=0.76 avg=1.12\n",
      "[366 | 3498.82] loss=1.10 avg=1.12\n",
      "[367 | 3508.22] loss=1.31 avg=1.12\n",
      "[675 | 6425.79] loss=1.04 avg=1.10\n",
      "[1440 | 13671.82] loss=0.95 avg=1.09\n",
      "[1441 | 13681.27] loss=0.87 avg=1.09\n",
      "[1442 | 13690.75] loss=1.08 avg=1.09\n",
      "[1443 | 13700.22] loss=0.84 avg=1.09\n",
      "[1444 | 13709.66] loss=0.75 avg=1.08\n",
      "[1445 | 13719.27] loss=1.17 avg=1.09\n",
      "[1446 | 13728.69] loss=0.88 avg=1.08\n",
      "[1447 | 13738.12] loss=1.02 avg=1.08\n",
      "[1448 | 13747.57] loss=0.99 avg=1.08\n",
      "[1449 | 13757.00] loss=1.35 avg=1.08\n",
      "[1450 | 13766.45] loss=1.08 avg=1.08\n",
      "[1451 | 13775.89] loss=1.13 avg=1.08\n",
      "[1452 | 13785.35] loss=1.08 avg=1.08\n",
      "[1453 | 13794.78] loss=1.52 avg=1.09\n",
      "[1454 | 13804.19] loss=1.03 avg=1.09\n",
      "[1455 | 13813.63] loss=2.08 avg=1.10\n",
      "[1456 | 13823.07] loss=1.37 avg=1.10\n",
      "[1457 | 13832.52] loss=0.91 avg=1.10\n",
      "[1458 | 13841.98] loss=1.17 avg=1.10\n",
      "[1459 | 13851.41] loss=0.93 avg=1.10\n",
      "[1460 | 13860.89] loss=1.09 avg=1.10\n",
      "[1461 | 13870.33] loss=0.64 avg=1.09\n",
      "[1462 | 13879.76] loss=1.62 avg=1.10\n",
      "[1463 | 13889.21] loss=1.41 avg=1.10\n",
      "[1464 | 13898.81] loss=1.29 avg=1.10\n",
      "[1465 | 13908.24] loss=1.05 avg=1.10\n",
      "[1466 | 13917.68] loss=0.71 avg=1.10\n",
      "[1467 | 13927.13] loss=1.12 avg=1.10\n",
      "[1468 | 13936.57] loss=1.08 avg=1.10\n",
      "[1469 | 13946.01] loss=1.37 avg=1.10\n",
      "[1470 | 13955.45] loss=0.79 avg=1.10\n",
      "[1471 | 13964.86] loss=1.27 avg=1.10\n",
      "[1472 | 13974.30] loss=1.25 avg=1.10\n",
      "[1473 | 13983.73] loss=1.50 avg=1.11\n",
      "[1474 | 13993.16] loss=0.95 avg=1.10\n",
      "[1475 | 14002.62] loss=0.99 avg=1.10\n",
      "[1476 | 14012.07] loss=1.02 avg=1.10\n",
      "[1477 | 14021.53] loss=0.99 avg=1.10\n",
      "[1478 | 14030.98] loss=0.72 avg=1.10\n",
      "[1479 | 14040.45] loss=0.65 avg=1.09\n",
      "[1480 | 14049.89] loss=1.02 avg=1.09\n",
      "[1481 | 14059.32] loss=1.02 avg=1.09\n",
      "[1482 | 14068.76] loss=1.44 avg=1.10\n",
      "[1483 | 14078.34] loss=0.96 avg=1.09\n",
      "[1484 | 14087.81] loss=1.18 avg=1.09\n",
      "[1485 | 14097.25] loss=0.57 avg=1.09\n",
      "[1486 | 14106.68] loss=0.77 avg=1.09\n",
      "[1487 | 14116.11] loss=1.18 avg=1.09\n",
      "[1488 | 14125.55] loss=1.51 avg=1.09\n",
      "[1489 | 14134.97] loss=1.14 avg=1.09\n",
      "[1490 | 14144.45] loss=0.98 avg=1.09\n",
      "[1491 | 14153.89] loss=0.96 avg=1.09\n",
      "[1492 | 14163.30] loss=1.39 avg=1.09\n",
      "[1493 | 14172.73] loss=1.50 avg=1.10\n",
      "[1494 | 14182.15] loss=0.98 avg=1.10\n",
      "[1495 | 14191.59] loss=0.73 avg=1.09\n",
      "[1496 | 14201.04] loss=1.18 avg=1.09\n",
      "[1497 | 14210.48] loss=0.79 avg=1.09\n",
      "[1498 | 14219.90] loss=1.00 avg=1.09\n",
      "[1499 | 14229.34] loss=1.28 avg=1.09\n",
      "[1500 | 14238.81] loss=1.27 avg=1.09\n",
      "Saving checkpoint/QGen_SQUAD_test/model-1500\n",
      "[1501 | 14251.77] loss=0.46 avg=1.09\n",
      "[1502 | 14261.35] loss=0.67 avg=1.08\n",
      "[1503 | 14270.80] loss=0.99 avg=1.08\n",
      "[1504 | 14280.34] loss=1.32 avg=1.08\n",
      "[1505 | 14289.80] loss=0.92 avg=1.08\n",
      "[1506 | 14299.22] loss=1.12 avg=1.08\n",
      "[1507 | 14308.63] loss=0.57 avg=1.08\n",
      "[1508 | 14318.08] loss=1.09 avg=1.08\n",
      "[1509 | 14327.50] loss=2.03 avg=1.09\n",
      "[1510 | 14336.89] loss=0.67 avg=1.08\n",
      "[1511 | 14346.29] loss=0.68 avg=1.08\n",
      "[1512 | 14355.78] loss=0.95 avg=1.08\n",
      "[1513 | 14365.18] loss=0.83 avg=1.07\n",
      "[1514 | 14374.58] loss=1.23 avg=1.08\n",
      "[1515 | 14383.96] loss=1.66 avg=1.08\n",
      "[1516 | 14393.37] loss=1.00 avg=1.08\n",
      "[1517 | 14402.80] loss=0.98 avg=1.08\n",
      "[1518 | 14412.21] loss=0.99 avg=1.08\n",
      "[1519 | 14421.66] loss=0.80 avg=1.08\n",
      "[1520 | 14431.07] loss=1.04 avg=1.08\n",
      "[1521 | 14440.69] loss=1.53 avg=1.08\n",
      "[1522 | 14450.09] loss=1.12 avg=1.08\n",
      "[1523 | 14459.49] loss=1.22 avg=1.08\n",
      "[1524 | 14468.89] loss=1.01 avg=1.08\n",
      "[1525 | 14478.30] loss=0.61 avg=1.08\n",
      "[1526 | 14487.73] loss=1.08 avg=1.08\n",
      "[1527 | 14497.14] loss=0.92 avg=1.08\n",
      "[1528 | 14506.53] loss=1.28 avg=1.08\n",
      "[1529 | 14515.95] loss=1.13 avg=1.08\n",
      "[1530 | 14525.39] loss=1.12 avg=1.08\n",
      "[1531 | 14534.79] loss=1.01 avg=1.08\n",
      "[1532 | 14544.20] loss=0.65 avg=1.07\n",
      "[1533 | 14553.60] loss=1.56 avg=1.08\n",
      "[1534 | 14563.00] loss=1.04 avg=1.08\n",
      "[1535 | 14572.44] loss=1.84 avg=1.09\n",
      "[1536 | 14581.85] loss=1.37 avg=1.09\n",
      "[1537 | 14591.25] loss=0.97 avg=1.09\n",
      "[1538 | 14600.64] loss=1.64 avg=1.09\n",
      "[1539 | 14610.04] loss=0.77 avg=1.09\n",
      "[1540 | 14619.61] loss=0.71 avg=1.09\n",
      "[1541 | 14629.05] loss=1.29 avg=1.09\n",
      "[1542 | 14638.47] loss=1.10 avg=1.09\n",
      "[1543 | 14647.86] loss=0.92 avg=1.09\n",
      "[1544 | 14657.26] loss=0.92 avg=1.08\n",
      "[1545 | 14666.66] loss=1.05 avg=1.08\n",
      "[1546 | 14676.06] loss=0.93 avg=1.08\n",
      "[1547 | 14685.48] loss=1.20 avg=1.08\n",
      "[1548 | 14694.89] loss=1.27 avg=1.09\n",
      "[1549 | 14704.30] loss=1.12 avg=1.09\n",
      "[1550 | 14713.69] loss=1.07 avg=1.09\n",
      "[1551 | 14723.15] loss=1.06 avg=1.09\n",
      "[1552 | 14732.55] loss=1.17 avg=1.09\n",
      "[1553 | 14741.96] loss=1.40 avg=1.09\n",
      "[1554 | 14751.36] loss=1.12 avg=1.09\n",
      "[1555 | 14760.77] loss=1.08 avg=1.09\n",
      "[1556 | 14770.17] loss=2.19 avg=1.10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1557 | 14779.58] loss=0.52 avg=1.10\n",
      "[1558 | 14789.00] loss=1.25 avg=1.10\n",
      "[1559 | 14798.57] loss=1.27 avg=1.10\n",
      "[1560 | 14807.99] loss=1.20 avg=1.10\n",
      "[1561 | 14817.39] loss=0.71 avg=1.10\n",
      "[1562 | 14826.85] loss=1.04 avg=1.10\n",
      "[1563 | 14836.26] loss=0.72 avg=1.09\n",
      "[1564 | 14845.69] loss=0.99 avg=1.09\n",
      "[1565 | 14855.10] loss=1.63 avg=1.10\n",
      "[1566 | 14864.50] loss=0.57 avg=1.09\n",
      "[1567 | 14873.91] loss=0.74 avg=1.09\n",
      "[1568 | 14883.29] loss=0.70 avg=1.08\n",
      "[1569 | 14892.69] loss=0.84 avg=1.08\n",
      "[1570 | 14902.11] loss=0.89 avg=1.08\n",
      "[1571 | 14911.51] loss=1.04 avg=1.08\n",
      "[1572 | 14920.92] loss=0.99 avg=1.08\n",
      "[1573 | 14930.32] loss=0.70 avg=1.07\n",
      "[1574 | 14939.84] loss=1.18 avg=1.07\n",
      "[1575 | 14949.25] loss=1.37 avg=1.08\n",
      "[1576 | 14958.66] loss=0.92 avg=1.08\n",
      "[1577 | 14968.08] loss=1.54 avg=1.08\n",
      "[1578 | 14977.69] loss=1.51 avg=1.09\n",
      "[1579 | 14987.09] loss=1.08 avg=1.09\n",
      "[1580 | 14996.52] loss=1.03 avg=1.08\n",
      "[1581 | 15005.95] loss=0.85 avg=1.08\n",
      "[1582 | 15015.35] loss=0.84 avg=1.08\n",
      "[1583 | 15024.77] loss=1.23 avg=1.08\n",
      "[1584 | 15034.23] loss=0.77 avg=1.08\n",
      "[1585 | 15043.64] loss=1.56 avg=1.08\n",
      "[1586 | 15053.05] loss=0.93 avg=1.08\n",
      "[1587 | 15062.47] loss=1.42 avg=1.08\n",
      "[1588 | 15071.86] loss=0.89 avg=1.08\n",
      "[1589 | 15081.27] loss=0.65 avg=1.08\n",
      "[1590 | 15090.69] loss=0.62 avg=1.07\n",
      "[1591 | 15100.09] loss=1.66 avg=1.08\n",
      "[1592 | 15109.49] loss=1.12 avg=1.08\n",
      "[1593 | 15118.92] loss=1.37 avg=1.08\n",
      "[1594 | 15128.32] loss=1.30 avg=1.09\n",
      "[1595 | 15137.72] loss=0.94 avg=1.08\n",
      "[1596 | 15147.14] loss=0.78 avg=1.08\n",
      "[1597 | 15156.67] loss=1.53 avg=1.09\n",
      "[1598 | 15166.06] loss=0.76 avg=1.08\n",
      "[1599 | 15175.47] loss=0.64 avg=1.08\n",
      "[1600 | 15184.93] loss=0.76 avg=1.07\n",
      "Saving checkpoint/QGen_SQUAD_test/model-1600\n",
      "[1601 | 15197.87] loss=1.32 avg=1.08\n",
      "[1602 | 15207.30] loss=2.05 avg=1.09\n",
      "[1603 | 15216.77] loss=2.27 avg=1.10\n",
      "[1604 | 15226.33] loss=1.16 avg=1.10\n",
      "[1605 | 15235.77] loss=1.14 avg=1.10\n",
      "[1606 | 15245.20] loss=1.14 avg=1.10\n",
      "[1607 | 15254.68] loss=0.75 avg=1.10\n",
      "[1608 | 15264.09] loss=0.68 avg=1.09\n",
      "[1609 | 15273.53] loss=1.08 avg=1.09\n",
      "[1610 | 15282.98] loss=1.13 avg=1.09\n",
      "[1611 | 15292.42] loss=0.98 avg=1.09\n",
      "[1612 | 15301.90] loss=0.92 avg=1.09\n",
      "[1613 | 15311.37] loss=1.03 avg=1.09\n",
      "[1614 | 15320.80] loss=1.01 avg=1.09\n",
      "[1615 | 15330.24] loss=1.22 avg=1.09\n",
      "[1616 | 15339.89] loss=1.70 avg=1.10\n",
      "[1617 | 15349.31] loss=1.44 avg=1.10\n",
      "[1618 | 15358.73] loss=0.96 avg=1.10\n",
      "[1619 | 15368.18] loss=1.81 avg=1.11\n",
      "[1620 | 15377.63] loss=1.05 avg=1.10\n",
      "[1621 | 15387.10] loss=1.33 avg=1.11\n",
      "[1622 | 15396.52] loss=0.72 avg=1.10\n",
      "[1623 | 15405.94] loss=1.31 avg=1.11\n",
      "[1624 | 15415.39] loss=1.15 avg=1.11\n",
      "[1625 | 15424.83] loss=0.65 avg=1.10\n",
      "[1626 | 15434.30] loss=0.82 avg=1.10\n",
      "[1627 | 15443.76] loss=1.80 avg=1.11\n",
      "[1628 | 15453.33] loss=1.13 avg=1.11\n",
      "[1629 | 15462.82] loss=0.85 avg=1.10\n",
      "[1630 | 15472.26] loss=1.01 avg=1.10\n",
      "[1631 | 15481.70] loss=1.13 avg=1.10\n",
      "[1632 | 15491.14] loss=0.82 avg=1.10\n",
      "[1633 | 15500.55] loss=1.22 avg=1.10\n",
      "[1634 | 15510.03] loss=1.06 avg=1.10\n",
      "[1635 | 15519.64] loss=0.75 avg=1.10\n",
      "[1636 | 15529.06] loss=1.04 avg=1.10\n",
      "[1637 | 15538.53] loss=0.87 avg=1.09\n",
      "[1638 | 15547.96] loss=0.65 avg=1.09\n",
      "[1639 | 15557.41] loss=1.03 avg=1.09\n",
      "[1640 | 15566.90] loss=0.78 avg=1.09\n",
      "[1641 | 15576.44] loss=1.34 avg=1.09\n",
      "[1642 | 15585.91] loss=1.32 avg=1.09\n",
      "[1643 | 15595.37] loss=0.96 avg=1.09\n",
      "[1644 | 15604.83] loss=0.54 avg=1.08\n",
      "[1645 | 15614.26] loss=1.21 avg=1.09\n",
      "[1646 | 15623.71] loss=1.06 avg=1.08\n",
      "[1647 | 15633.16] loss=1.21 avg=1.09\n",
      "[1648 | 15642.59] loss=1.02 avg=1.09\n",
      "[1649 | 15652.02] loss=0.96 avg=1.08\n",
      "[1650 | 15661.46] loss=1.08 avg=1.08\n",
      "[1651 | 15670.90] loss=1.18 avg=1.09\n",
      "[1652 | 15680.34] loss=0.92 avg=1.08\n",
      "[1653 | 15689.81] loss=0.82 avg=1.08\n",
      "[1654 | 15699.42] loss=0.71 avg=1.08\n",
      "[1655 | 15708.87] loss=1.08 avg=1.08\n",
      "[1656 | 15718.34] loss=1.11 avg=1.08\n",
      "[1657 | 15727.78] loss=1.55 avg=1.08\n",
      "[1658 | 15737.22] loss=1.01 avg=1.08\n",
      "[1659 | 15746.67] loss=1.10 avg=1.08\n",
      "[1660 | 15756.11] loss=1.04 avg=1.08\n",
      "[1661 | 15765.54] loss=0.62 avg=1.08\n",
      "[1662 | 15774.97] loss=0.38 avg=1.07\n",
      "[1663 | 15784.41] loss=1.25 avg=1.07\n",
      "[1664 | 15793.84] loss=1.16 avg=1.07\n",
      "[1665 | 15803.28] loss=0.75 avg=1.07\n",
      "[1666 | 15812.74] loss=1.56 avg=1.07\n",
      "[1667 | 15822.19] loss=0.66 avg=1.07\n",
      "[1668 | 15831.62] loss=1.16 avg=1.07\n",
      "[1669 | 15841.09] loss=0.84 avg=1.07\n",
      "[1670 | 15850.52] loss=1.09 avg=1.07\n",
      "[1671 | 15860.00] loss=0.95 avg=1.07\n",
      "[1672 | 15869.50] loss=1.23 avg=1.07\n",
      "[1673 | 15879.15] loss=1.28 avg=1.07\n",
      "[1674 | 15888.59] loss=1.19 avg=1.07\n",
      "[1826 | 17329.23] loss=0.85 avg=1.11\n",
      "[2576 | 24581.73] loss=1.14 avg=1.09\n",
      "[2888 | 27523.00] loss=1.27 avg=1.08\n",
      "[3103 | 29553.55] loss=1.34 avg=1.04\n",
      "[3104 | 29563.22] loss=0.75 avg=1.04\n",
      "[3105 | 29572.67] loss=0.73 avg=1.03\n",
      "[3106 | 29582.06] loss=1.07 avg=1.03\n",
      "[3107 | 29591.70] loss=1.47 avg=1.04\n",
      "[3108 | 29601.19] loss=1.21 avg=1.04\n",
      "[3109 | 29610.86] loss=0.59 avg=1.03\n",
      "[3110 | 29620.50] loss=0.89 avg=1.03\n",
      "[3111 | 29630.10] loss=1.90 avg=1.04\n",
      "[3112 | 29639.56] loss=0.84 avg=1.04\n",
      "[3113 | 29649.21] loss=1.05 avg=1.04\n",
      "[3114 | 29658.73] loss=0.81 avg=1.04\n",
      "[3115 | 29668.38] loss=1.49 avg=1.04\n",
      "[3116 | 29677.95] loss=1.63 avg=1.05\n",
      "[3117 | 29687.57] loss=0.96 avg=1.05\n",
      "[3118 | 29697.14] loss=0.78 avg=1.04\n",
      "[3119 | 29706.75] loss=0.97 avg=1.04\n",
      "[3120 | 29716.33] loss=0.82 avg=1.04\n",
      "[3121 | 29725.92] loss=0.96 avg=1.04\n",
      "[3122 | 29735.64] loss=1.14 avg=1.04\n",
      "[3123 | 29745.14] loss=0.99 avg=1.04\n",
      "[3124 | 29754.81] loss=0.89 avg=1.04\n",
      "[3125 | 29764.30] loss=0.82 avg=1.04\n",
      "[3126 | 29773.96] loss=0.94 avg=1.04\n",
      "[3127 | 29783.49] loss=1.10 avg=1.04\n",
      "[3128 | 29793.18] loss=0.36 avg=1.03\n",
      "[3129 | 29802.74] loss=1.14 avg=1.03\n",
      "[3130 | 29812.36] loss=0.77 avg=1.03\n",
      "[3131 | 29821.97] loss=1.30 avg=1.03\n",
      "[3132 | 29831.56] loss=0.88 avg=1.03\n",
      "[3133 | 29841.13] loss=1.47 avg=1.03\n",
      "[3134 | 29850.72] loss=1.25 avg=1.04\n",
      "[3135 | 29860.30] loss=0.82 avg=1.03\n",
      "[3136 | 29869.82] loss=0.85 avg=1.03\n",
      "[3137 | 29879.57] loss=0.91 avg=1.03\n",
      "[3138 | 29889.15] loss=1.45 avg=1.04\n",
      "[3139 | 29898.65] loss=1.51 avg=1.04\n",
      "[3140 | 29908.30] loss=1.21 avg=1.04\n",
      "[3141 | 29917.97] loss=0.75 avg=1.04\n",
      "[3142 | 29927.65] loss=0.86 avg=1.04\n",
      "[3143 | 29937.22] loss=1.04 avg=1.04\n",
      "[3144 | 29946.77] loss=1.15 avg=1.04\n",
      "[3145 | 29956.16] loss=0.92 avg=1.04\n",
      "[3146 | 29965.55] loss=1.22 avg=1.04\n",
      "[3147 | 29974.97] loss=0.81 avg=1.04\n",
      "[3148 | 29984.37] loss=1.19 avg=1.04\n",
      "[3149 | 29993.80] loss=0.65 avg=1.03\n",
      "[3150 | 30003.22] loss=0.61 avg=1.03\n",
      "[3151 | 30012.62] loss=1.02 avg=1.03\n",
      "[3152 | 30022.02] loss=0.55 avg=1.03\n",
      "[3153 | 30031.45] loss=0.58 avg=1.02\n",
      "[3154 | 30040.85] loss=0.86 avg=1.02\n",
      "[3155 | 30050.25] loss=0.90 avg=1.02\n",
      "[3156 | 30059.65] loss=1.14 avg=1.02\n",
      "[3157 | 30069.05] loss=0.65 avg=1.02\n",
      "[3158 | 30078.43] loss=1.00 avg=1.02\n",
      "[3159 | 30087.81] loss=1.18 avg=1.02\n",
      "[3160 | 30097.36] loss=0.58 avg=1.01\n",
      "[3161 | 30106.77] loss=0.60 avg=1.01\n",
      "[3162 | 30116.16] loss=0.87 avg=1.01\n",
      "[3163 | 30125.55] loss=1.06 avg=1.01\n",
      "[3164 | 30134.96] loss=2.02 avg=1.02\n",
      "[3165 | 30144.36] loss=0.56 avg=1.01\n",
      "[3166 | 30153.78] loss=1.64 avg=1.02\n",
      "[3167 | 30163.18] loss=1.16 avg=1.02\n",
      "[3168 | 30172.58] loss=0.59 avg=1.02\n",
      "[3169 | 30182.01] loss=1.28 avg=1.02\n",
      "[3170 | 30191.45] loss=0.86 avg=1.02\n",
      "[3171 | 30200.84] loss=1.01 avg=1.02\n",
      "[3172 | 30210.24] loss=1.45 avg=1.02\n",
      "[3173 | 30219.65] loss=0.98 avg=1.02\n",
      "[3174 | 30229.04] loss=1.25 avg=1.02\n",
      "[3175 | 30238.44] loss=0.76 avg=1.02\n",
      "[3176 | 30247.83] loss=0.85 avg=1.02\n",
      "[3177 | 30257.23] loss=0.76 avg=1.02\n",
      "[3178 | 30266.64] loss=0.68 avg=1.01\n",
      "[3179 | 30276.14] loss=0.92 avg=1.01\n",
      "[3180 | 30285.54] loss=1.04 avg=1.01\n",
      "[3181 | 30294.93] loss=0.97 avg=1.01\n",
      "[3182 | 30304.31] loss=0.82 avg=1.01\n",
      "[3183 | 30313.71] loss=1.06 avg=1.01\n",
      "[3184 | 30323.11] loss=1.75 avg=1.02\n",
      "[3185 | 30332.52] loss=1.11 avg=1.02\n",
      "[3186 | 30341.92] loss=0.73 avg=1.02\n",
      "[3187 | 30351.32] loss=1.46 avg=1.02\n",
      "[3188 | 30360.72] loss=1.04 avg=1.02\n",
      "[3189 | 30370.14] loss=1.41 avg=1.02\n",
      "[3190 | 30379.53] loss=0.98 avg=1.02\n",
      "[3191 | 30388.91] loss=0.96 avg=1.02\n",
      "[3192 | 30398.29] loss=1.28 avg=1.03\n",
      "[3193 | 30407.69] loss=0.87 avg=1.02\n",
      "[3194 | 30417.08] loss=0.98 avg=1.02\n",
      "[3195 | 30426.49] loss=0.83 avg=1.02\n",
      "[3196 | 30435.89] loss=1.60 avg=1.03\n",
      "[3197 | 30445.32] loss=1.13 avg=1.03\n",
      "[3198 | 30454.81] loss=1.41 avg=1.03\n",
      "[3199 | 30464.32] loss=1.16 avg=1.03\n",
      "[3200 | 30473.71] loss=0.96 avg=1.03\n",
      "Saving checkpoint/QGen_SQUAD_test/model-3200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3201 | 30486.62] loss=0.97 avg=1.03\n",
      "[3202 | 30496.00] loss=0.65 avg=1.03\n",
      "[3203 | 30505.40] loss=1.10 avg=1.03\n",
      "[3204 | 30514.91] loss=1.61 avg=1.04\n",
      "[3205 | 30524.33] loss=0.92 avg=1.03\n",
      "[3206 | 30533.71] loss=1.62 avg=1.04\n",
      "[3207 | 30543.09] loss=1.42 avg=1.04\n",
      "[3208 | 30552.49] loss=1.68 avg=1.05\n",
      "[3209 | 30561.88] loss=1.17 avg=1.05\n",
      "[3210 | 30571.26] loss=1.19 avg=1.05\n",
      "[3211 | 30580.64] loss=0.70 avg=1.05\n",
      "[3212 | 30590.03] loss=0.71 avg=1.05\n",
      "[3213 | 30599.40] loss=1.16 avg=1.05\n",
      "[3214 | 30608.79] loss=1.47 avg=1.05\n",
      "[3215 | 30618.18] loss=1.28 avg=1.05\n",
      "[3216 | 30627.57] loss=1.07 avg=1.05\n",
      "[3217 | 30637.08] loss=0.97 avg=1.05\n",
      "[3218 | 30646.47] loss=1.35 avg=1.06\n",
      "[3219 | 30655.86] loss=1.05 avg=1.06\n",
      "[3220 | 30665.23] loss=0.99 avg=1.06\n",
      "[3221 | 30674.60] loss=0.83 avg=1.05\n",
      "[3222 | 30683.99] loss=0.60 avg=1.05\n",
      "[3223 | 30693.39] loss=0.99 avg=1.05\n",
      "[3224 | 30702.75] loss=1.48 avg=1.05\n",
      "[3225 | 30712.13] loss=1.73 avg=1.06\n",
      "[3226 | 30721.51] loss=1.40 avg=1.06\n",
      "[3227 | 30730.89] loss=0.56 avg=1.06\n",
      "[3228 | 30740.26] loss=1.20 avg=1.06\n",
      "[3229 | 30749.65] loss=1.52 avg=1.06\n",
      "[3230 | 30759.02] loss=0.84 avg=1.06\n",
      "[3231 | 30768.42] loss=1.07 avg=1.06\n",
      "[3232 | 30777.80] loss=1.52 avg=1.07\n",
      "[3233 | 30787.19] loss=1.38 avg=1.07\n",
      "[3234 | 30796.58] loss=0.90 avg=1.07\n",
      "[3235 | 30805.98] loss=1.52 avg=1.07\n",
      "[3236 | 30815.46] loss=1.79 avg=1.08\n",
      "[3237 | 30824.93] loss=0.69 avg=1.08\n",
      "[3238 | 30834.32] loss=1.50 avg=1.08\n",
      "[3239 | 30843.68] loss=1.57 avg=1.08\n",
      "[3240 | 30853.06] loss=1.26 avg=1.09\n",
      "[3241 | 30862.44] loss=1.12 avg=1.09\n",
      "[3242 | 30871.84] loss=1.03 avg=1.09\n",
      "[3243 | 30881.21] loss=0.65 avg=1.08\n",
      "[3244 | 30890.62] loss=0.88 avg=1.08\n",
      "[3245 | 30900.00] loss=1.41 avg=1.08\n",
      "[3246 | 30909.37] loss=0.82 avg=1.08\n",
      "[3247 | 30918.73] loss=1.05 avg=1.08\n",
      "[3248 | 30928.11] loss=0.88 avg=1.08\n",
      "[3249 | 30937.50] loss=1.01 avg=1.08\n",
      "[3250 | 30946.88] loss=1.30 avg=1.08\n",
      "[3251 | 30956.30] loss=1.08 avg=1.08\n",
      "[3252 | 30965.69] loss=1.07 avg=1.08\n",
      "[3253 | 30975.04] loss=0.75 avg=1.08\n",
      "[3254 | 30984.43] loss=0.62 avg=1.07\n",
      "[3255 | 30993.87] loss=0.79 avg=1.07\n",
      "[3256 | 31003.35] loss=1.09 avg=1.07\n",
      "[3257 | 31012.72] loss=1.07 avg=1.07\n",
      "[3258 | 31022.09] loss=1.40 avg=1.07\n",
      "[3259 | 31031.46] loss=1.04 avg=1.07\n",
      "[3260 | 31040.83] loss=1.55 avg=1.08\n",
      "[3261 | 31050.22] loss=1.05 avg=1.08\n",
      "[3262 | 31059.60] loss=1.14 avg=1.08\n",
      "[3263 | 31069.00] loss=1.01 avg=1.08\n",
      "[3264 | 31078.38] loss=0.81 avg=1.07\n",
      "[3265 | 31087.76] loss=1.66 avg=1.08\n",
      "[3266 | 31097.16] loss=0.99 avg=1.08\n",
      "[3267 | 31106.55] loss=0.99 avg=1.08\n",
      "[3268 | 31115.94] loss=0.98 avg=1.08\n",
      "[3269 | 31125.33] loss=1.53 avg=1.08\n",
      "[3270 | 31134.69] loss=0.90 avg=1.08\n",
      "[3271 | 31144.05] loss=1.15 avg=1.08\n",
      "[3272 | 31153.41] loss=0.88 avg=1.08\n",
      "[3273 | 31162.80] loss=0.74 avg=1.07\n",
      "[3274 | 31172.19] loss=1.01 avg=1.07\n",
      "[3275 | 31181.76] loss=0.91 avg=1.07\n",
      "[3276 | 31191.14] loss=0.89 avg=1.07\n",
      "[3277 | 31200.51] loss=1.50 avg=1.08\n",
      "[3278 | 31209.89] loss=1.55 avg=1.08\n",
      "[3279 | 31219.27] loss=1.01 avg=1.08\n",
      "[3280 | 31228.65] loss=1.00 avg=1.08\n",
      "[3281 | 31238.03] loss=0.84 avg=1.08\n",
      "[3282 | 31247.42] loss=0.90 avg=1.07\n",
      "[3283 | 31256.79] loss=1.76 avg=1.08\n",
      "[3284 | 31266.17] loss=0.95 avg=1.08\n",
      "[3285 | 31275.55] loss=0.69 avg=1.08\n",
      "[3286 | 31284.93] loss=0.87 avg=1.07\n",
      "[3287 | 31294.31] loss=1.31 avg=1.08\n",
      "[3288 | 31303.71] loss=1.40 avg=1.08\n",
      "[3289 | 31313.07] loss=0.97 avg=1.08\n",
      "[3290 | 31322.46] loss=0.58 avg=1.07\n",
      "[3291 | 31331.85] loss=1.18 avg=1.07\n",
      "[3292 | 31341.23] loss=1.07 avg=1.07\n",
      "[3293 | 31350.62] loss=1.29 avg=1.08\n",
      "[3294 | 31360.20] loss=0.89 avg=1.07\n",
      "[3295 | 31369.58] loss=0.83 avg=1.07\n",
      "[3296 | 31378.95] loss=1.02 avg=1.07\n",
      "[3297 | 31388.33] loss=0.97 avg=1.07\n",
      "[3298 | 31397.70] loss=1.10 avg=1.07\n",
      "[3299 | 31407.07] loss=0.85 avg=1.07\n",
      "[3300 | 31416.44] loss=1.35 avg=1.07\n",
      "Saving checkpoint/QGen_SQUAD_test/model-3300\n",
      "[3301 | 31429.36] loss=1.55 avg=1.08\n",
      "[3302 | 31438.82] loss=0.62 avg=1.07\n",
      "[3303 | 31448.24] loss=0.89 avg=1.07\n",
      "[3304 | 31457.76] loss=0.81 avg=1.07\n",
      "[3305 | 31467.19] loss=0.91 avg=1.07\n",
      "[3306 | 31476.59] loss=1.16 avg=1.07\n",
      "[3307 | 31485.99] loss=0.95 avg=1.07\n",
      "[3308 | 31495.40] loss=2.30 avg=1.08\n",
      "[3309 | 31504.80] loss=0.92 avg=1.08\n",
      "[3310 | 31514.20] loss=1.20 avg=1.08\n",
      "[3311 | 31523.62] loss=1.24 avg=1.08\n",
      "[3312 | 31533.07] loss=1.06 avg=1.08\n",
      "[3313 | 31542.66] loss=1.20 avg=1.08\n",
      "[3314 | 31552.04] loss=0.94 avg=1.08\n",
      "[3315 | 31561.46] loss=1.17 avg=1.08\n",
      "[3316 | 31570.85] loss=1.10 avg=1.08\n",
      "[3317 | 31580.29] loss=0.56 avg=1.07\n",
      "[3318 | 31589.71] loss=0.70 avg=1.07\n",
      "[3319 | 31599.10] loss=0.82 avg=1.07\n",
      "[3320 | 31608.50] loss=1.03 avg=1.07\n",
      "[3321 | 31617.90] loss=1.32 avg=1.07\n",
      "[3322 | 31627.29] loss=1.21 avg=1.07\n",
      "[3323 | 31636.66] loss=0.61 avg=1.07\n",
      "[3324 | 31646.06] loss=0.89 avg=1.07\n",
      "[3325 | 31655.46] loss=0.98 avg=1.06\n",
      "[3326 | 31664.87] loss=1.63 avg=1.07\n",
      "[3327 | 31674.30] loss=0.76 avg=1.07\n",
      "[3328 | 31683.72] loss=1.04 avg=1.07\n",
      "[3329 | 31693.13] loss=0.00 avg=1.06\n",
      "[3330 | 31702.54] loss=1.09 avg=1.06\n",
      "[3331 | 31711.99] loss=1.18 avg=1.06\n",
      "[3332 | 31721.59] loss=0.74 avg=1.05\n",
      "[3333 | 31731.03] loss=0.98 avg=1.05\n",
      "[3334 | 31740.41] loss=0.95 avg=1.05\n",
      "[3335 | 31749.84] loss=0.85 avg=1.05\n",
      "[3336 | 31759.22] loss=0.92 avg=1.05\n",
      "[3337 | 31768.63] loss=1.29 avg=1.05\n",
      "[3338 | 31778.02] loss=0.84 avg=1.05\n",
      "[3339 | 31787.43] loss=0.81 avg=1.05\n",
      "[3340 | 31796.83] loss=1.12 avg=1.05\n",
      "[3341 | 31806.23] loss=1.31 avg=1.05\n",
      "[3342 | 31815.65] loss=1.12 avg=1.05\n",
      "[3343 | 31825.06] loss=0.66 avg=1.05\n",
      "[3344 | 31834.47] loss=1.21 avg=1.05\n",
      "[3345 | 31843.87] loss=0.59 avg=1.04\n",
      "[3346 | 31853.27] loss=1.00 avg=1.04\n",
      "[3347 | 31862.67] loss=1.96 avg=1.05\n",
      "[3348 | 31872.09] loss=1.48 avg=1.06\n",
      "[3349 | 31881.49] loss=0.89 avg=1.06\n",
      "[3350 | 31890.91] loss=1.57 avg=1.06\n",
      "[3351 | 31900.52] loss=1.10 avg=1.06\n",
      "[3352 | 31909.92] loss=1.15 avg=1.06\n",
      "[3353 | 31919.32] loss=1.30 avg=1.06\n",
      "[3354 | 31928.71] loss=0.77 avg=1.06\n",
      "[3355 | 31938.12] loss=1.45 avg=1.07\n",
      "[3356 | 31947.50] loss=0.95 avg=1.06\n",
      "[3357 | 31956.89] loss=1.51 avg=1.07\n",
      "[3358 | 31966.29] loss=0.86 avg=1.07\n",
      "[3359 | 31975.71] loss=1.41 avg=1.07\n",
      "[3360 | 31985.13] loss=0.53 avg=1.06\n",
      "[3361 | 31994.53] loss=1.34 avg=1.07\n",
      "[3362 | 32003.93] loss=0.75 avg=1.06\n",
      "[3363 | 32013.34] loss=1.00 avg=1.06\n",
      "[3364 | 32022.73] loss=0.69 avg=1.06\n",
      "[3365 | 32032.13] loss=0.71 avg=1.06\n",
      "[3366 | 32041.54] loss=1.55 avg=1.06\n",
      "[3367 | 32050.97] loss=0.76 avg=1.06\n",
      "[3368 | 32060.39] loss=0.85 avg=1.06\n",
      "[3369 | 32069.81] loss=0.99 avg=1.06\n",
      "[3370 | 32079.42] loss=0.90 avg=1.05\n",
      "[3371 | 32088.84] loss=1.43 avg=1.06\n",
      "[3372 | 32098.26] loss=0.57 avg=1.05\n",
      "[3373 | 32107.64] loss=1.24 avg=1.06\n",
      "[3374 | 32117.03] loss=1.05 avg=1.06\n",
      "[3375 | 32126.43] loss=1.54 avg=1.06\n",
      "[3376 | 32135.83] loss=0.91 avg=1.06\n",
      "[3377 | 32145.22] loss=2.15 avg=1.07\n",
      "[3378 | 32154.62] loss=0.72 avg=1.07\n",
      "[3379 | 32164.03] loss=1.16 avg=1.07\n",
      "[3380 | 32173.41] loss=1.34 avg=1.07\n",
      "[3381 | 32182.81] loss=0.78 avg=1.07\n",
      "[3382 | 32192.22] loss=0.69 avg=1.06\n",
      "[3383 | 32201.62] loss=1.05 avg=1.06\n",
      "[3384 | 32211.04] loss=1.00 avg=1.06\n",
      "[3385 | 32220.47] loss=1.04 avg=1.06\n",
      "[3386 | 32229.89] loss=1.16 avg=1.06\n",
      "[3387 | 32239.27] loss=1.20 avg=1.06\n",
      "[3388 | 32248.65] loss=1.17 avg=1.07\n",
      "[3389 | 32258.21] loss=1.14 avg=1.07\n",
      "[3390 | 32267.61] loss=1.10 avg=1.07\n",
      "[3391 | 32277.03] loss=0.83 avg=1.06\n",
      "[3392 | 32286.42] loss=0.98 avg=1.06\n",
      "[3393 | 32295.80] loss=0.83 avg=1.06\n",
      "[3394 | 32305.18] loss=1.06 avg=1.06\n",
      "[3395 | 32314.57] loss=1.38 avg=1.06\n",
      "[3396 | 32323.99] loss=1.00 avg=1.06\n",
      "[3397 | 32333.39] loss=1.55 avg=1.07\n",
      "[3398 | 32342.77] loss=1.04 avg=1.07\n",
      "[3399 | 32352.19] loss=0.82 avg=1.07\n",
      "[3400 | 32361.60] loss=0.88 avg=1.06\n",
      "Saving checkpoint/QGen_SQUAD_test/model-3400\n",
      "[3401 | 32374.56] loss=1.35 avg=1.07\n",
      "[3402 | 32383.92] loss=0.94 avg=1.07\n",
      "[3403 | 32393.32] loss=1.04 avg=1.06\n",
      "[3404 | 32402.80] loss=1.14 avg=1.07\n",
      "[3405 | 32412.18] loss=1.27 avg=1.07\n",
      "[3406 | 32421.59] loss=1.12 avg=1.07\n",
      "[3407 | 32430.99] loss=1.44 avg=1.07\n",
      "[3408 | 32440.55] loss=1.66 avg=1.08\n",
      "[3409 | 32449.92] loss=0.81 avg=1.08\n",
      "[3410 | 32459.31] loss=1.46 avg=1.08\n",
      "[3411 | 32468.69] loss=0.92 avg=1.08\n",
      "[3412 | 32478.08] loss=1.36 avg=1.08\n",
      "[3413 | 32487.47] loss=1.01 avg=1.08\n",
      "[3414 | 32496.82] loss=0.85 avg=1.08\n",
      "[3415 | 32506.21] loss=0.83 avg=1.08\n",
      "[3416 | 32515.58] loss=1.05 avg=1.07\n",
      "[3417 | 32524.99] loss=0.89 avg=1.07\n",
      "[3418 | 32534.37] loss=1.23 avg=1.07\n",
      "[3419 | 32543.76] loss=1.03 avg=1.07\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3420 | 32553.15] loss=0.69 avg=1.07\n",
      "[3421 | 32562.52] loss=1.85 avg=1.08\n",
      "[3422 | 32571.92] loss=0.82 avg=1.08\n",
      "[3423 | 32581.30] loss=16.88 avg=1.23\n",
      "[3424 | 32590.71] loss=0.65 avg=1.23\n",
      "[3425 | 32600.08] loss=1.43 avg=1.23\n",
      "[3426 | 32609.45] loss=0.85 avg=1.23\n",
      "[3427 | 32619.01] loss=0.93 avg=1.22\n",
      "[3428 | 32628.38] loss=1.08 avg=1.22\n",
      "[3429 | 32637.78] loss=0.95 avg=1.22\n",
      "[3430 | 32647.14] loss=1.22 avg=1.22\n",
      "[3431 | 32656.52] loss=1.18 avg=1.22\n",
      "[3432 | 32665.92] loss=1.16 avg=1.22\n",
      "[3433 | 32675.30] loss=1.03 avg=1.22\n",
      "[3434 | 32684.68] loss=1.10 avg=1.21\n",
      "[3435 | 32694.07] loss=1.20 avg=1.21\n",
      "[3436 | 32703.44] loss=1.04 avg=1.21\n",
      "[3437 | 32712.82] loss=1.25 avg=1.21\n",
      "[3438 | 32722.21] loss=1.61 avg=1.22\n",
      "[3439 | 32731.61] loss=1.19 avg=1.22\n",
      "[3440 | 32740.99] loss=1.17 avg=1.22\n",
      "[3441 | 32750.36] loss=1.24 avg=1.22\n",
      "[3442 | 32759.75] loss=0.95 avg=1.21\n",
      "[3443 | 32769.13] loss=1.44 avg=1.22\n",
      "[3444 | 32778.50] loss=1.33 avg=1.22\n",
      "[3445 | 32787.91] loss=1.26 avg=1.22\n",
      "[3446 | 32797.43] loss=0.79 avg=1.21\n",
      "[3447 | 32806.80] loss=1.27 avg=1.21\n",
      "[3448 | 32816.20] loss=1.52 avg=1.22\n",
      "[3449 | 32825.59] loss=0.77 avg=1.21\n",
      "[3450 | 32834.99] loss=1.46 avg=1.22\n",
      "[3451 | 32844.36] loss=1.13 avg=1.21\n",
      "[3452 | 32853.74] loss=1.14 avg=1.21\n",
      "[3453 | 32863.10] loss=0.66 avg=1.21\n",
      "[3454 | 32872.49] loss=1.09 avg=1.21\n",
      "[3455 | 32881.87] loss=0.70 avg=1.20\n",
      "[3456 | 32891.26] loss=0.72 avg=1.20\n",
      "[3457 | 32900.61] loss=0.92 avg=1.19\n",
      "[3458 | 32910.01] loss=0.73 avg=1.19\n",
      "[3459 | 32919.37] loss=0.85 avg=1.19\n",
      "[3460 | 32928.73] loss=0.56 avg=1.18\n",
      "[3461 | 32938.12] loss=0.86 avg=1.18\n",
      "[3462 | 32947.50] loss=0.85 avg=1.17\n",
      "[3463 | 32956.85] loss=0.90 avg=1.17\n",
      "[3464 | 32966.26] loss=0.88 avg=1.17\n",
      "[3465 | 32975.80] loss=1.21 avg=1.17\n",
      "[3466 | 32985.19] loss=0.61 avg=1.16\n",
      "[3467 | 32994.64] loss=1.01 avg=1.16\n",
      "[3468 | 33004.05] loss=0.93 avg=1.16\n",
      "[3469 | 33013.44] loss=1.15 avg=1.16\n",
      "[3470 | 33022.80] loss=0.95 avg=1.16\n",
      "[3471 | 33032.17] loss=1.17 avg=1.16\n",
      "[3472 | 33041.56] loss=1.18 avg=1.16\n",
      "[3473 | 33050.95] loss=1.06 avg=1.16\n",
      "[3474 | 33060.33] loss=0.76 avg=1.15\n",
      "[3475 | 33069.71] loss=0.63 avg=1.15\n",
      "[3476 | 33079.08] loss=0.98 avg=1.14\n",
      "[3477 | 33088.48] loss=1.17 avg=1.15\n",
      "[3478 | 33097.86] loss=0.96 avg=1.14\n",
      "[3479 | 33107.23] loss=1.21 avg=1.14\n",
      "[3480 | 33116.61] loss=1.13 avg=1.14\n",
      "[3481 | 33125.98] loss=0.87 avg=1.14\n",
      "[3482 | 33135.36] loss=1.12 avg=1.14\n",
      "[3483 | 33144.76] loss=0.68 avg=1.14\n",
      "[3484 | 33154.21] loss=0.91 avg=1.13\n",
      "[3485 | 33163.69] loss=1.41 avg=1.14\n",
      "[3486 | 33173.11] loss=0.65 avg=1.13\n",
      "[3487 | 33182.48] loss=0.36 avg=1.12\n",
      "[3488 | 33191.87] loss=0.82 avg=1.12\n",
      "[3489 | 33201.26] loss=1.22 avg=1.12\n",
      "[3490 | 33210.65] loss=0.54 avg=1.12\n",
      "[3491 | 33220.06] loss=1.55 avg=1.12\n",
      "[3492 | 33229.46] loss=1.34 avg=1.12\n",
      "[3493 | 33238.83] loss=1.18 avg=1.12\n",
      "[3494 | 33248.21] loss=0.87 avg=1.12\n",
      "[3495 | 33257.63] loss=1.16 avg=1.12\n",
      "[3496 | 33267.04] loss=0.79 avg=1.12\n",
      "[3497 | 33276.40] loss=1.35 avg=1.12\n",
      "[3498 | 33285.79] loss=0.87 avg=1.12\n",
      "[3499 | 33295.17] loss=0.89 avg=1.12\n",
      "[3500 | 33304.61] loss=0.77 avg=1.11\n",
      "Saving checkpoint/QGen_SQUAD_test/model-3500\n",
      "[3501 | 33317.54] loss=0.90 avg=1.11\n",
      "[3502 | 33326.95] loss=1.27 avg=1.11\n",
      "[3503 | 33336.51] loss=0.95 avg=1.11\n",
      "[3504 | 33346.01] loss=1.07 avg=1.11\n",
      "[3638 | 34610.50] loss=1.23 avg=1.06\n",
      "[3639 | 34619.91] loss=0.97 avg=1.06\n",
      "[3640 | 34629.27] loss=0.80 avg=1.06\n",
      "[3641 | 34638.65] loss=0.95 avg=1.05\n",
      "[3642 | 34648.04] loss=1.50 avg=1.06\n",
      "[3643 | 34657.43] loss=1.00 avg=1.06\n",
      "[3644 | 34666.80] loss=0.50 avg=1.05\n",
      "[3645 | 34676.17] loss=0.66 avg=1.05\n",
      "[3646 | 34685.54] loss=1.26 avg=1.05\n",
      "[3647 | 34694.91] loss=1.34 avg=1.05\n",
      "[3648 | 34704.31] loss=0.94 avg=1.05\n",
      "[3649 | 34713.69] loss=0.82 avg=1.05\n",
      "[3650 | 34723.08] loss=0.89 avg=1.05\n",
      "[3651 | 34732.46] loss=1.01 avg=1.05\n",
      "[3652 | 34741.85] loss=1.20 avg=1.05\n",
      "[3653 | 34751.24] loss=1.05 avg=1.05\n",
      "[3654 | 34760.61] loss=1.56 avg=1.06\n",
      "[3655 | 34770.01] loss=0.99 avg=1.05\n",
      "[3656 | 34779.57] loss=1.05 avg=1.05\n",
      "[3657 | 34788.94] loss=0.98 avg=1.05\n",
      "[3658 | 34798.31] loss=1.16 avg=1.05\n",
      "[3659 | 34807.68] loss=0.91 avg=1.05\n",
      "[3660 | 34817.06] loss=0.85 avg=1.05\n",
      "[3661 | 34826.46] loss=0.90 avg=1.05\n",
      "[3662 | 34835.84] loss=0.94 avg=1.05\n",
      "[3663 | 34845.20] loss=0.78 avg=1.05\n",
      "[3664 | 34854.58] loss=1.02 avg=1.05\n",
      "[3665 | 34864.01] loss=0.89 avg=1.04\n",
      "[3666 | 34873.37] loss=0.56 avg=1.04\n",
      "[3667 | 34882.74] loss=1.15 avg=1.04\n",
      "[3668 | 34892.15] loss=1.13 avg=1.04\n",
      "[3669 | 34901.53] loss=0.87 avg=1.04\n",
      "[3670 | 34910.92] loss=1.00 avg=1.04\n",
      "[3671 | 34920.28] loss=1.40 avg=1.04\n",
      "[3672 | 34929.67] loss=1.22 avg=1.04\n",
      "[3673 | 34939.08] loss=1.75 avg=1.05\n",
      "[3674 | 34948.48] loss=1.05 avg=1.05\n",
      "[3675 | 34958.01] loss=1.06 avg=1.05\n",
      "[3676 | 34967.38] loss=0.66 avg=1.05\n",
      "[3677 | 34976.76] loss=0.65 avg=1.04\n",
      "[3678 | 34986.14] loss=0.92 avg=1.04\n",
      "[3679 | 34995.51] loss=0.96 avg=1.04\n",
      "[3680 | 35004.94] loss=0.97 avg=1.04\n",
      "[3681 | 35014.32] loss=0.81 avg=1.04\n",
      "[3682 | 35023.71] loss=1.42 avg=1.04\n",
      "[3683 | 35033.09] loss=0.46 avg=1.04\n",
      "[3684 | 35042.47] loss=1.22 avg=1.04\n",
      "[3685 | 35051.83] loss=1.04 avg=1.04\n",
      "[3686 | 35061.22] loss=1.65 avg=1.04\n",
      "[3687 | 35070.64] loss=0.85 avg=1.04\n",
      "[3688 | 35080.03] loss=1.12 avg=1.04\n",
      "[3689 | 35089.42] loss=0.93 avg=1.04\n",
      "[3690 | 35098.79] loss=1.06 avg=1.04\n",
      "[3691 | 35108.16] loss=1.32 avg=1.05\n",
      "[3692 | 35117.53] loss=0.64 avg=1.04\n",
      "[3693 | 35126.91] loss=1.04 avg=1.04\n",
      "[3694 | 35136.40] loss=0.73 avg=1.04\n",
      "[3695 | 35145.80] loss=1.04 avg=1.04\n",
      "[3696 | 35155.17] loss=0.91 avg=1.04\n",
      "[3697 | 35164.57] loss=0.92 avg=1.04\n",
      "[3698 | 35173.98] loss=1.28 avg=1.04\n",
      "[3699 | 35183.35] loss=0.86 avg=1.04\n",
      "[3700 | 35192.76] loss=1.43 avg=1.04\n",
      "Saving checkpoint/QGen_SQUAD_test/model-3700\n",
      "[3701 | 35205.67] loss=0.75 avg=1.04\n",
      "[3702 | 35215.07] loss=1.00 avg=1.04\n",
      "[3703 | 35224.47] loss=1.28 avg=1.04\n",
      "[3704 | 35233.98] loss=1.12 avg=1.04\n",
      "[3705 | 35243.35] loss=1.25 avg=1.04\n",
      "[3706 | 35252.78] loss=1.01 avg=1.04\n",
      "[3707 | 35262.18] loss=1.31 avg=1.04\n",
      "[3708 | 35271.57] loss=1.09 avg=1.04\n",
      "[3709 | 35280.97] loss=1.58 avg=1.05\n",
      "[3710 | 35290.36] loss=1.15 avg=1.05\n",
      "[3711 | 35299.77] loss=0.46 avg=1.05\n",
      "[3712 | 35309.20] loss=0.80 avg=1.04\n",
      "[3713 | 35318.77] loss=0.95 avg=1.04\n",
      "[3714 | 35328.18] loss=1.01 avg=1.04\n",
      "[3715 | 35337.59] loss=0.95 avg=1.04\n",
      "[3716 | 35347.00] loss=0.69 avg=1.04\n",
      "[3717 | 35356.38] loss=1.14 avg=1.04\n",
      "[3718 | 35365.78] loss=1.11 avg=1.04\n",
      "[3719 | 35375.20] loss=0.97 avg=1.04\n",
      "[3720 | 35384.60] loss=1.37 avg=1.04\n",
      "[3721 | 35394.00] loss=1.07 avg=1.04\n",
      "[3722 | 35403.40] loss=1.09 avg=1.04\n",
      "[3723 | 35412.80] loss=0.64 avg=1.04\n",
      "[3724 | 35422.20] loss=0.89 avg=1.04\n"
     ]
    }
   ],
   "source": [
    "gpt2.finetune(sess,\n",
    "              dataset= \"data/squad_train.txt\", \n",
    "              model_name='345M', \n",
    "              steps=4000, \n",
    "              restore_from='fresh', \n",
    "              run_name='QGen_SQUAD_test', \n",
    "              print_every=1, \n",
    "              sample_every=2000, \n",
    "              save_every=100  \n",
    "             )\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing Trained Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = open('data/squad_test.txt')\n",
    "testset = file.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_lst = testset.split('<|endoftext|>\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_str = test_lst[0].split('[QUESTION]:')[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "ctx = test_str\n",
    "pre = ctx + \" [QUESTION]:\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading checkpoint checkpoint/QGen_SQUAD_test/model-4000\n",
      "INFO:tensorflow:Restoring parameters from checkpoint/QGen_SQUAD_test/model-4000\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "sess = gpt2.start_tf_sess()\n",
    "gpt2.load_gpt2(sess, run_name='QGen_SQUAD_test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|startoftext|>\n",
      "[CONTEXT]: The Normans (Norman: Nourmands; French: Normands; Latin: Normanni) were the people who in the 10th and 11th centuries gave their name to Normandy, a region in France. They were descended from Norse (\"Norman\" comes from \"Norseman\") raiders and pirates from Denmark, Iceland and Norway who, under their leader Rollo, agreed to swear fealty to King Charles III of West Francia. Through generations of assimilation and mixing with the native Frankish and Roman-Gaulish populations, their descendants would gradually merge with the Carolingian-based cultures of West Francia. The distinct cultural and ethnic identity of the Normans emerged initially in the first half of the 10th century, and it continued to evolve over the succeeding centuries.  [QUESTION]: What language was the Normans called? <|endoftext|>\n",
      "<|startoftext|>\n",
      "[CONTEXT]: The Normans (Norman: Nourmands; French: Normands; Latin: Normanni) were the people who in the 10th and 11th centuries gave their name to Normandy, a region in France. They were descended from Norse (\"Norman\" comes from \"Norseman\") raiders and pirates from Denmark, Iceland and Norway who, under their leader Rollo, agreed to swear fealty to King Charles III of West Francia. Through generations of assimilation and mixing with the native Frankish and Roman-Gaulish populations, their descendants would gradually merge with the Carolingian-based cultures of West Francia. The distinct cultural and ethnic identity of the Normans emerged initially in the first half of the 10th century, and it continued to evolve over the succeeding centuries.  [QUESTION]: Where did the Normans come from? <|endoftext|>\n",
      "<|startoftext|>\n",
      "[CONTEXT]: The Normans (Norman: Nourmands; French: Normands; Latin: Normanni) were the people who in the 10th and 11th centuries gave their name to Normandy, a region in France. They were descended from Norse (\"Norman\" comes from \"Norseman\") raiders and pirates from Denmark, Iceland and Norway who, under their leader Rollo, agreed to swear fealty to King Charles III of West Francia. Through generations of assimilation and mixing with the native Frankish and Roman-Gaulish populations, their descendants would gradually merge with the Carolingian-based cultures of West Francia. The distinct cultural and ethnic identity of the Normans emerged initially in the first half of the 10th century, and it continued to evolve over the succeeding centuries.  [QUESTION]: What language was the Normans called? <|endoftext|>\n",
      "<|startoftext|>\n",
      "[CONTEXT]: The Normans (Norman: Nourmands; French: Normands; Latin: Normanni) were the people who in the 10th and 11th centuries gave their name to Normandy, a region in France. They were descended from Norse (\"Norman\" comes from \"Norseman\") raiders and pirates from Denmark, Iceland and Norway who, under their leader Rollo, agreed to swear fealty to King Charles III of West Francia. Through generations of assimilation and mixing with the native Frankish and Roman-Gaulish populations, their descendants would gradually merge with the Carolingian-based cultures of West Francia. The distinct cultural and ethnic identity of the Normans emerged initially in the first half of the 10th century, and it continued to evolve over the succeeding centuries.  [QUESTION]: When did the Normans' culture of West Francia merge with the Carolingian? <|endoftext|>\n",
      "<|startoftext|>\n",
      "[CONTEXT]: The Normans (Norman: Nourmands; French: Normanni) were the people who in the 10th and 11th centuries gave their name to Normandy, a region in France. They were descended from Norse (\"Norman\" comes from \"Norseman\") raiders and pirates from Denmark, Iceland and Norway who, under their leader Rollo, agreed to swear fealty to King Charles III of West Francia. Through generations of assimilation and mixing with the native Frankish and Roman-Gaulish populations, their descendants would gradually merge with the Carolingian-based cultures of West Francia. The distinct cultural and ethnic identity of the Normans emerged initially in the first half of the 10th century, and it continued to evolve over the succeeding centuries.  [QUESTION]: Who did the Normans' descend from? <|endoftext|>\n",
      "<|startoftext|>\n",
      "[CONTEXT]: The Normans (Norman: Nourmands; French: Normanni) were the people who in the 10th\n",
      "====================\n",
      "<|startoftext|>\n",
      "[CONTEXT]: The Normans (Norman: Nourmands; French: Normands; Latin: Normanni) were the people who in the 10th and 11th centuries gave their name to Normandy, a region in France. They were descended from Norse (\"Norman\" comes from \"Norseman\") raiders and pirates from Denmark, Iceland and Norway who, under their leader Rollo, agreed to swear fealty to King Charles III of West Francia. Through generations of assimilation and mixing with the native Frankish and Roman-Gaulish populations, their descendants would gradually merge with the Carolingian-based cultures of West Francia. The distinct cultural and ethnic identity of the Normans emerged initially in the first half of the 10th century, and it continued to evolve over the succeeding centuries.  [QUESTION]: Who gave their name to Normandy? <|endoftext|>\n",
      "<|startoftext|>\n",
      "[CONTEXT]: The Normans (Norman: Nourmands; French: Normands; Latin: Normanni) were the people who in the 10th and 11th centuries gave their name to Normandy, a region in France. They were descended from Norse (\"Norman\" comes from \"Norseman\") raiders and pirates from Denmark, Iceland and Norway who, under their leader Rollo, agreed to swear fealty to King Charles III of West Francia. Through generations of assimilation and mixing with the native Frankish and Roman-Gaulish populations, their descendants would gradually merge with the Carolingian-based cultures of West Francia. The distinct cultural and ethnic identity of the Normans emerged initially in the first half of the 10th century, and it continued to evolve over the succeeding centuries.  [QUESTION]: When did Normans establish themselves in Normandy? <|endoftext|>\n",
      "<|startoftext|>\n",
      "[CONTEXT]: The Normans (Norman: Nourmands; French: Normands; Latin: Normanni) were the people who in the 10th and 11th centuries gave their name to Normandy, a region in France. They were descended from Norse (\"Norman\" comes from \"Norseman\") raiders and pirates from Denmark, Iceland and Norway who, under their leader Rollo, agreed to swear fealty to King Charles III of West Francia. Through generations of assimilation and mixing with the native Frankish and Roman-Gaulish populations, their descendants would gradually merge with the Carolingian-based cultures of West Francia. The distinct cultural and ethnic identity of the Normans emerged initially in the first half of the 10th century, and it continued to evolve over the succeeding centuries.  [QUESTION]: Where did the Normans originate? <|endoftext|>\n",
      "<|startoftext|>\n",
      "[CONTEXT]: The Normans (Norman: Nourmands; French: Normands; Latin: Normanni) were the people who in the 10th and 11th centuries gave their name to Normandy, a region in France. They were descended from Norse (\"Norman\" comes from \"Norseman\") raiders and pirates from Denmark, Iceland and Norway who, under their leader Rollo, agreed to swear fealty to King Charles III of West Francia. Through generations of assimilation and mixing with the native Frankish and Roman-Gaulish populations, their descendants would gradually merge with the Carolingian-based cultures of West Francia. The distinct cultural and ethnic identity of the Normans emerged initially in the first half of the 10th century, and it continued to evolve over the succeeding centuries.  [QUESTION]: Where did the Normans give their name to? <|endoftext|>\n",
      "<|startoftext|>\n",
      "[CONTEXT]: The Normans (Norman: Nourmands; French: Normanni) were the people who in the 10th and 11th centuries gave their name to Normandy, a region in France. They were descended from Norse (\"Norman\" comes from \"Norseman\") raiders and pirates from Denmark, Iceland and Norway who, under their leader Rollo, agreed to swear fealty to King Charles III of West Francia. Through generations of assimilation and mixing with the native Frankish and Roman-Gaulish populations, their descendants would gradually merge with the Carolingian-based cultures of West Francia. The distinct cultural and ethnic identity of the Normans emerged initially in the first half of the 10th century, and it continued to evolve over the succeeding centuries.  [QUESTION]: What happened to the Normans? <|endoftext|>\n",
      "<|startoftext|>\n",
      "[CONTEXT]: The Normans (Norman: Nourmands; French: Normanni) were the people who in the 10th and 11th centuries gave their name to Normandy, a\n",
      "====================\n",
      "<|startoftext|>\n",
      "[CONTEXT]: The Normans (Norman: Nourmands; French: Normands; Latin: Normanni) were the people who in the 10th and 11th centuries gave their name to Normandy, a region in France. They were descended from Norse (\"Norman\" comes from \"Norseman\") raiders and pirates from Denmark, Iceland and Norway who, under their leader Rollo, agreed to swear fealty to King Charles III of West Francia. Through generations of assimilation and mixing with the native Frankish and Roman-Gaulish populations, their descendants would gradually merge with the Carolingian-based cultures of West Francia. The distinct cultural and ethnic identity of the Normans emerged initially in the first half of the 10th century, and it continued to evolve over the succeeding centuries.  [QUESTION]: What was the name of the Normans? <|endoftext|>\n",
      "<|startoftext|>\n",
      "[CONTEXT]: The Normans (Norman: Nourmands; French: Normants; Latin: Normanni) were the people who in the 10th and 11th centuries gave their name to Normandy, a region in France. They were descended from Norse (\"Norman\" comes from \"Norseman\") raiders and pirates from Denmark, Iceland and Norway who, under their leader Rollo, agreed to swear fealty to King Charles III of West Francia. Through generations of assimilation and mixing with the native Frankish and Roman-Gaulish populations, their descendants would gradually merge with the Carolingian-based cultures of West Francia. The distinct cultural and ethnic identity of the Normans emerged initially in the first half of the 10th century, and it continued to evolve over the succeeding centuries.  [QUESTION]: When did the Normans give their name? <|endoftext|>\n",
      "<|startoftext|>\n",
      "[CONTEXT]: The Normans (Norman: Nourmands; French: Normants; Latin: Normanni) were the people who in the 10th and 11th centuries gave their name to Normandy, a region in France. They were descended from Norse (\"Norman\" comes from \"Norseman\") raiders and pirates from Denmark, Iceland and Norway who, under their leader Rollo, agreed to swear fealty to King Charles III of West Francia. Through generations of assimilation and mixing with the native Frankish and Roman-Gaulish populations, their descendants would gradually merge with the Carolingian-based cultures of West Francia. The distinct cultural and ethnic identity of the Normans emerged initially in the first half of the 10th century, and it continued to evolve over the succeeding centuries.  [QUESTION]: What was the Normans named? <|endoftext|>\n",
      "<|startoftext|>\n",
      "[CONTEXT]: The Normans (Norman: Nourmands; French: Normants; Latin: Normanni) were the people who in the 10th and 11th centuries gave their name to Normandy, a region in France. They were descended from Norse (\"Norman\" comes from \"Norseman\") raiders and pirates from Denmark, Iceland and Norway who, under their leader Rollo, agreed to swear fealty to King Charles III of West Francia. Through generations of assimilation and mixing with the native Frankish and Roman-Gaulish populations, their descendants would gradually merge with the Carolingian-based cultures of West Francia. The distinct cultural and ethnic identity of the Normans emerged initially in the first half of the 10th century, and it continued to evolve over the succeeding centuries.  [QUESTION]: When did the Normans merge with the Carolingians? <|endoftext|>\n",
      "<|startoftext|>\n",
      "[CONTEXT]: The Normans (Norman: Nourmands; French: Normants; Latin: Normanni) were the people who in the 10th and 11th centuries gave their name to Normandy, a region in France. They were descended from Norse (\"Norman\" comes from \"Norseman\") raiders and pirates from Denmark, Iceland and Norway who, under their leader Rollo, agreed to swear fealty to King Charles III of West Francia. Through generations of assimilation and mixing with the native Frankish and Roman-Gaulish populations, their descendants would gradually merge with the Carolingian-based cultures of West Francia. The distinct cultural and ethnic identity of the Normans emerged initially in the first half of the 10th century, and it continued to evolve over the succeeding centuries.  [QUESTION]: When the Normans took over Normandy, what language did they gain? <|endoftext|>\n",
      "<|startoftext|>\n",
      "[CONTEXT]: The Normans (Norman: Nourmands; French: Normants; Latin: Norm\n",
      "====================\n",
      "<|startoftext|>\n",
      "[CONTEXT]: The Normans (Norman: Nourmands; French: Normands; Latin: Normanni) were the people who in the 10th and 11th centuries gave their name to Normandy, a region in France. They were descended from Norse (\"Norman\" comes from \"Norseman\") raiders and pirates from Denmark, Iceland and Norway who, under their leader Rollo, agreed to swear fealty to King Charles III of West Francia. Through generations of assimilation and mixing with the native Frankish and Roman-Gaulish populations, their descendants would gradually merge with the Carolingian-based cultures of West Francia. The distinct cultural and ethnic identity of the Normans emerged initially in the first half of the 10th century, and it continued to evolve over the succeeding centuries.  [QUESTION]: What was the name of the Normans? <|endoftext|>\n",
      "<|startoftext|>\n",
      "[CONTEXT]: The Normans (Norman: Nourmands; French: Normands; Latin: Normanni) were the people who in the 10th and 11th centuries gave their name to Normandy, a region in France. They were descended from Norse (\"Norman\" comes from \"Norseman\") raiders and pirates from Denmark, Iceland and Norway who, under their leader Rollo, agreed to swear fealty to King Charles III of West Francia. Through generations of assimilation and mixing with the native Frankish and Roman-Gaulish populations, their descendants would gradually merge with the Carolingian-based cultures of West Francia. The distinct cultural and ethnic identity of the Normans emerged initially in the first half of the 10th century, and it continued to evolve over the succeeding centuries.  [QUESTION]: When did the Normans take on their named name? <|endoftext|>\n",
      "<|startoftext|>\n",
      "[CONTEXT]: The Normans (Norman: Nourmands; French: Normands; Latin: Normanni) were the people who in the 10th and 11th centuries gave their name to Normandy, a region in France. They were descended from Norse (\"Norman\" comes from \"Norseman\") raiders and pirates from Denmark, Iceland and Norway who, under their leader Rollo, agreed to swear fealty to King Charles III of West Francia. Through generations of assimilation and mixing with the native Frankish and Roman-Gaulish populations, their descendants would gradually merge with the Carolingian-based cultures of West Francia. The distinct cultural and ethnic identity of the Normans emerged initially in the first half of the 10th century, and it continued to evolve over the succeeding centuries.  [QUESTION]: What was the name of the Normans? <|endoftext|>\n",
      "<|startoftext|>\n",
      "[CONTEXT]: The Normans (Norman: Nourmands; French: Normands; Latin: Normanni) were the people who in the 10th and 11th centuries gave their name to Normandy, a region in France. They were descended from Norse (\"Norman\" comes from \"Norseman\") raiders and pirates from Denmark, Iceland and Norway who, under their leader Rollo, agreed to swear fealty to King Charles III of West Francia. Through generations of assimilation and mixing with the native Frankish and Roman-Gaulish populations, their descendants would gradually merge with the Carolingian-based cultures of West Francia. The distinct cultural and ethnic identity of the Normans emerged initially in the first half of the 10th century, and it continued to evolve over the succeeding centuries.  [QUESTION]: What did the Normans give their name to? <|endoftext|>\n",
      "<|startoftext|>\n",
      "[CONTEXT]: The Normans (Norman: Nourmands; French: Normanni) were the people who in the 10th and 11th centuries gave their name to Normandy, a region in France. They were descended from Norse (\"Norman\" comes from \"Norseman\") raiders and pirates from Denmark, Iceland and Norway who, under their leader Rollo, agreed to swear fealty to King Charles III of West Francia. Through generations of assimilation and mixing with the native Frankish and Roman-Gaulish populations, their descendants would gradually merge with the Carolingian-based cultures of West Francia. The distinct cultural and ethnic identity of the Normans emerged initially in the first half of the 10th century, and it continued to evolve over the succeeding centuries.  [QUESTION]: What was the name of the Normans that gave Normandy? <|endoftext|>\n",
      "<|startoftext|>\n",
      "[CONTEXT]: The Normans (Norman: Nourmands; French: Normanni) were the people who in the 10th\n",
      "====================\n",
      "<|startoftext|>\n",
      "[CONTEXT]: The Normans (Norman: Nourmands; French: Normands; Latin: Normanni) were the people who in the 10th and 11th centuries gave their name to Normandy, a region in France. They were descended from Norse (\"Norman\" comes from \"Norseman\") raiders and pirates from Denmark, Iceland and Norway who, under their leader Rollo, agreed to swear fealty to King Charles III of West Francia. Through generations of assimilation and mixing with the native Frankish and Roman-Gaulish populations, their descendants would gradually merge with the Carolingian-based cultures of West Francia. The distinct cultural and ethnic identity of the Normans emerged initially in the first half of the 10th century, and it continued to evolve over the succeeding centuries.  [QUESTION]: Which people were the Normans? <|endoftext|>\n",
      "<|startoftext|>\n",
      "[CONTEXT]: The Normans (Norman: Nourmands; French: Normands; Latin: Normanni) were the people who in the 10th and 11th centuries gave their name to Normandy, a region in France. They were descended from Norse (\"Norman\" comes from \"Norseman\") raiders and pirates from Denmark, Iceland and Norway who, under their leader Rollo, agreed to swear fealty to King Charles III of West Francia. Through generations of assimilation and mixing with the native Frankish and Roman-Gaulish populations, their descendants would gradually merge with the Carolingian-based cultures of West Francia. The distinct cultural and ethnic identity of the Normans emerged initially in the first half of the 10th century, and it continued to evolve over the succeeding centuries.  [QUESTION]: What was the name of the leader of the Normans? <|endoftext|>\n",
      "<|startoftext|>\n",
      "[CONTEXT]: The Normans (Norman: Nourmands; French: Normands; Latin: Normanni) were the people who in the 10th and 11th centuries gave their name to Normandy, a region in France. They were descended from Norse (\"Norman\" comes from \"Norseman\") raiders and pirates from Denmark, Iceland and Norway who, under their leader Rollo, agreed to swear fealty to King Charles III of West Francia. Through generations of assimilation and mixing with the native Frankish and Roman-Gaulish populations, their descendants would gradually merge with the Carolingian-based cultures of West Francia. The distinct cultural and ethnic identity of the Normans emerged initially in the first half of the 10th century, and it continued to evolve over the succeeding centuries.  [QUESTION]: Which culture was the Normans descended from? <|endoftext|>\n",
      "<|startoftext|>\n",
      "[CONTEXT]: The Normans (Norman: Nourmands; French: Normands; Latin: Normanni) were the people who in the 10th and 11th centuries gave their name to Normandy, a region in France. They were descended from Norse (\"Norman\" comes from \"Norseman\") raiders and pirates from Denmark, Iceland and Norway who, under their leader Rollo, agreed to swear fealty to King Charles III of West Francia. Through generations of assimilation and mixing with the native Frankish and Roman-Gaulish populations, their descendants would gradually merge with the Carolingian-based cultures of West Francia. The distinct cultural and ethnic identity of the Normans emerged initially in the first half of the 10th century, and it continued to evolve over the succeeding centuries.  [QUESTION]: When the Normans were from the Norse, what was their name? <|endoftext|>\n",
      "<|startoftext|>\n",
      "[CONTEXT]: The Normans (Norman: Nourmands; French: Normanni) were the people who in the 10th and 11th centuries gave their name to Normandy, a region in France. They were descended from Norse (\"Norseman\") raiders and pirates from Denmark, Iceland and Norway who, under their leader Rollo, agreed to swear fealty to King Charles III of West Francia. Through generations of assimilation and mixing with the native Frankish and Roman-Gaulish populations, their descendants would gradually merge with the Carolingian-based cultures of West Francia. The distinct cultural and ethnic identity of the Normans emerged initially in the first half of the 10th century, and it continued to evolve over the succeeding centuries.  [QUESTION]: What was the name of the name of the Normans? <|endoftext|>\n",
      "<|startoftext|>\n",
      "[CONTEXT]: The Normans (Norman: Nourmands; French: Normanni) were the people who in the 10th and 11th\n",
      "====================\n",
      "<|startoftext|>\n",
      "[CONTEXT]: The Normans (Norman: Nourmands; French: Normands; Latin: Normanni) were the people who in the 10th and 11th centuries gave their name to Normandy, a region in France. They were descended from Norse (\"Norman\" comes from \"Norseman\") raiders and pirates from Denmark, Iceland and Norway who, under their leader Rollo, agreed to swear fealty to King Charles III of West Francia. Through generations of assimilation and mixing with the native Frankish and Roman-Gaulish populations, their descendants would gradually merge with the Carolingian-based cultures of West Francia. The distinct cultural and ethnic identity of the Normans emerged initially in the first half of the 10th century, and it continued to evolve over the succeeding centuries.  [QUESTION]: What did the Normans give their name to? <|endoftext|>\n",
      "<|startoftext|>\n",
      "[CONTEXT]: The Normans (Norman: Nourmands; French: Normands; Latin: Normanni) were the people who in the 10th and 11th centuries gave their name to Normandy, a region in France. They were descended from Norse (\"Norman\" comes from \"Norseman\") raiders and pirates from Denmark, Iceland and Norway who, under their leader Rollo, agreed to swear fealty to King Charles III of West Francia. Through generations of assimilation and mixing with the native Frankish and Roman-Gaulish populations, their descendants would gradually merge with the Carolingian-based cultures of West Francia. The distinct cultural and ethnic identity of the Normans emerged initially in the first half of the 10th century, and it continued to evolve over the succeeding centuries.  [QUESTION]: Who was the leader of Rollo? <|endoftext|>\n",
      "<|startoftext|>\n",
      "[CONTEXT]: The Normans (Norman: Nourmands; French: Normands; Latin: Normanni) were the people who in the 10th and 11th centuries gave their name to Normandy, a region in France. They were descended from Norse (\"Norman\" comes from \"Norseman\") raiders and pirates from Denmark, Iceland and Norway who, under their leader Rollo, agreed to swear fealty to King Charles III of West Francia. Through generations of assimilation and mixing with the native Frankish and Roman-Gaulish populations, their descendants would gradually merge with the Carolingian-based cultures of West Francia. The distinct cultural and ethnic identity of the Normans emerged initially in the first half of the 10th century, and it continued to evolve over the succeeding centuries.  [QUESTION]: What was the Normans' origins? <|endoftext|>\n",
      "<|startoftext|>\n",
      "[CONTEXT]: The Normans (Norman: Nourmands; French: Normands; Latin: Normanni) were the people who in the 10th and 11th centuries gave their name to Normandy, a region in France. They were descended from Norse (\"Norman\" comes from \"Norseman\") raiders and pirates from Denmark, Iceland and Norway who, under their leader Rollo, agreed to swear fealty to King Charles III of West Francia. Through generations of assimilation and mixing with the native Frankish and Roman-Gaulish populations, their descendants would gradually merge with the Carolingian-based cultures of West Francia. The distinct cultural and ethnic identity of the Normans emerged initially in the first half of the 10th century, and it continued to evolve over the succeeding centuries.  [QUESTION]: What is the Normans' fyeinglange? <|endoftext|>\n",
      "<|startoftext|>\n",
      "[CONTEXT]: The Normans (Norman: Nourmands; French: Normanni) were the people who in the 10th and 11th centuries gave their name to Normandy, a region in France. They were descended from Norse (\"Norman\" comes from \"Norseman\") raiders and pirates from Denmark, Iceland and Norway who, under their leader Rollo, agreed to swear fealty to King Charles III of West Francia. Through generations of assimilation and mixing with the native Frankish and Roman-Gaulish populations, their descendants would gradually merge with the Carolingian-based cultures of West Francia. The distinct cultural and ethnic identity of the Normans emerged initially in the first half of the 10th century, and it continued to evolve over the succeeding centuries.  [QUESTION]: When was the Normans' fyeinglange? <|endoftext|>\n",
      "<|startoftext|>\n",
      "[CONTEXT]: The Normans (Norman: Nourmands; French: Normanni) were the people who in the 10th and\n",
      "====================\n",
      "<|startoftext|>\n",
      "[CONTEXT]: The Normans (Norman: Nourmands; French: Normands; Latin: Normanni) were the people who in the 10th and 11th centuries gave their name to Normandy, a region in France. They were descended from Norse (\"Norman\" comes from \"Norseman\") raiders and pirates from Denmark, Iceland and Norway who, under their leader Rollo, agreed to swear fealty to King Charles III of West Francia. Through generations of assimilation and mixing with the native Frankish and Roman-Gaulish populations, their descendants would gradually merge with the Carolingian-based cultures of West Francia. The distinct cultural and ethnic identity of the Normans emerged initially in the first half of the 10th century, and it continued to evolve over the succeeding centuries.  [QUESTION]: Which group of people gave their name to Normandy? <|endoftext|>\n",
      "<|startoftext|>\n",
      "[CONTEXT]: The Normans (Norman: Nourmands; French: Normands; Latin: Normanni) were the people who in the 10th and 11th centuries gave their name to Normandy, a region in France. They were descended from Norse (\"Norman\" comes from \"Norseman\") raiders and pirates from Denmark, Iceland and Norway who, under their leader Rollo, agreed to swear fealty to King Charles III of West Francia. Through generations of assimilation and mixing with the native Frankish and Roman-Gaulish populations, their descendants would gradually merge with the Carolingian-based cultures of West Francia. The distinct cultural and ethnic identity of the Normans emerged initially in the first half of the 10th century, and it continued to evolve over the succeeding centuries.  [QUESTION]: Who is the leader from Denmark? <|endoftext|>\n",
      "<|startoftext|>\n",
      "[CONTEXT]: The Normans (Norman: Nourmands; French: Normands; Latin: Normanni) were the people who in the 10th and 11th centuries gave their name to Normandy, a region in France. They were descended from Norse (\"Norman\" comes from \"Norseman\") raiders and pirates from Denmark, Iceland and Norway who, under their leader Rollo, agreed to swear fealty to King Charles III of West Francia. Through generations of assimilation and mixing with the native Frankish and Roman-Gaulish populations, their descendants would gradually merge with the Carolingian-based cultures of West Francia. The distinct cultural and ethnic identity of the Normans emerged initially in the first half of the 10th century, and it continued to evolve over the succeeding centuries.  [QUESTION]: What language is Normans' language in? <|endoftext|>\n",
      "<|startoftext|>\n",
      "[CONTEXT]: The Normans (Norman: Nourmands; French: Normants; Latin: Normanni) were the people who in the 10th and 11th centuries gave their name to Normandy, a region in France. They were descended from Norse (\"Norman\" comes from \"Norseman\") raiders and pirates from Denmark, Iceland and Norway who, under their leader Rollo, agreed to swear fealty to King Charles III of West Francia. Through generations of assimilation and mixing with the native Frankish and Roman-Gaulish populations, their descendants would gradually merge with the Carolingian-based cultures of West Francia. The distinct cultural and ethnic identity of the Normans emerged initially in the first half of the 10th century, and it continued to evolve over the succeeding centuries.  [QUESTION]: Who would the Normans' descendants be? <|endoftext|>\n",
      "<|startoftext|>\n",
      "[CONTEXT]: The Normans (Norman: Nourmands; French: Normants; Latin: Normanni) were the people who in the 10th and 11th centuries gave their name to Normandy, a region in France. They were descended from Norse (\"Norman\" comes from \"Norseman\") raiders and pirates from Denmark, Iceland and Norway who, under their leader Rollo, agreed to swear fealty to King Charles III of West Francia. Through generations of assimilation and mixing with the native Frankish and Roman-Gaulish populations, their descendants would gradually merge with the Carolingian-based cultures of West Francia. The distinct cultural and ethnic identity of the Normans emerged initially in the first half of the 10th century, and it continued to evolve over the succeeding centuries.  [QUESTION]: Which language is the Normans' language in? <|endoftext|>\n",
      "<|startoftext|>\n",
      "[CONTEXT]: The Normans (Norman: Nourmands; French: Normants; Latin: Normanni) were the people who\n",
      "====================\n",
      "<|startoftext|>\n",
      "[CONTEXT]: The Normans (Norman: Nourmands; French: Normands; Latin: Normanni) were the people who in the 10th and 11th centuries gave their name to Normandy, a region in France. They were descended from Norse (\"Norman\" comes from \"Norseman\") raiders and pirates from Denmark, Iceland and Norway who, under their leader Rollo, agreed to swear fealty to King Charles III of West Francia. Through generations of assimilation and mixing with the native Frankish and Roman-Gaulish populations, their descendants would gradually merge with the Carolingian-based cultures of West Francia. The distinct cultural and ethnic identity of the Normans emerged initially in the first half of the 10th century, and it continued to evolve over the succeeding centuries.  [QUESTION]: What did the Normans their names of Normandie and Normandie derive from? <|endoftext|>\n",
      "<|startoftext|>\n",
      "[CONTEXT]: The Normans (Norman: Nourmands; French: Normands; Latin: Normanni) were the people who in the 10th and 11th centuries gave their name to Normandy, a region in France. They were descended from Norse (\"Norman\" comes from \"Norseman\") raiders and pirates from Denmark, Iceland and Norway who, under their leader Rollo, agreed to swear fealty to King Charles III of West Francia. Through generations of assimilation and mixing with the native Frankish and Roman-Gaulish populations, their descendants would gradually merge with the Carolingian-based cultures of West Francia. The distinct cultural and ethnic identity of the Normans emerged initially in the first half of the 10th century, and it continued to evolve over the succeeding centuries.  [QUESTION]: What was the leader of the Normans? <|endoftext|>\n",
      "<|startoftext|>\n",
      "[CONTEXT]: The Normans (Norman: Nourmands; French: Normands; Latin: Normanni) were the people who in the 10th and 11th centuries gave their name to Normandy, a region in France. They were descended from Norse (\"Norman\" comes from \"Norseman\") raiders and pirates from Denmark, Iceland and Norway who, under their leader Rollo, agreed to swear fealty to King Charles III of West Francia. Through generations of assimilation and mixing with the native Frankish and Roman-Gaulish populations, their descendants would gradually merge with the Carolingian-based cultures of West Francia. The distinct cultural and ethnic identity of the Normans emerged initially in the first half of the 10th century, and it continued to evolve over the succeeding centuries.  [QUESTION]: What were the Normans called by the Normans? <|endoftext|>\n",
      "<|startoftext|>\n",
      "[CONTEXT]: The Normans (Norman: Nourmands; French: Normands; Latin: Normanni) were the people who in the 10th and 11th centuries gave their name to Normandy, a region in France. They were descended from Norse (\"Norman\" comes from \"Norseman\") raiders and pirates from Denmark, Iceland and Norway who, under their leader Rollo, agreed to swear fealty to King Charles III of West Francia. Through generations of assimilation and mixing with the native Frankish and Roman-Gaulish populations, their descendants would gradually merge with the Carolingian-based cultures of West Francia. The distinct cultural and ethnic identity of the Normans emerged initially in the first half of the 10th century, and it continued to evolve over the succeeding centuries.  [QUESTION]: Where did the Normans from Normandie come from? <|endoftext|>\n",
      "<|startoftext|>\n",
      "[CONTEXT]: The Normans (Norman: Nourmands; French: Normandies; Latin: Normanni) were the people who in the 10th and 11th centuries gave their name to Normandy, a region in France. They were descended from Norse (\"Norman\" comes from \"Norseman\") raiders and pirates from Denmark, Iceland and Norway who, under their leader Rollo, agreed to swear fealty to King Charles III of West Francia. Through generations of assimilation and mixing with the native Frankish and Roman-Gaulish populations, their descendants would gradually merge with the Carolingian-based cultures of West Francia. The distinct cultural and ethnic identity of the Normans emerged initially in the first half of the 10th century, and it continued to evolve over the succeeding centuries.  [QUESTION]: What was the name of the Normans before they came to the present state? <|endoftext|>\n",
      "<|startoftext|>\n",
      "[CONTEXT]: The Normans (\n",
      "====================\n",
      "<|startoftext|>\n",
      "[CONTEXT]: The Normans (Norman: Nourmands; French: Normands; Latin: Normanni) were the people who in the 10th and 11th centuries gave their name to Normandy, a region in France. They were descended from Norse (\"Norman\" comes from \"Norseman\") raiders and pirates from Denmark, Iceland and Norway who, under their leader Rollo, agreed to swear fealty to King Charles III of West Francia. Through generations of assimilation and mixing with the native Frankish and Roman-Gaulish populations, their descendants would gradually merge with the Carolingian-based cultures of West Francia. The distinct cultural and ethnic identity of the Normans emerged initially in the first half of the 10th century, and it continued to evolve over the succeeding centuries.  [QUESTION]: What was the name of the Normans? <|endoftext|>\n",
      "<|startoftext|>\n",
      "[CONTEXT]: The Normans (Norman: Nourmands; French: Normands; Latin: Normanni) were the people who in the 10th and 11th centuries gave their name to Normandy, a region in France. They were descended from Norse (\"Norman\" comes from \"Norseman\") raiders and pirates from Denmark, Iceland and Norway who, under their leader Rollo, agreed to swear fealty to King Charles III of West Francia. Through generations of assimilation and mixing with the native Frankish and Roman-Gaulish populations, their descendants would gradually merge with the Carolingian-based cultures of West Francia. The distinct cultural and ethnic identity of the Normans emerged initially in the first half of the 10th century, and it continued to evolve over the succeeding centuries.  [QUESTION]: Who was Rollo? <|endoftext|>\n",
      "<|startoftext|>\n",
      "[CONTEXT]: The Normans (Norman: Nourmands; French: Normands; Latin: Normanni) were the people who in the 10th and 11th centuries gave their name to Normandy, a region in France. They were descended from Norse (\"Norman\" comes from \"Norseman\") raiders and pirates from Denmark, Iceland and Norway who, under their leader Rollo, agreed to swear fealty to King Charles III of West Francia. Through generations of assimilation and mixing with the native Frankish and Roman-Gaulish populations, their descendants would gradually merge with the Carolingian-based cultures of West Francia. The distinct cultural and ethnic identity of the Normans emerged initially in the first half of the 10th century, and it continued to evolve over the succeeding centuries.  [QUESTION]: What language was the Normans from? <|endoftext|>\n",
      "<|startoftext|>\n",
      "[CONTEXT]: The Normans (Norman: Nourmands; French: Normands; Latin: Normanni) were the people who in the 10th and 11th centuries gave their name to Normandy, a region in France. They were descended from Norse (\"Norman\" comes from \"Norseman\") raiders and pirates from Denmark, Iceland and Norway who, under their leader Rollo, agreed to swear fealty to King Charles III of West Francia. Through generations of assimilation and mixing with the native Frankish and Roman-Gaulish populations, their descendants would gradually merge with the Carolingian-based cultures of West Francia. The distinct cultural and ethnic identity of the Normans emerged initially in the first half of the 10th century, and it continued to evolve over the succeeding centuries.  [QUESTION]: What language was the Normans from? <|endoftext|>\n",
      "<|startoftext|>\n",
      "[CONTEXT]: The Normans (Norman: Nourmands; French: Normanni) were the people who in the 10th and 11th centuries gave their name to Normandy, a region in France. They were descended from Norse (\"Norman\" comes from \"Norseman\") raiders and pirates from Denmark, Iceland and Norway who, under their leader Rollo, agreed to swear fealty to King Charles III of West Francia. Through generations of assimilation and mixing with the native Frankish and Roman-Gaulish populations, their descendants would gradually merge with the Carolingian-based cultures of West Francia. The distinct cultural and ethnic identity of the Normans emerged initially in the first half of the 10th century, and it continued to evolve over the succeeding centuries.  [QUESTION]: What was the Normans' culture? <|endoftext|>\n",
      "<|startoftext|>\n",
      "[CONTEXT]: The Normans (Norman: Nourmands; French: Normanni) were the people who in the 10th and 11th centuries gave their name to Normandy, a region in\n",
      "====================\n",
      "<|startoftext|>\n",
      "[CONTEXT]: The Normans (Norman: Nourmands; French: Normands; Latin: Normanni) were the people who in the 10th and 11th centuries gave their name to Normandy, a region in France. They were descended from Norse (\"Norman\" comes from \"Norseman\") raiders and pirates from Denmark, Iceland and Norway who, under their leader Rollo, agreed to swear fealty to King Charles III of West Francia. Through generations of assimilation and mixing with the native Frankish and Roman-Gaulish populations, their descendants would gradually merge with the Carolingian-based cultures of West Francia. The distinct cultural and ethnic identity of the Normans emerged initially in the first half of the 10th century, and it continued to evolve over the succeeding centuries.  [QUESTION]: What language did the Normans speak? <|endoftext|>\n",
      "<|startoftext|>\n",
      "[CONTEXT]: The Normans (Norman: Nourmands; French: Normands; Latin: Normanni) were the people who in the 10th and 11th centuries gave their name to Normandy, a region in France. They were descended from Norse (\"Norman\" comes from \"Norseman\") raiders and pirates from Denmark, Iceland and Norway who, under their leader Rollo, agreed to swear fealty to King Charles III of West Francia. Through generations of assimilation and mixing with the native Frankish and Roman-Gaulish populations, their descendants would gradually merge with the Carolingian-based cultures of West Francia. The distinct cultural and ethnic identity of the Normans emerged initially in the first half of the 10th century, and it continued to evolve over the succeeding centuries.  [QUESTION]: What language was the Normans descended from? <|endoftext|>\n",
      "<|startoftext|>\n",
      "[CONTEXT]: The Normans (Norman: Nourmands; French: Normands; Latin: Normanni) were the people who in the 10th and 11th centuries gave their name to Normandy, a region in France. They were descended from Norse (\"Norman\" comes from \"Norseman\") raiders and pirates from Denmark, Iceland and Norway who, under their leader Rollo, agreed to swear fealty to King Charles III of West Francia. Through generations of assimilation and mixing with the native Frankish and Roman-Gaulish populations, their descendants would gradually merge with the Carolingian-based cultures of West Francia. The distinct cultural and ethnic identity of the Normans emerged initially in the first half of the 10th century, and it continued to evolve over the succeeding centuries.  [QUESTION]: What was the name of Rollo's brother? <|endoftext|>\n",
      "<|startoftext|>\n",
      "[CONTEXT]: The Normans (Norman: Nourmands; French: Normands; Latin: Normanni) were the people who in the 10th and 11th centuries gave their name to Normandy, a region in France. They were descended from Norse (\"Norman\" comes from \"Norseman\") raiders and pirates from Denmark, Iceland and Norway who, under their leader Rollo, agreed to swear fealty to King Charles III of West Francia. Through generations of assimilation and mixing with the native Frankish and Roman-Gaulish populations, their descendants would gradually merge with the Carolingian-based cultures of West Francia. The distinct cultural and ethnic identity of the Normans emerged initially in the first half of the 10th century, and it continued to evolve over the succeeding centuries.  [QUESTION]: What linguistics was applied to the Normans? <|endoftext|>\n",
      "<|startoftext|>\n",
      "[CONTEXT]: The Normans (Norman: Nourmands; French: Normanni) were the people who in the 10th and 11th centuries gave their name to Normandy, a region in France. They were descended from Norse (\"Norman\" comes from \"Norseman\") raiders and pirates from Denmark, Iceland and Norway who, under their leader Rollo, agreed to swear fealty to King Charles III of West Francia. Through generations of assimilation and mixing with the native Frankish and Roman-Gaulish populations, their descendants would gradually merge with the Carolingian-based cultures of West Francia. The distinct cultural and ethnic identity of the Normans emerged initially in the first half of the 10th century, and it continued to evolve over the succeeding centuries.  [QUESTION]: Where did the Normans come from? <|endoftext|>\n",
      "<|startoftext|>\n",
      "[CONTEXT]: The Normans (Norman: Nourmands; French: Normanni) were the people who in the 10th and 11th centuries gave their\n",
      "====================\n"
     ]
    }
   ],
   "source": [
    "gpt2.generate(sess,\n",
    "              temperature=0.7,\n",
    "              prefix=pre,\n",
    "              nsamples=10,\n",
    "              batch_size=10,\n",
    "              run_name=\"QGen_SQUAD_test\",\n",
    "              )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### QuAC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"345M\"\n",
    "if not os.path.isdir(os.path.join(\"models\", model_name)):\n",
    "    print(f\"Downloading {model_name} model...\")\n",
    "    gpt2.download_gpt2(model_name=model_name)   # model is saved into current directory under /models/<size>/\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "qsess = gpt2.start_tf_sess()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/gpt_2_simple/src/sample.py:17: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "WARNING:tensorflow:From /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/gpt_2_simple/src/memory_saving_gradients.py:62: get_backward_walk_ops (from tensorflow.contrib.graph_editor.select) is deprecated and will be removed after 2019-06-06.\n",
      "Instructions for updating:\n",
      "Please use tensorflow.python.ops.op_selector.get_backward_walk_ops.\n",
      "Loading checkpoint models/345M/model.ckpt\n",
      "INFO:tensorflow:Restoring parameters from models/345M/model.ckpt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [03:56<00:00, 236.68s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset has 46016379 tokens\n",
      "Training...\n",
      "[1 | 22.46] loss=1.58 avg=1.58\n",
      "[2 | 32.63] loss=1.71 avg=1.65\n",
      "[3 | 43.03] loss=1.58 avg=1.63\n",
      "[4 | 53.19] loss=1.34 avg=1.55\n",
      "[5 | 63.32] loss=1.75 avg=1.59\n",
      "[6 | 73.38] loss=2.57 avg=1.76\n",
      "[7 | 83.41] loss=1.17 avg=1.67\n",
      "[8 | 93.47] loss=1.53 avg=1.65\n",
      "[9 | 103.46] loss=1.81 avg=1.67\n",
      "[10 | 113.49] loss=1.83 avg=1.69\n",
      "[11 | 123.59] loss=1.18 avg=1.64\n",
      "[12 | 133.72] loss=2.77 avg=1.74\n",
      "[13 | 143.79] loss=1.02 avg=1.68\n",
      "[14 | 153.88] loss=1.45 avg=1.66\n",
      "[15 | 163.96] loss=1.92 avg=1.68\n",
      "[16 | 174.15] loss=2.63 avg=1.75\n",
      "[17 | 184.27] loss=1.06 avg=1.70\n",
      "[18 | 194.36] loss=2.36 avg=1.74\n",
      "[19 | 204.43] loss=1.74 avg=1.74\n",
      "[20 | 214.54] loss=2.08 avg=1.76\n",
      "[21 | 225.03] loss=1.02 avg=1.72\n",
      "[22 | 235.13] loss=1.33 avg=1.70\n",
      "[23 | 245.28] loss=1.18 avg=1.68\n",
      "[24 | 255.32] loss=1.65 avg=1.68\n",
      "[25 | 265.42] loss=2.41 avg=1.71\n",
      "[26 | 275.45] loss=2.47 avg=1.74\n",
      "[27 | 285.43] loss=1.43 avg=1.73\n",
      "[28 | 295.48] loss=1.81 avg=1.73\n",
      "[29 | 305.54] loss=1.21 avg=1.71\n",
      "[30 | 315.59] loss=1.28 avg=1.70\n",
      "[31 | 325.64] loss=2.16 avg=1.71\n",
      "[32 | 335.65] loss=1.23 avg=1.69\n",
      "[33 | 345.58] loss=2.01 avg=1.71\n",
      "[34 | 355.63] loss=1.99 avg=1.72\n",
      "[35 | 365.71] loss=1.20 avg=1.70\n",
      "[36 | 375.75] loss=2.50 avg=1.72\n",
      "[37 | 385.80] loss=2.20 avg=1.74\n",
      "[38 | 395.83] loss=2.17 avg=1.75\n",
      "[39 | 406.19] loss=2.34 avg=1.77\n",
      "[40 | 416.20] loss=1.31 avg=1.76\n",
      "[41 | 426.22] loss=3.07 avg=1.80\n",
      "[42 | 436.31] loss=2.03 avg=1.80\n",
      "[43 | 446.36] loss=2.93 avg=1.84\n",
      "[44 | 456.36] loss=1.35 avg=1.82\n",
      "[45 | 466.34] loss=1.50 avg=1.81\n",
      "[46 | 476.39] loss=1.33 avg=1.80\n",
      "[47 | 486.44] loss=1.80 avg=1.80\n",
      "[48 | 496.48] loss=2.90 avg=1.83\n",
      "[49 | 506.52] loss=1.55 avg=1.82\n",
      "[50 | 516.58] loss=1.69 avg=1.82\n",
      "[51 | 526.63] loss=2.38 avg=1.83\n",
      "[52 | 536.63] loss=1.70 avg=1.83\n",
      "[53 | 546.58] loss=2.80 avg=1.85\n",
      "[54 | 556.58] loss=1.44 avg=1.84\n",
      "[55 | 566.70] loss=2.46 avg=1.86\n",
      "[56 | 576.88] loss=2.42 avg=1.87\n",
      "[57 | 587.09] loss=2.99 avg=1.90\n",
      "[58 | 597.06] loss=1.17 avg=1.88\n",
      "[59 | 607.08] loss=1.88 avg=1.88\n",
      "[60 | 617.02] loss=2.73 avg=1.90\n",
      "[61 | 627.01] loss=2.39 avg=1.91\n",
      "[62 | 637.03] loss=2.22 avg=1.92\n",
      "[63 | 647.03] loss=1.41 avg=1.91\n",
      "[64 | 657.02] loss=1.33 avg=1.89\n",
      "[65 | 667.09] loss=1.55 avg=1.89\n",
      "[66 | 677.13] loss=2.10 avg=1.89\n",
      "[67 | 687.12] loss=2.17 avg=1.90\n",
      "[68 | 697.13] loss=1.39 avg=1.89\n",
      "[69 | 707.14] loss=2.28 avg=1.89\n",
      "[70 | 717.13] loss=2.39 avg=1.90\n",
      "[71 | 727.31] loss=1.84 avg=1.90\n",
      "[72 | 737.33] loss=1.41 avg=1.89\n",
      "[73 | 747.38] loss=2.52 avg=1.90\n",
      "[74 | 757.57] loss=0.98 avg=1.89\n",
      "[75 | 767.76] loss=2.23 avg=1.89\n",
      "[76 | 777.76] loss=1.32 avg=1.88\n",
      "[77 | 787.86] loss=1.28 avg=1.87\n",
      "[78 | 797.88] loss=1.61 avg=1.87\n",
      "[79 | 807.90] loss=3.06 avg=1.89\n",
      "[80 | 817.92] loss=1.59 avg=1.88\n",
      "[81 | 828.01] loss=2.51 avg=1.89\n",
      "[82 | 838.08] loss=1.37 avg=1.89\n",
      "[83 | 848.09] loss=2.06 avg=1.89\n",
      "[84 | 858.26] loss=1.62 avg=1.88\n",
      "[85 | 868.33] loss=1.38 avg=1.88\n",
      "[86 | 878.47] loss=1.48 avg=1.87\n",
      "[87 | 888.54] loss=1.31 avg=1.86\n",
      "[88 | 898.58] loss=1.66 avg=1.86\n",
      "[89 | 908.65] loss=1.41 avg=1.85\n",
      "[90 | 918.73] loss=1.20 avg=1.84\n",
      "[91 | 928.73] loss=1.68 avg=1.83\n",
      "[92 | 939.09] loss=1.52 avg=1.83\n",
      "[93 | 949.22] loss=1.92 avg=1.83\n",
      "[94 | 959.27] loss=1.44 avg=1.82\n",
      "[95 | 969.33] loss=2.11 avg=1.83\n",
      "[96 | 979.37] loss=1.07 avg=1.82\n",
      "[97 | 989.41] loss=0.84 avg=1.80\n",
      "[98 | 999.39] loss=2.08 avg=1.81\n",
      "[99 | 1009.38] loss=2.69 avg=1.82\n",
      "[100 | 1019.42] loss=2.36 avg=1.83\n",
      "Saving checkpoint/QGen_QUAC_test/model-100\n",
      "[101 | 1033.04] loss=2.04 avg=1.83\n",
      "[102 | 1042.99] loss=1.87 avg=1.83\n",
      "[103 | 1052.98] loss=1.67 avg=1.83\n",
      "[104 | 1063.02] loss=2.18 avg=1.83\n",
      "[105 | 1073.08] loss=2.22 avg=1.84\n",
      "[106 | 1083.08] loss=1.87 avg=1.84\n",
      "[107 | 1093.02] loss=1.19 avg=1.83\n",
      "[108 | 1103.01] loss=2.71 avg=1.84\n",
      "[109 | 1113.04] loss=3.28 avg=1.87\n",
      "[110 | 1123.31] loss=2.76 avg=1.88\n",
      "[111 | 1133.36] loss=1.28 avg=1.87\n",
      "[112 | 1143.38] loss=1.96 avg=1.87\n",
      "[113 | 1153.39] loss=1.27 avg=1.86\n",
      "[114 | 1163.43] loss=2.13 avg=1.87\n",
      "[115 | 1173.44] loss=2.08 avg=1.87\n",
      "[116 | 1183.44] loss=1.79 avg=1.87\n",
      "[117 | 1193.41] loss=1.53 avg=1.86\n",
      "[118 | 1203.43] loss=1.30 avg=1.86\n",
      "[119 | 1213.37] loss=1.19 avg=1.85\n",
      "[120 | 1223.41] loss=2.48 avg=1.86\n",
      "[121 | 1233.44] loss=1.68 avg=1.85\n",
      "[122 | 1243.44] loss=1.49 avg=1.85\n",
      "[123 | 1253.45] loss=1.41 avg=1.84\n",
      "[124 | 1263.42] loss=1.53 avg=1.84\n",
      "[125 | 1273.45] loss=2.55 avg=1.85\n",
      "[126 | 1283.45] loss=1.60 avg=1.84\n",
      "[127 | 1293.47] loss=0.95 avg=1.83\n",
      "[128 | 1303.83] loss=1.05 avg=1.82\n",
      "[129 | 1313.89] loss=2.78 avg=1.83\n",
      "[130 | 1323.97] loss=1.55 avg=1.83\n",
      "[131 | 1334.01] loss=1.31 avg=1.82\n",
      "[132 | 1344.08] loss=2.92 avg=1.84\n",
      "[133 | 1354.14] loss=2.18 avg=1.84\n",
      "[134 | 1364.13] loss=3.16 avg=1.86\n",
      "[135 | 1374.07] loss=2.08 avg=1.86\n",
      "[136 | 1384.05] loss=1.91 avg=1.86\n",
      "[137 | 1394.04] loss=1.85 avg=1.86\n",
      "[138 | 1404.06] loss=3.42 avg=1.88\n",
      "[139 | 1414.03] loss=1.70 avg=1.88\n",
      "[140 | 1424.05] loss=1.52 avg=1.88\n",
      "[141 | 1434.05] loss=1.27 avg=1.87\n",
      "[142 | 1444.06] loss=1.27 avg=1.86\n",
      "[143 | 1454.02] loss=1.35 avg=1.85\n",
      "[144 | 1463.99] loss=1.15 avg=1.85\n",
      "[145 | 1474.02] loss=2.52 avg=1.85\n",
      "[146 | 1484.31] loss=1.45 avg=1.85\n",
      "[147 | 1494.31] loss=3.05 avg=1.86\n",
      "[148 | 1504.33] loss=1.58 avg=1.86\n",
      "[149 | 1514.29] loss=2.20 avg=1.87\n",
      "[150 | 1524.28] loss=2.53 avg=1.87\n",
      "[151 | 1534.28] loss=1.76 avg=1.87\n",
      "[152 | 1544.24] loss=1.77 avg=1.87\n",
      "[153 | 1554.22] loss=0.82 avg=1.86\n",
      "[154 | 1564.19] loss=1.86 avg=1.86\n",
      "[155 | 1574.20] loss=1.68 avg=1.86\n",
      "[156 | 1584.18] loss=1.42 avg=1.85\n",
      "[157 | 1594.21] loss=1.85 avg=1.85\n",
      "[158 | 1604.22] loss=2.31 avg=1.86\n",
      "[159 | 1614.26] loss=2.88 avg=1.87\n",
      "[160 | 1624.20] loss=1.90 avg=1.87\n",
      "[161 | 1634.17] loss=1.89 avg=1.87\n",
      "[162 | 1644.14] loss=2.13 avg=1.87\n",
      "[163 | 1654.11] loss=3.07 avg=1.89\n",
      "[164 | 1664.40] loss=2.26 avg=1.89\n",
      "[165 | 1674.37] loss=2.36 avg=1.90\n",
      "[166 | 1684.33] loss=1.38 avg=1.89\n",
      "[167 | 1694.33] loss=1.80 avg=1.89\n",
      "[168 | 1704.33] loss=1.29 avg=1.88\n",
      "[169 | 1714.36] loss=1.34 avg=1.88\n",
      "[170 | 1724.32] loss=1.09 avg=1.87\n",
      "[171 | 1734.29] loss=1.54 avg=1.86\n",
      "[172 | 1744.34] loss=3.21 avg=1.88\n",
      "[173 | 1754.35] loss=1.55 avg=1.87\n",
      "[174 | 1764.38] loss=1.70 avg=1.87\n",
      "[175 | 1774.39] loss=2.78 avg=1.88\n",
      "[176 | 1784.37] loss=2.43 avg=1.89\n",
      "[177 | 1794.38] loss=2.69 avg=1.90\n",
      "[178 | 1804.38] loss=1.23 avg=1.89\n",
      "[179 | 1814.35] loss=1.55 avg=1.89\n",
      "[180 | 1824.37] loss=1.15 avg=1.88\n",
      "[181 | 1834.39] loss=1.36 avg=1.87\n",
      "[182 | 1844.69] loss=1.11 avg=1.86\n",
      "[183 | 1854.77] loss=1.30 avg=1.86\n",
      "[184 | 1864.76] loss=2.29 avg=1.86\n",
      "[185 | 1874.71] loss=1.77 avg=1.86\n",
      "[186 | 1884.70] loss=2.56 avg=1.87\n",
      "[187 | 1894.71] loss=1.49 avg=1.86\n",
      "[188 | 1904.69] loss=1.75 avg=1.86\n",
      "[189 | 1914.68] loss=1.09 avg=1.85\n",
      "[190 | 1924.69] loss=3.37 avg=1.87\n",
      "[191 | 1934.72] loss=1.44 avg=1.87\n",
      "[192 | 1944.67] loss=1.36 avg=1.86\n",
      "[193 | 1954.67] loss=1.25 avg=1.85\n",
      "[194 | 1964.67] loss=3.09 avg=1.87\n",
      "[195 | 1974.65] loss=1.41 avg=1.86\n",
      "[196 | 1984.62] loss=3.16 avg=1.88\n",
      "[197 | 1994.67] loss=1.26 avg=1.87\n",
      "[198 | 2004.64] loss=2.28 avg=1.88\n",
      "[199 | 2014.63] loss=1.70 avg=1.87\n",
      "[200 | 2024.96] loss=1.71 avg=1.87\n",
      "Saving checkpoint/QGen_QUAC_test/model-200\n",
      "WARNING:tensorflow:From /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/tensorflow_core/python/training/saver.py:963: remove_checkpoint (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use standard file APIs to delete files with this prefix.\n",
      "[201 | 2037.99] loss=1.41 avg=1.87\n",
      "[202 | 2048.02] loss=2.14 avg=1.87\n",
      "[203 | 2058.13] loss=2.75 avg=1.88\n",
      "[204 | 2068.31] loss=2.13 avg=1.88\n",
      "[205 | 2078.34] loss=2.54 avg=1.89\n",
      "[206 | 2088.39] loss=1.67 avg=1.89\n",
      "[207 | 2098.47] loss=1.51 avg=1.88\n",
      "[208 | 2108.52] loss=2.16 avg=1.89\n",
      "[209 | 2118.58] loss=1.74 avg=1.88\n",
      "[210 | 2128.69] loss=1.58 avg=1.88\n",
      "[211 | 2138.71] loss=1.53 avg=1.88\n",
      "[212 | 2148.75] loss=2.08 avg=1.88\n",
      "[213 | 2158.80] loss=1.43 avg=1.87\n",
      "[214 | 2168.90] loss=2.74 avg=1.88\n",
      "[215 | 2178.97] loss=1.50 avg=1.88\n",
      "[216 | 2189.04] loss=1.48 avg=1.88\n",
      "[217 | 2199.34] loss=1.34 avg=1.87\n",
      "[218 | 2209.44] loss=2.04 avg=1.87\n",
      "[219 | 2219.46] loss=1.44 avg=1.87\n",
      "[220 | 2229.56] loss=1.77 avg=1.87\n",
      "[221 | 2239.58] loss=2.56 avg=1.87\n",
      "[222 | 2249.60] loss=1.35 avg=1.87\n",
      "[223 | 2259.63] loss=1.13 avg=1.86\n",
      "[224 | 2269.66] loss=1.79 avg=1.86\n",
      "[225 | 2279.68] loss=2.25 avg=1.86\n",
      "[226 | 2289.75] loss=1.47 avg=1.86\n",
      "[227 | 2299.81] loss=1.32 avg=1.85\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[228 | 2309.80] loss=3.15 avg=1.87\n",
      "[229 | 2319.82] loss=1.92 avg=1.87\n",
      "[230 | 2329.83] loss=1.37 avg=1.86\n",
      "[231 | 2339.92] loss=2.16 avg=1.87\n",
      "[232 | 2349.95] loss=1.19 avg=1.86\n",
      "[233 | 2359.99] loss=2.59 avg=1.87\n",
      "[234 | 2370.00] loss=2.04 avg=1.87\n",
      "[235 | 2380.27] loss=1.57 avg=1.86\n",
      "[236 | 2390.33] loss=1.09 avg=1.86\n",
      "[237 | 2400.40] loss=2.76 avg=1.87\n",
      "[238 | 2410.45] loss=2.10 avg=1.87\n",
      "[239 | 2420.46] loss=2.65 avg=1.88\n",
      "[240 | 2430.51] loss=1.67 avg=1.87\n",
      "[241 | 2440.50] loss=1.34 avg=1.87\n",
      "[242 | 2450.48] loss=1.39 avg=1.86\n",
      "[243 | 2460.45] loss=2.18 avg=1.87\n",
      "[244 | 2470.45] loss=1.09 avg=1.86\n",
      "[245 | 2480.44] loss=2.38 avg=1.86\n",
      "[246 | 2490.51] loss=1.45 avg=1.86\n",
      "[247 | 2500.51] loss=2.31 avg=1.86\n",
      "[248 | 2510.55] loss=1.93 avg=1.87\n",
      "[249 | 2520.57] loss=3.11 avg=1.88\n",
      "[250 | 2530.59] loss=2.80 avg=1.89\n",
      "[251 | 2540.66] loss=2.41 avg=1.89\n",
      "[252 | 2550.72] loss=1.31 avg=1.89\n",
      "[253 | 2561.02] loss=1.77 avg=1.89\n",
      "[254 | 2571.09] loss=2.43 avg=1.89\n",
      "[255 | 2581.14] loss=1.11 avg=1.88\n",
      "[256 | 2591.20] loss=1.62 avg=1.88\n",
      "[257 | 2601.22] loss=1.76 avg=1.88\n",
      "[258 | 2611.33] loss=1.99 avg=1.88\n",
      "[259 | 2621.39] loss=1.81 avg=1.88\n",
      "[260 | 2631.42] loss=1.92 avg=1.88\n",
      "[261 | 2641.45] loss=1.27 avg=1.87\n",
      "[262 | 2651.44] loss=1.88 avg=1.87\n",
      "[263 | 2661.49] loss=2.27 avg=1.88\n",
      "[264 | 2671.59] loss=1.61 avg=1.88\n",
      "[265 | 2681.60] loss=2.59 avg=1.88\n",
      "[266 | 2691.58] loss=2.32 avg=1.89\n",
      "[267 | 2701.59] loss=1.50 avg=1.88\n",
      "[268 | 2711.59] loss=1.40 avg=1.88\n",
      "[269 | 2721.62] loss=1.21 avg=1.87\n",
      "[270 | 2731.65] loss=1.80 avg=1.87\n",
      "[271 | 2741.93] loss=0.97 avg=1.86\n",
      "[272 | 2751.93] loss=3.31 avg=1.88\n",
      "[273 | 2761.97] loss=2.53 avg=1.88\n",
      "[274 | 2772.00] loss=2.30 avg=1.89\n",
      "[275 | 2782.06] loss=1.09 avg=1.88\n",
      "[276 | 2792.12] loss=1.74 avg=1.88\n",
      "[277 | 2802.19] loss=1.21 avg=1.87\n",
      "[278 | 2812.23] loss=2.06 avg=1.87\n",
      "[279 | 2822.26] loss=3.29 avg=1.89\n",
      "[280 | 2832.25] loss=1.46 avg=1.88\n",
      "[281 | 2842.25] loss=2.82 avg=1.89\n",
      "[282 | 2852.30] loss=1.88 avg=1.89\n",
      "[283 | 2862.36] loss=1.70 avg=1.89\n",
      "[284 | 2872.39] loss=1.41 avg=1.89\n",
      "[285 | 2882.42] loss=1.68 avg=1.88\n",
      "[286 | 2892.43] loss=1.23 avg=1.88\n",
      "[287 | 2902.47] loss=1.68 avg=1.87\n",
      "[288 | 2912.47] loss=1.73 avg=1.87\n",
      "[289 | 2922.79] loss=1.14 avg=1.87\n",
      "[290 | 2932.79] loss=2.59 avg=1.87\n",
      "[291 | 2942.85] loss=1.82 avg=1.87\n",
      "[292 | 2952.86] loss=1.33 avg=1.87\n",
      "[293 | 2962.87] loss=2.61 avg=1.87\n",
      "[294 | 2972.89] loss=1.93 avg=1.88\n",
      "[295 | 2982.94] loss=1.24 avg=1.87\n",
      "[296 | 2992.96] loss=1.34 avg=1.86\n",
      "[297 | 3003.04] loss=1.91 avg=1.86\n",
      "[298 | 3013.10] loss=1.08 avg=1.86\n",
      "[299 | 3023.18] loss=1.27 avg=1.85\n",
      "[300 | 3033.20] loss=1.24 avg=1.84\n",
      "Saving checkpoint/QGen_QUAC_test/model-300\n",
      "[301 | 3046.18] loss=1.91 avg=1.84\n",
      "[302 | 3056.13] loss=1.13 avg=1.84\n",
      "[303 | 3066.10] loss=2.18 avg=1.84\n",
      "[304 | 3076.14] loss=2.27 avg=1.84\n",
      "[305 | 3086.18] loss=1.99 avg=1.85\n",
      "[306 | 3096.22] loss=1.58 avg=1.84\n",
      "[307 | 3106.44] loss=0.89 avg=1.83\n",
      "[308 | 3116.43] loss=1.92 avg=1.83\n",
      "[309 | 3126.38] loss=2.12 avg=1.84\n",
      "[310 | 3136.37] loss=2.01 avg=1.84\n",
      "[311 | 3146.30] loss=1.47 avg=1.83\n",
      "[312 | 3156.29] loss=1.52 avg=1.83\n",
      "[313 | 3166.28] loss=1.44 avg=1.83\n",
      "[314 | 3176.28] loss=2.02 avg=1.83\n",
      "[315 | 3186.31] loss=1.87 avg=1.83\n",
      "[316 | 3196.33] loss=0.82 avg=1.82\n",
      "[317 | 3206.32] loss=1.31 avg=1.81\n",
      "[318 | 3216.32] loss=1.57 avg=1.81\n",
      "[319 | 3226.29] loss=2.58 avg=1.82\n",
      "[320 | 3236.23] loss=3.00 avg=1.83\n",
      "[321 | 3246.21] loss=2.08 avg=1.83\n",
      "[322 | 3256.18] loss=2.75 avg=1.84\n",
      "[323 | 3266.18] loss=1.42 avg=1.84\n",
      "[324 | 3276.22] loss=3.14 avg=1.85\n",
      "[325 | 3286.49] loss=1.32 avg=1.85\n",
      "[326 | 3296.49] loss=0.96 avg=1.84\n",
      "[327 | 3306.44] loss=3.24 avg=1.85\n",
      "[328 | 3316.41] loss=2.66 avg=1.86\n",
      "[329 | 3326.39] loss=1.11 avg=1.85\n",
      "[330 | 3336.40] loss=1.70 avg=1.85\n",
      "[331 | 3346.46] loss=1.09 avg=1.84\n",
      "[332 | 3356.56] loss=2.38 avg=1.85\n",
      "[334 | 3376.57] loss=1.43 avg=1.84\n",
      "[335 | 3386.57] loss=2.82 avg=1.85\n",
      "[336 | 3396.53] loss=1.13 avg=1.84\n",
      "[337 | 3406.54] loss=1.81 avg=1.84\n",
      "[338 | 3416.51] loss=1.53 avg=1.84\n",
      "[339 | 3426.50] loss=0.93 avg=1.83\n",
      "[340 | 3436.50] loss=1.15 avg=1.82\n",
      "[341 | 3446.49] loss=1.98 avg=1.82\n",
      "[342 | 3456.60] loss=1.93 avg=1.82\n",
      "[343 | 3466.77] loss=2.48 avg=1.83\n",
      "[344 | 3476.75] loss=1.74 avg=1.83\n",
      "[345 | 3486.75] loss=1.16 avg=1.82\n",
      "[346 | 3496.69] loss=1.16 avg=1.82\n",
      "[347 | 3506.68] loss=2.46 avg=1.82\n",
      "[348 | 3516.63] loss=1.68 avg=1.82\n",
      "[349 | 3526.59] loss=1.37 avg=1.82\n",
      "[350 | 3536.57] loss=2.29 avg=1.82\n",
      "[351 | 3546.55] loss=1.50 avg=1.82\n",
      "[352 | 3556.53] loss=1.46 avg=1.81\n",
      "[353 | 3566.50] loss=1.52 avg=1.81\n",
      "[354 | 3576.48] loss=1.06 avg=1.80\n",
      "[355 | 3586.49] loss=0.98 avg=1.80\n",
      "[356 | 3596.46] loss=1.33 avg=1.79\n",
      "[357 | 3606.43] loss=1.67 avg=1.79\n",
      "[358 | 3616.41] loss=2.08 avg=1.79\n",
      "[359 | 3626.42] loss=1.81 avg=1.79\n",
      "[360 | 3636.50] loss=1.89 avg=1.79\n",
      "[361 | 3646.67] loss=3.23 avg=1.81\n",
      "[362 | 3656.66] loss=1.18 avg=1.80\n",
      "[363 | 3666.65] loss=1.81 avg=1.80\n",
      "[364 | 3676.62] loss=1.26 avg=1.80\n",
      "[365 | 3686.63] loss=1.09 avg=1.79\n",
      "[366 | 3696.61] loss=1.42 avg=1.78\n",
      "[367 | 3706.63] loss=1.53 avg=1.78\n",
      "[368 | 3716.60] loss=1.35 avg=1.78\n",
      "[369 | 3726.60] loss=2.01 avg=1.78\n",
      "[370 | 3736.58] loss=0.96 avg=1.77\n",
      "[371 | 3746.64] loss=1.38 avg=1.77\n",
      "[372 | 3756.58] loss=1.58 avg=1.77\n",
      "[373 | 3766.58] loss=1.18 avg=1.76\n",
      "[374 | 3776.58] loss=2.64 avg=1.77\n",
      "[375 | 3786.57] loss=1.27 avg=1.76\n",
      "[376 | 3796.56] loss=2.68 avg=1.77\n",
      "[377 | 3806.62] loss=1.83 avg=1.77\n",
      "[378 | 3816.76] loss=1.15 avg=1.77\n",
      "[379 | 3826.99] loss=2.21 avg=1.77\n",
      "[380 | 3836.93] loss=1.90 avg=1.77\n",
      "[381 | 3846.90] loss=2.27 avg=1.78\n",
      "[382 | 3856.90] loss=2.12 avg=1.78\n",
      "[383 | 3866.92] loss=1.42 avg=1.78\n",
      "[384 | 3876.94] loss=1.39 avg=1.77\n",
      "[385 | 3886.89] loss=1.86 avg=1.78\n",
      "[386 | 3896.89] loss=2.36 avg=1.78\n",
      "[387 | 3906.93] loss=2.10 avg=1.78\n",
      "[388 | 3916.91] loss=1.94 avg=1.79\n",
      "[389 | 3926.90] loss=1.22 avg=1.78\n",
      "[390 | 3936.87] loss=2.24 avg=1.79\n",
      "[391 | 3946.85] loss=2.35 avg=1.79\n",
      "[392 | 3956.86] loss=1.08 avg=1.78\n",
      "[393 | 3966.85] loss=1.80 avg=1.78\n",
      "[394 | 3976.81] loss=1.85 avg=1.78\n",
      "[395 | 3986.78] loss=1.78 avg=1.78\n",
      "[396 | 3996.92] loss=1.43 avg=1.78\n",
      "[397 | 4007.09] loss=3.52 avg=1.80\n",
      "[398 | 4017.05] loss=1.61 avg=1.80\n",
      "[399 | 4027.03] loss=1.96 avg=1.80\n",
      "[400 | 4037.03] loss=1.69 avg=1.80\n",
      "Saving checkpoint/QGen_QUAC_test/model-400\n",
      "[401 | 4050.02] loss=2.59 avg=1.81\n",
      "[402 | 4060.05] loss=1.89 avg=1.81\n",
      "[403 | 4070.11] loss=1.16 avg=1.80\n",
      "[404 | 4080.24] loss=1.80 avg=1.80\n",
      "[405 | 4090.23] loss=2.27 avg=1.80\n",
      "[406 | 4100.28] loss=1.11 avg=1.80\n",
      "[407 | 4110.28] loss=1.37 avg=1.79\n",
      "[408 | 4120.32] loss=1.94 avg=1.79\n",
      "[409 | 4130.29] loss=1.11 avg=1.79\n",
      "[410 | 4140.28] loss=1.15 avg=1.78\n",
      "[411 | 4150.31] loss=2.18 avg=1.78\n",
      "[412 | 4160.29] loss=1.17 avg=1.78\n",
      "[413 | 4170.30] loss=1.01 avg=1.77\n",
      "[414 | 4180.51] loss=1.60 avg=1.77\n",
      "[415 | 4190.50] loss=1.53 avg=1.77\n",
      "[416 | 4200.52] loss=1.15 avg=1.76\n",
      "[417 | 4210.57] loss=1.24 avg=1.76\n",
      "[418 | 4220.65] loss=2.34 avg=1.76\n",
      "[419 | 4230.69] loss=2.15 avg=1.77\n",
      "[420 | 4240.72] loss=2.21 avg=1.77\n",
      "[421 | 4250.72] loss=3.06 avg=1.78\n",
      "[422 | 4260.73] loss=1.81 avg=1.78\n",
      "[423 | 4270.73] loss=2.59 avg=1.79\n",
      "[424 | 4280.71] loss=1.63 avg=1.79\n",
      "[425 | 4290.73] loss=2.16 avg=1.79\n",
      "[426 | 4300.79] loss=1.67 avg=1.79\n",
      "[427 | 4310.82] loss=1.39 avg=1.79\n",
      "[428 | 4320.91] loss=2.19 avg=1.79\n",
      "[429 | 4330.93] loss=1.82 avg=1.79\n",
      "[430 | 4340.98] loss=3.02 avg=1.80\n",
      "[431 | 4350.99] loss=1.23 avg=1.80\n",
      "[432 | 4361.27] loss=1.34 avg=1.79\n",
      "[433 | 4371.31] loss=1.58 avg=1.79\n",
      "[434 | 4381.37] loss=1.22 avg=1.79\n",
      "[435 | 4391.38] loss=1.83 avg=1.79\n",
      "[436 | 4401.46] loss=1.63 avg=1.79\n",
      "[437 | 4411.50] loss=1.35 avg=1.78\n",
      "[438 | 4421.54] loss=2.07 avg=1.78\n",
      "[439 | 4431.54] loss=2.27 avg=1.79\n",
      "[440 | 4441.56] loss=2.65 avg=1.80\n",
      "[441 | 4451.58] loss=2.00 avg=1.80\n",
      "[442 | 4461.58] loss=1.54 avg=1.80\n",
      "[443 | 4471.65] loss=2.22 avg=1.80\n",
      "[444 | 4481.63] loss=1.56 avg=1.80\n",
      "[445 | 4491.64] loss=1.53 avg=1.80\n",
      "[446 | 4501.67] loss=1.31 avg=1.79\n",
      "[447 | 4511.72] loss=2.35 avg=1.80\n",
      "[448 | 4521.69] loss=2.77 avg=1.81\n",
      "[449 | 4531.73] loss=1.85 avg=1.81\n",
      "[450 | 4541.93] loss=1.35 avg=1.80\n",
      "[451 | 4552.00] loss=2.73 avg=1.81\n",
      "[452 | 4562.02] loss=2.65 avg=1.82\n",
      "[453 | 4572.14] loss=1.34 avg=1.82\n",
      "[454 | 4582.15] loss=2.10 avg=1.82\n",
      "[455 | 4592.22] loss=1.27 avg=1.81\n",
      "[456 | 4602.32] loss=1.70 avg=1.81\n",
      "[457 | 4612.31] loss=1.48 avg=1.81\n",
      "[458 | 4622.34] loss=1.67 avg=1.81\n",
      "[459 | 4632.40] loss=1.95 avg=1.81\n",
      "[460 | 4642.39] loss=1.59 avg=1.81\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[461 | 4652.42] loss=2.46 avg=1.81\n",
      "[462 | 4662.46] loss=0.78 avg=1.80\n",
      "[463 | 4672.47] loss=1.42 avg=1.80\n",
      "[464 | 4682.45] loss=2.90 avg=1.81\n",
      "[465 | 4692.49] loss=1.75 avg=1.81\n",
      "[466 | 4702.47] loss=1.66 avg=1.81\n",
      "[467 | 4712.50] loss=1.27 avg=1.80\n",
      "[468 | 4722.80] loss=1.90 avg=1.80\n",
      "[469 | 4732.86] loss=1.90 avg=1.80\n",
      "[470 | 4742.90] loss=2.77 avg=1.81\n",
      "[471 | 4752.87] loss=1.62 avg=1.81\n",
      "[472 | 4762.88] loss=1.14 avg=1.80\n",
      "[473 | 4772.84] loss=1.24 avg=1.80\n",
      "[474 | 4782.89] loss=3.33 avg=1.81\n",
      "[475 | 4792.88] loss=1.58 avg=1.81\n",
      "[476 | 4802.91] loss=2.42 avg=1.82\n",
      "[477 | 4812.96] loss=1.82 avg=1.82\n",
      "[1485 | 14968.24] loss=1.72 avg=1.69\n",
      "[1486 | 14978.48] loss=1.41 avg=1.69\n",
      "[1487 | 14988.54] loss=2.00 avg=1.69\n",
      "[1488 | 14998.55] loss=2.14 avg=1.70\n",
      "[1489 | 15008.54] loss=1.15 avg=1.69\n",
      "[1490 | 15018.56] loss=1.86 avg=1.69\n",
      "[1491 | 15028.62] loss=1.46 avg=1.69\n",
      "[1492 | 15038.59] loss=1.10 avg=1.68\n",
      "[1493 | 15048.60] loss=2.89 avg=1.70\n",
      "[1494 | 15058.63] loss=1.41 avg=1.69\n",
      "[1495 | 15068.58] loss=2.11 avg=1.70\n",
      "[1496 | 15078.59] loss=2.80 avg=1.71\n",
      "[1497 | 15088.64] loss=1.44 avg=1.71\n",
      "[1498 | 15098.61] loss=2.26 avg=1.71\n",
      "[1499 | 15108.65] loss=2.41 avg=1.72\n",
      "[1500 | 15118.67] loss=2.32 avg=1.72\n",
      "Saving checkpoint/QGen_QUAC_test/model-1500\n",
      "[1501 | 15131.53] loss=1.45 avg=1.72\n",
      "[1502 | 15141.53] loss=2.55 avg=1.73\n",
      "[1503 | 15151.54] loss=2.34 avg=1.74\n",
      "[1504 | 15161.78] loss=2.56 avg=1.74\n",
      "[1505 | 15171.82] loss=1.25 avg=1.74\n",
      "[1506 | 15181.81] loss=2.24 avg=1.74\n",
      "[1507 | 15191.81] loss=1.35 avg=1.74\n",
      "[1508 | 15201.82] loss=1.96 avg=1.74\n",
      "[1509 | 15211.75] loss=0.94 avg=1.73\n",
      "[1510 | 15221.76] loss=1.65 avg=1.73\n",
      "[1511 | 15231.75] loss=1.81 avg=1.73\n",
      "[1512 | 15241.79] loss=1.84 avg=1.74\n",
      "[1513 | 15251.73] loss=1.93 avg=1.74\n",
      "[1514 | 15261.66] loss=2.07 avg=1.74\n",
      "[1515 | 15271.64] loss=1.86 avg=1.74\n",
      "[1516 | 15281.62] loss=1.50 avg=1.74\n",
      "[1517 | 15291.58] loss=1.17 avg=1.73\n",
      "[1518 | 15301.55] loss=1.09 avg=1.73\n",
      "[1519 | 15311.52] loss=3.26 avg=1.74\n",
      "[1520 | 15321.49] loss=1.11 avg=1.74\n",
      "[1521 | 15331.52] loss=1.58 avg=1.74\n",
      "[1522 | 15341.72] loss=1.58 avg=1.73\n",
      "[1523 | 15351.75] loss=1.56 avg=1.73\n",
      "[1524 | 15361.72] loss=1.90 avg=1.73\n",
      "[1525 | 15371.70] loss=1.01 avg=1.73\n",
      "[1526 | 15381.67] loss=1.00 avg=1.72\n",
      "[1527 | 15391.64] loss=3.07 avg=1.73\n",
      "[1528 | 15401.61] loss=1.06 avg=1.73\n",
      "[1529 | 15411.56] loss=2.66 avg=1.74\n",
      "[1530 | 15421.61] loss=2.42 avg=1.74\n",
      "[1531 | 15431.61] loss=1.97 avg=1.74\n",
      "[1532 | 15441.64] loss=2.72 avg=1.75\n",
      "[1533 | 15451.63] loss=1.75 avg=1.75\n",
      "[1534 | 15461.59] loss=1.46 avg=1.75\n",
      "[1535 | 15471.60] loss=2.69 avg=1.76\n",
      "[1536 | 15481.63] loss=1.65 avg=1.76\n",
      "[1537 | 15491.65] loss=1.06 avg=1.75\n",
      "[1538 | 15501.64] loss=1.64 avg=1.75\n",
      "[1539 | 15511.60] loss=1.51 avg=1.75\n",
      "[1540 | 15521.87] loss=3.34 avg=1.76\n",
      "[1541 | 15531.86] loss=1.11 avg=1.76\n",
      "[1542 | 15541.92] loss=1.70 avg=1.76\n",
      "[1543 | 15551.97] loss=1.22 avg=1.75\n",
      "[1544 | 15561.92] loss=0.89 avg=1.74\n",
      "[1545 | 15571.96] loss=1.14 avg=1.74\n",
      "[1546 | 15581.94] loss=2.08 avg=1.74\n",
      "[1547 | 15591.91] loss=1.76 avg=1.74\n",
      "[1548 | 15602.00] loss=2.11 avg=1.74\n",
      "[1549 | 15612.01] loss=2.51 avg=1.75\n",
      "[1550 | 15621.95] loss=2.35 avg=1.76\n",
      "[1551 | 15631.93] loss=0.88 avg=1.75\n",
      "[1552 | 15641.94] loss=1.36 avg=1.75\n",
      "[1553 | 15651.98] loss=1.83 avg=1.75\n",
      "[1554 | 15661.99] loss=2.59 avg=1.75\n",
      "[1555 | 15671.99] loss=1.53 avg=1.75\n",
      "[1556 | 15681.97] loss=1.96 avg=1.75\n",
      "[1557 | 15691.98] loss=0.72 avg=1.74\n",
      "[1558 | 15702.23] loss=1.42 avg=1.74\n",
      "[1559 | 15712.22] loss=1.10 avg=1.73\n",
      "[1560 | 15722.29] loss=1.12 avg=1.73\n",
      "[1561 | 15732.26] loss=1.33 avg=1.72\n",
      "[1562 | 15742.25] loss=1.54 avg=1.72\n",
      "[1563 | 15752.27] loss=1.60 avg=1.72\n",
      "[1564 | 15762.36] loss=1.37 avg=1.72\n",
      "[1565 | 15772.31] loss=1.76 avg=1.72\n",
      "[1566 | 15782.30] loss=1.84 avg=1.72\n",
      "[1567 | 15792.29] loss=1.33 avg=1.72\n",
      "[1568 | 15802.31] loss=1.48 avg=1.71\n",
      "[1569 | 15812.28] loss=1.94 avg=1.72\n",
      "[1570 | 15822.28] loss=1.93 avg=1.72\n",
      "[1571 | 15832.22] loss=2.08 avg=1.72\n",
      "[1572 | 15842.19] loss=1.47 avg=1.72\n",
      "[1573 | 15852.19] loss=1.50 avg=1.72\n",
      "[1574 | 15862.14] loss=1.60 avg=1.72\n",
      "[1575 | 15872.14] loss=1.46 avg=1.71\n",
      "[1576 | 15882.35] loss=1.33 avg=1.71\n",
      "[1577 | 15892.34] loss=2.02 avg=1.71\n",
      "[1578 | 15902.32] loss=1.71 avg=1.71\n",
      "[1579 | 15912.30] loss=1.23 avg=1.71\n",
      "[1580 | 15922.28] loss=1.16 avg=1.70\n",
      "[1581 | 15932.27] loss=1.88 avg=1.70\n",
      "[1582 | 15942.24] loss=1.63 avg=1.70\n",
      "[1583 | 15952.22] loss=2.16 avg=1.71\n",
      "[1584 | 15962.26] loss=2.40 avg=1.71\n",
      "[1585 | 15972.25] loss=2.82 avg=1.73\n",
      "[1586 | 15982.25] loss=1.68 avg=1.72\n",
      "[1587 | 15992.22] loss=2.29 avg=1.73\n",
      "[1588 | 16002.14] loss=1.80 avg=1.73\n",
      "[1589 | 16012.09] loss=0.99 avg=1.72\n",
      "[1590 | 16022.03] loss=1.22 avg=1.72\n",
      "[1591 | 16032.05] loss=1.59 avg=1.72\n",
      "[1592 | 16042.02] loss=1.34 avg=1.71\n",
      "[1593 | 16052.04] loss=1.88 avg=1.72\n",
      "[1594 | 16062.27] loss=3.43 avg=1.73\n",
      "[1595 | 16072.28] loss=1.86 avg=1.73\n",
      "[1596 | 16082.30] loss=1.89 avg=1.74\n",
      "[1597 | 16092.29] loss=1.59 avg=1.73\n",
      "[1598 | 16102.30] loss=2.68 avg=1.74\n",
      "[1599 | 16112.31] loss=2.24 avg=1.75\n",
      "[1600 | 16122.28] loss=1.02 avg=1.74\n",
      "Saving checkpoint/QGen_QUAC_test/model-1600\n",
      "[1601 | 16135.23] loss=0.87 avg=1.73\n",
      "[1602 | 16145.31] loss=2.07 avg=1.74\n",
      "[1603 | 16155.33] loss=1.16 avg=1.73\n",
      "[1604 | 16165.36] loss=1.97 avg=1.73\n",
      "[1605 | 16175.35] loss=1.74 avg=1.73\n",
      "[1606 | 16185.33] loss=1.88 avg=1.73\n",
      "[1607 | 16195.35] loss=1.57 avg=1.73\n",
      "[1608 | 16205.38] loss=1.66 avg=1.73\n",
      "[1609 | 16215.39] loss=1.60 avg=1.73\n",
      "[1610 | 16225.40] loss=1.37 avg=1.73\n",
      "[1611 | 16235.40] loss=2.70 avg=1.74\n",
      "[1612 | 16245.72] loss=1.26 avg=1.73\n",
      "[1613 | 16255.77] loss=2.50 avg=1.74\n",
      "[1614 | 16265.78] loss=0.91 avg=1.73\n",
      "[1615 | 16275.78] loss=1.07 avg=1.72\n",
      "[1616 | 16285.75] loss=1.28 avg=1.72\n",
      "[1617 | 16295.76] loss=1.99 avg=1.72\n",
      "[1618 | 16305.80] loss=1.57 avg=1.72\n",
      "[1619 | 16315.85] loss=2.31 avg=1.73\n",
      "[1620 | 16325.85] loss=2.62 avg=1.74\n",
      "[1621 | 16335.89] loss=2.13 avg=1.74\n",
      "[1622 | 16345.88] loss=3.23 avg=1.75\n",
      "[1623 | 16355.86] loss=2.03 avg=1.76\n",
      "[1624 | 16365.87] loss=1.95 avg=1.76\n",
      "[1625 | 16375.81] loss=1.67 avg=1.76\n",
      "[1626 | 16385.89] loss=1.99 avg=1.76\n",
      "[1627 | 16395.95] loss=1.38 avg=1.76\n",
      "[1628 | 16405.95] loss=1.41 avg=1.75\n",
      "[1629 | 16415.99] loss=1.92 avg=1.76\n",
      "[1630 | 16426.23] loss=2.31 avg=1.76\n",
      "[1631 | 16436.30] loss=1.94 avg=1.76\n",
      "[1632 | 16446.34] loss=1.93 avg=1.76\n",
      "[1633 | 16456.35] loss=1.24 avg=1.76\n",
      "[1634 | 16466.30] loss=1.43 avg=1.76\n",
      "[1635 | 16476.36] loss=1.95 avg=1.76\n",
      "[1636 | 16486.36] loss=1.58 avg=1.76\n",
      "[1637 | 16496.41] loss=1.24 avg=1.75\n",
      "[1638 | 16506.45] loss=2.13 avg=1.75\n",
      "[1639 | 16516.52] loss=1.57 avg=1.75\n",
      "[1640 | 16526.54] loss=1.48 avg=1.75\n",
      "[1641 | 16536.57] loss=1.61 avg=1.75\n",
      "[1642 | 16546.59] loss=1.27 avg=1.74\n",
      "[1643 | 16556.56] loss=1.52 avg=1.74\n",
      "[1644 | 16566.60] loss=1.86 avg=1.74\n",
      "[1645 | 16576.56] loss=2.55 avg=1.75\n",
      "[1646 | 16586.58] loss=1.66 avg=1.75\n",
      "[1647 | 16596.66] loss=1.33 avg=1.75\n",
      "[1648 | 16606.92] loss=1.63 avg=1.74\n",
      "[1649 | 16616.94] loss=1.78 avg=1.74\n",
      "[1650 | 16626.96] loss=1.23 avg=1.74\n",
      "[1651 | 16636.92] loss=1.92 avg=1.74\n",
      "[1652 | 16646.92] loss=1.66 avg=1.74\n",
      "[1653 | 16656.95] loss=2.09 avg=1.74\n",
      "[1654 | 16666.92] loss=0.91 avg=1.74\n",
      "[1655 | 16676.99] loss=1.80 avg=1.74\n",
      "[1656 | 16686.94] loss=1.88 avg=1.74\n",
      "[1657 | 16696.90] loss=1.58 avg=1.74\n",
      "[1658 | 16706.92] loss=3.26 avg=1.75\n",
      "[1659 | 16716.94] loss=1.34 avg=1.75\n",
      "[1660 | 16726.91] loss=2.31 avg=1.75\n",
      "[1661 | 16737.00] loss=2.44 avg=1.76\n",
      "[1662 | 16747.02] loss=1.59 avg=1.76\n",
      "[1663 | 16756.99] loss=1.56 avg=1.76\n",
      "[1664 | 16766.95] loss=1.21 avg=1.75\n",
      "[1665 | 16776.99] loss=0.96 avg=1.74\n",
      "[1666 | 16787.22] loss=2.08 avg=1.75\n",
      "[1667 | 16797.19] loss=2.20 avg=1.75\n",
      "[1668 | 16807.20] loss=1.71 avg=1.75\n",
      "[1669 | 16817.21] loss=1.31 avg=1.75\n",
      "[1670 | 16827.22] loss=1.24 avg=1.74\n",
      "[1671 | 16837.17] loss=1.21 avg=1.74\n",
      "[1672 | 16847.19] loss=1.45 avg=1.73\n",
      "[1673 | 16857.17] loss=1.09 avg=1.73\n",
      "[1674 | 16867.15] loss=1.07 avg=1.72\n",
      "[1675 | 16877.19] loss=1.71 avg=1.72\n",
      "[1676 | 16887.22] loss=1.41 avg=1.72\n",
      "[1677 | 16897.23] loss=1.98 avg=1.72\n",
      "[1678 | 16907.15] loss=1.39 avg=1.72\n",
      "[1679 | 16917.18] loss=1.29 avg=1.71\n",
      "[1680 | 16927.22] loss=2.88 avg=1.72\n",
      "[1681 | 16937.18] loss=1.18 avg=1.72\n",
      "[1682 | 16947.15] loss=1.98 avg=1.72\n",
      "[1683 | 16957.33] loss=1.44 avg=1.72\n",
      "[1684 | 16967.47] loss=1.49 avg=1.72\n",
      "[1685 | 16977.45] loss=2.20 avg=1.72\n",
      "[1686 | 16987.50] loss=2.16 avg=1.72\n",
      "[1687 | 16997.50] loss=1.72 avg=1.72\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1688 | 17007.50] loss=1.11 avg=1.72\n",
      "[1689 | 17017.46] loss=1.12 avg=1.71\n",
      "[1690 | 17027.51] loss=2.35 avg=1.72\n",
      "[1691 | 17037.61] loss=1.00 avg=1.71\n",
      "[1692 | 17047.65] loss=1.29 avg=1.71\n",
      "[1693 | 17057.62] loss=1.28 avg=1.70\n",
      "[1694 | 17067.65] loss=1.76 avg=1.70\n",
      "[1695 | 17077.66] loss=2.54 avg=1.71\n",
      "[1696 | 17087.65] loss=2.46 avg=1.72\n",
      "[1697 | 17097.71] loss=0.98 avg=1.71\n",
      "[1698 | 17107.72] loss=2.76 avg=1.72\n",
      "[1699 | 17117.74] loss=1.19 avg=1.72\n",
      "[1700 | 17127.81] loss=1.27 avg=1.71\n",
      "Saving checkpoint/QGen_QUAC_test/model-1700\n",
      "[1701 | 17141.01] loss=2.10 avg=1.72\n",
      "[1702 | 17150.96] loss=1.92 avg=1.72\n",
      "[1703 | 17160.95] loss=1.70 avg=1.72\n",
      "[1704 | 17171.04] loss=1.62 avg=1.72\n",
      "[1705 | 17181.03] loss=1.37 avg=1.71\n",
      "[1706 | 17191.04] loss=1.84 avg=1.72\n",
      "[1707 | 17200.99] loss=1.58 avg=1.71\n",
      "[1708 | 17210.99] loss=2.10 avg=1.72\n",
      "[1709 | 17220.95] loss=1.75 avg=1.72\n",
      "[1710 | 17230.97] loss=3.26 avg=1.73\n",
      "[1711 | 17240.92] loss=2.25 avg=1.74\n",
      "[1712 | 17250.89] loss=2.44 avg=1.75\n",
      "[1713 | 17260.85] loss=1.63 avg=1.74\n",
      "[1714 | 17270.82] loss=1.21 avg=1.74\n",
      "[1715 | 17280.83] loss=3.37 avg=1.76\n",
      "[1716 | 17290.83] loss=1.23 avg=1.75\n",
      "[1717 | 17300.82] loss=1.46 avg=1.75\n",
      "[1718 | 17310.85] loss=1.84 avg=1.75\n",
      "[1719 | 17321.04] loss=1.77 avg=1.75\n",
      "[1720 | 17331.06] loss=2.96 avg=1.76\n",
      "[1721 | 17341.06] loss=1.64 avg=1.76\n",
      "[1722 | 17351.07] loss=2.42 avg=1.77\n",
      "[1723 | 17361.10] loss=1.40 avg=1.76\n",
      "[1724 | 17371.18] loss=2.52 avg=1.77\n",
      "[1725 | 17381.17] loss=2.60 avg=1.78\n",
      "[1726 | 17391.15] loss=2.63 avg=1.79\n",
      "[1727 | 17401.16] loss=1.50 avg=1.78\n",
      "[1728 | 17411.12] loss=1.31 avg=1.78\n",
      "[1729 | 17421.08] loss=1.94 avg=1.78\n",
      "[1730 | 17431.07] loss=1.19 avg=1.78\n",
      "[1731 | 17441.06] loss=1.78 avg=1.78\n",
      "[1732 | 17451.03] loss=1.34 avg=1.77\n",
      "[1733 | 17461.01] loss=2.64 avg=1.78\n",
      "[1734 | 17470.98] loss=2.04 avg=1.78\n",
      "[1735 | 17480.99] loss=0.98 avg=1.77\n",
      "[1736 | 17490.98] loss=2.02 avg=1.78\n",
      "[1737 | 17501.21] loss=1.49 avg=1.77\n",
      "[1738 | 17511.18] loss=1.08 avg=1.77\n",
      "[1739 | 17521.16] loss=1.62 avg=1.77\n",
      "[1740 | 17531.14] loss=3.16 avg=1.78\n",
      "[1741 | 17541.18] loss=1.39 avg=1.78\n",
      "[1742 | 17551.13] loss=1.43 avg=1.77\n",
      "[1743 | 17561.12] loss=1.90 avg=1.77\n",
      "[1744 | 17571.09] loss=1.48 avg=1.77\n",
      "[1745 | 17581.16] loss=1.61 avg=1.77\n",
      "[1746 | 17591.22] loss=1.10 avg=1.76\n",
      "[1747 | 17601.27] loss=2.79 avg=1.77\n",
      "[1748 | 17611.36] loss=1.21 avg=1.77\n",
      "[1749 | 17621.40] loss=1.10 avg=1.76\n",
      "[1750 | 17631.49] loss=1.39 avg=1.76\n",
      "[1751 | 17641.53] loss=0.84 avg=1.75\n",
      "[1752 | 17651.53] loss=1.26 avg=1.74\n",
      "[1753 | 17661.54] loss=2.36 avg=1.75\n",
      "[1754 | 17671.62] loss=1.64 avg=1.75\n",
      "[1755 | 17681.87] loss=1.98 avg=1.75\n",
      "[1756 | 17691.96] loss=1.68 avg=1.75\n",
      "[1757 | 17701.96] loss=1.22 avg=1.74\n",
      "[1758 | 17711.99] loss=2.13 avg=1.75\n",
      "[1759 | 17722.00] loss=1.52 avg=1.75\n",
      "[1760 | 17732.00] loss=3.10 avg=1.76\n",
      "[1761 | 17741.98] loss=1.12 avg=1.75\n",
      "[1762 | 17751.98] loss=1.95 avg=1.75\n",
      "[1763 | 17761.97] loss=1.66 avg=1.75\n",
      "[1764 | 17772.00] loss=1.67 avg=1.75\n",
      "[1765 | 17781.97] loss=2.17 avg=1.76\n",
      "[1766 | 17792.02] loss=1.07 avg=1.75\n",
      "[1767 | 17802.06] loss=1.83 avg=1.75\n",
      "[1768 | 17812.02] loss=2.54 avg=1.76\n",
      "[1769 | 17822.07] loss=1.27 avg=1.75\n",
      "[1770 | 17832.07] loss=2.00 avg=1.76\n",
      "[1771 | 17842.09] loss=1.58 avg=1.75\n",
      "[1772 | 17852.08] loss=1.39 avg=1.75\n",
      "[1773 | 17862.28] loss=1.65 avg=1.75\n",
      "[1774 | 17872.30] loss=1.29 avg=1.75\n",
      "[1775 | 17882.33] loss=2.51 avg=1.75\n",
      "[1776 | 17892.40] loss=1.13 avg=1.75\n",
      "[1777 | 17902.39] loss=1.30 avg=1.74\n",
      "[1778 | 17912.39] loss=1.92 avg=1.74\n",
      "[1779 | 17922.38] loss=1.04 avg=1.74\n",
      "[1780 | 17932.33] loss=0.86 avg=1.73\n",
      "[1781 | 17942.38] loss=2.10 avg=1.73\n",
      "[1782 | 17952.33] loss=2.03 avg=1.73\n",
      "[1783 | 17962.29] loss=1.31 avg=1.73\n",
      "[1784 | 17972.24] loss=2.40 avg=1.74\n",
      "[1785 | 17982.27] loss=2.12 avg=1.74\n",
      "[1786 | 17992.23] loss=2.11 avg=1.74\n",
      "[1787 | 18002.20] loss=2.07 avg=1.75\n",
      "[1788 | 18012.12] loss=1.77 avg=1.75\n",
      "[1789 | 18022.15] loss=1.55 avg=1.75\n",
      "[1790 | 18032.11] loss=1.59 avg=1.74\n",
      "[1791 | 18042.27] loss=0.84 avg=1.74\n",
      "[1792 | 18052.22] loss=1.90 avg=1.74\n",
      "[1793 | 18062.24] loss=1.72 avg=1.74\n",
      "[1794 | 18072.26] loss=2.36 avg=1.74\n",
      "[1795 | 18082.25] loss=1.82 avg=1.74\n",
      "[1796 | 18092.25] loss=2.55 avg=1.75\n",
      "[1797 | 18102.25] loss=2.20 avg=1.76\n",
      "[1798 | 18112.28] loss=1.36 avg=1.75\n",
      "[1799 | 18122.24] loss=1.38 avg=1.75\n",
      "[1800 | 18132.24] loss=1.07 avg=1.74\n",
      "Saving checkpoint/QGen_QUAC_test/model-1800\n",
      "[1801 | 18145.18] loss=1.49 avg=1.74\n",
      "[1802 | 18155.25] loss=0.97 avg=1.73\n",
      "[1803 | 18165.31] loss=2.99 avg=1.74\n",
      "[1804 | 18175.32] loss=1.25 avg=1.74\n",
      "[1805 | 18185.37] loss=2.60 avg=1.75\n",
      "[1806 | 18195.36] loss=1.35 avg=1.74\n",
      "[1807 | 18205.39] loss=2.14 avg=1.75\n",
      "[1808 | 18215.49] loss=1.51 avg=1.75\n",
      "[1809 | 18225.77] loss=1.60 avg=1.74\n",
      "[1810 | 18235.77] loss=2.47 avg=1.75\n",
      "[1811 | 18245.78] loss=1.04 avg=1.74\n",
      "[1812 | 18255.80] loss=1.44 avg=1.74\n",
      "[1813 | 18265.80] loss=1.75 avg=1.74\n",
      "[1814 | 18275.84] loss=1.79 avg=1.74\n",
      "[1815 | 18285.85] loss=2.70 avg=1.75\n",
      "[1816 | 18295.86] loss=1.60 avg=1.75\n",
      "[1817 | 18305.89] loss=1.52 avg=1.75\n",
      "[1818 | 18315.90] loss=1.16 avg=1.74\n",
      "[1819 | 18325.93] loss=0.73 avg=1.73\n",
      "[1820 | 18335.94] loss=1.71 avg=1.73\n",
      "[1821 | 18346.02] loss=3.41 avg=1.75\n",
      "[1822 | 18356.14] loss=2.94 avg=1.76\n",
      "[1823 | 18366.32] loss=2.42 avg=1.77\n",
      "[1824 | 18376.40] loss=1.78 avg=1.77\n",
      "[1825 | 18386.49] loss=1.98 avg=1.77\n",
      "[1826 | 18396.67] loss=1.73 avg=1.77\n",
      "[1827 | 18406.95] loss=1.18 avg=1.76\n",
      "[1828 | 18417.02] loss=2.63 avg=1.77\n",
      "[1829 | 18427.14] loss=1.46 avg=1.77\n",
      "[1830 | 18437.19] loss=1.56 avg=1.77\n",
      "[1831 | 18447.31] loss=2.22 avg=1.77\n",
      "[1832 | 18457.41] loss=2.95 avg=1.78\n",
      "[1833 | 18467.51] loss=1.55 avg=1.78\n",
      "[1834 | 18477.62] loss=2.21 avg=1.78\n",
      "[1835 | 18487.78] loss=2.61 avg=1.79\n",
      "[1836 | 18497.88] loss=1.52 avg=1.79\n",
      "[1837 | 18508.08] loss=1.69 avg=1.79\n",
      "[1838 | 18518.22] loss=1.62 avg=1.79\n",
      "[1839 | 18528.33] loss=2.94 avg=1.80\n",
      "[1840 | 18538.43] loss=2.15 avg=1.80\n",
      "[1841 | 18548.49] loss=0.68 avg=1.79\n",
      "[1842 | 18558.59] loss=1.15 avg=1.78\n",
      "[1843 | 18568.68] loss=1.60 avg=1.78\n",
      "[1844 | 18578.96] loss=1.43 avg=1.78\n",
      "[1845 | 18589.08] loss=1.11 avg=1.77\n",
      "[1846 | 18599.20] loss=1.67 avg=1.77\n",
      "[1847 | 18609.34] loss=1.26 avg=1.77\n",
      "[1848 | 18619.45] loss=0.79 avg=1.76\n",
      "[1849 | 18629.57] loss=1.56 avg=1.75\n",
      "[1850 | 18639.63] loss=1.26 avg=1.75\n",
      "[1851 | 18649.66] loss=1.02 avg=1.74\n",
      "[1852 | 18659.71] loss=1.78 avg=1.74\n",
      "[1853 | 18669.79] loss=1.21 avg=1.74\n",
      "[1854 | 18679.84] loss=1.94 avg=1.74\n",
      "[1855 | 18689.90] loss=1.47 avg=1.74\n",
      "[1856 | 18700.00] loss=2.26 avg=1.74\n",
      "[1857 | 18710.05] loss=2.59 avg=1.75\n",
      "[1858 | 18720.17] loss=1.21 avg=1.75\n",
      "[1859 | 18730.26] loss=2.20 avg=1.75\n",
      "[1860 | 18740.34] loss=1.49 avg=1.75\n",
      "[1861 | 18750.39] loss=1.75 avg=1.75\n",
      "[1862 | 18760.66] loss=1.20 avg=1.74\n",
      "[1863 | 18770.77] loss=2.16 avg=1.75\n",
      "[1864 | 18780.88] loss=2.73 avg=1.76\n",
      "[1865 | 18790.96] loss=1.09 avg=1.75\n",
      "[1866 | 18801.01] loss=1.19 avg=1.74\n",
      "[1867 | 18811.10] loss=1.20 avg=1.74\n",
      "[1868 | 18821.15] loss=1.61 avg=1.74\n",
      "[1869 | 18831.20] loss=1.24 avg=1.73\n",
      "[1870 | 18841.31] loss=2.35 avg=1.74\n",
      "[1871 | 18851.37] loss=0.99 avg=1.73\n",
      "[1872 | 18861.43] loss=0.83 avg=1.72\n",
      "[1873 | 18871.48] loss=1.47 avg=1.72\n",
      "[1874 | 18881.61] loss=3.16 avg=1.73\n",
      "[1875 | 18891.70] loss=1.53 avg=1.73\n",
      "[1876 | 18901.77] loss=1.64 avg=1.73\n",
      "[1877 | 18911.87] loss=1.84 avg=1.73\n",
      "[1878 | 18921.95] loss=1.28 avg=1.73\n",
      "[1879 | 18932.03] loss=1.76 avg=1.73\n",
      "[1880 | 18942.33] loss=2.80 avg=1.74\n",
      "[1881 | 18952.41] loss=1.70 avg=1.74\n",
      "[1882 | 18962.58] loss=1.37 avg=1.73\n",
      "[1883 | 18972.64] loss=1.42 avg=1.73\n",
      "[1884 | 18982.73] loss=2.30 avg=1.74\n",
      "[1885 | 18992.78] loss=2.10 avg=1.74\n",
      "[1886 | 19002.84] loss=1.12 avg=1.73\n",
      "[1887 | 19012.90] loss=1.17 avg=1.73\n",
      "[1888 | 19022.99] loss=1.65 avg=1.73\n",
      "[1889 | 19033.03] loss=2.07 avg=1.73\n",
      "[1890 | 19043.12] loss=1.39 avg=1.73\n",
      "[1891 | 19053.16] loss=1.85 avg=1.73\n",
      "[1892 | 19063.24] loss=2.91 avg=1.74\n",
      "[1893 | 19073.35] loss=1.17 avg=1.74\n",
      "[1894 | 19083.43] loss=1.76 avg=1.74\n",
      "[1895 | 19093.49] loss=1.48 avg=1.73\n",
      "[1896 | 19103.67] loss=2.08 avg=1.74\n",
      "[1897 | 19113.81] loss=1.04 avg=1.73\n",
      "[1898 | 19124.22] loss=1.28 avg=1.72\n",
      "[1899 | 19134.32] loss=2.25 avg=1.73\n",
      "[1900 | 19144.46] loss=1.61 avg=1.73\n",
      "Saving checkpoint/QGen_QUAC_test/model-1900\n",
      "[1901 | 19157.51] loss=1.30 avg=1.72\n",
      "[1902 | 19167.55] loss=1.57 avg=1.72\n",
      "[1903 | 19177.65] loss=1.73 avg=1.72\n",
      "[1904 | 19187.75] loss=1.52 avg=1.72\n",
      "[1905 | 19197.78] loss=1.25 avg=1.72\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1906 | 19207.88] loss=1.74 avg=1.72\n",
      "[1907 | 19217.95] loss=1.00 avg=1.71\n",
      "[1908 | 19228.06] loss=2.20 avg=1.71\n",
      "[1909 | 19238.13] loss=1.99 avg=1.72\n",
      "[1910 | 19248.13] loss=2.00 avg=1.72\n",
      "[1911 | 19258.19] loss=2.07 avg=1.72\n",
      "[1912 | 19268.30] loss=1.71 avg=1.72\n",
      "[1913 | 19278.37] loss=1.81 avg=1.72\n",
      "[1914 | 19288.40] loss=3.08 avg=1.74\n",
      "[1915 | 19298.67] loss=2.46 avg=1.74\n",
      "[1916 | 19308.71] loss=1.18 avg=1.74\n",
      "[1917 | 19318.79] loss=1.51 avg=1.74\n",
      "[1918 | 19328.87] loss=1.42 avg=1.73\n",
      "[1919 | 19338.83] loss=1.19 avg=1.73\n",
      "[1920 | 19348.90] loss=2.29 avg=1.73\n",
      "[1921 | 19358.96] loss=0.89 avg=1.73\n",
      "[1922 | 19368.99] loss=1.77 avg=1.73\n",
      "[1923 | 19379.08] loss=2.73 avg=1.74\n",
      "[1924 | 19389.13] loss=2.09 avg=1.74\n",
      "[1925 | 19399.19] loss=1.82 avg=1.74\n",
      "[1926 | 19409.24] loss=2.34 avg=1.75\n",
      "[1927 | 19419.29] loss=2.31 avg=1.75\n",
      "[1928 | 19429.34] loss=1.93 avg=1.75\n",
      "[1929 | 19439.38] loss=1.43 avg=1.75\n",
      "[1930 | 19449.51] loss=1.57 avg=1.75\n",
      "[1931 | 19459.58] loss=1.43 avg=1.75\n",
      "[1932 | 19469.69] loss=1.76 avg=1.75\n",
      "[1933 | 19479.98] loss=1.48 avg=1.74\n",
      "[1934 | 19490.05] loss=1.13 avg=1.74\n",
      "[1935 | 19500.22] loss=1.27 avg=1.73\n",
      "[1936 | 19510.31] loss=1.23 avg=1.73\n",
      "[1937 | 19520.30] loss=1.47 avg=1.72\n",
      "[1938 | 19530.31] loss=1.43 avg=1.72\n",
      "[1939 | 19540.40] loss=1.40 avg=1.72\n",
      "[1940 | 19550.42] loss=1.26 avg=1.71\n",
      "[1941 | 19560.47] loss=0.65 avg=1.70\n",
      "[1942 | 19570.60] loss=0.93 avg=1.70\n",
      "[1943 | 19580.64] loss=1.82 avg=1.70\n",
      "[1944 | 19590.75] loss=1.32 avg=1.69\n",
      "[1945 | 19600.82] loss=1.59 avg=1.69\n",
      "[1946 | 19610.83] loss=1.26 avg=1.69\n",
      "[1947 | 19620.92] loss=1.35 avg=1.68\n",
      "[1948 | 19630.94] loss=1.73 avg=1.68\n",
      "[1949 | 19641.08] loss=2.26 avg=1.69\n",
      "[1950 | 19651.13] loss=1.22 avg=1.69\n",
      "[1951 | 19661.38] loss=2.13 avg=1.69\n",
      "[1952 | 19671.39] loss=1.33 avg=1.69\n",
      "[1953 | 19681.35] loss=2.09 avg=1.69\n",
      "[1954 | 19691.34] loss=1.42 avg=1.69\n",
      "[1955 | 19701.30] loss=1.21 avg=1.68\n",
      "[1956 | 19711.32] loss=1.36 avg=1.68\n",
      "[1957 | 19721.42] loss=1.53 avg=1.68\n",
      "[1958 | 19731.48] loss=1.37 avg=1.68\n",
      "[1959 | 19741.50] loss=1.29 avg=1.67\n",
      "[1960 | 19751.49] loss=1.72 avg=1.67\n",
      "[1961 | 19761.46] loss=1.26 avg=1.67\n",
      "[1962 | 19771.49] loss=1.78 avg=1.67\n",
      "[1963 | 19781.49] loss=1.19 avg=1.66\n",
      "[1964 | 19791.49] loss=0.87 avg=1.66\n",
      "[1965 | 19801.54] loss=2.08 avg=1.66\n",
      "[1966 | 19811.55] loss=1.34 avg=1.66\n",
      "[1967 | 19821.51] loss=1.82 avg=1.66\n",
      "[1968 | 19831.58] loss=1.73 avg=1.66\n",
      "[1969 | 19841.82] loss=1.87 avg=1.66\n",
      "[1970 | 19851.81] loss=1.92 avg=1.66\n",
      "[1971 | 19861.81] loss=1.26 avg=1.66\n",
      "[1972 | 19871.81] loss=1.45 avg=1.66\n",
      "[1973 | 19881.81] loss=1.86 avg=1.66\n",
      "[1974 | 19891.76] loss=1.69 avg=1.66\n",
      "[1975 | 19901.78] loss=1.83 avg=1.66\n",
      "[1976 | 19911.80] loss=1.86 avg=1.66\n",
      "[1977 | 19921.77] loss=1.21 avg=1.66\n",
      "[1978 | 19931.75] loss=1.83 avg=1.66\n",
      "[1979 | 19941.74] loss=1.20 avg=1.66\n",
      "[1980 | 19951.71] loss=1.66 avg=1.66\n",
      "[1981 | 19961.69] loss=1.55 avg=1.66\n",
      "[1982 | 19971.66] loss=1.25 avg=1.65\n",
      "[1983 | 19981.73] loss=1.68 avg=1.65\n",
      "[1984 | 19991.75] loss=1.20 avg=1.65\n",
      "[1985 | 20001.71] loss=2.26 avg=1.65\n",
      "[1986 | 20011.67] loss=1.41 avg=1.65\n",
      "[1987 | 20021.87] loss=2.54 avg=1.66\n",
      "[1988 | 20031.93] loss=1.42 avg=1.66\n",
      "[1989 | 20041.94] loss=2.09 avg=1.66\n",
      "[1990 | 20051.97] loss=2.71 avg=1.67\n",
      "[1991 | 20061.93] loss=1.17 avg=1.67\n",
      "[1992 | 20071.93] loss=1.42 avg=1.66\n",
      "[1993 | 20081.90] loss=2.08 avg=1.67\n",
      "[1994 | 20091.88] loss=1.03 avg=1.66\n",
      "[1995 | 20101.88] loss=2.01 avg=1.67\n",
      "[1996 | 20111.92] loss=2.69 avg=1.68\n",
      "[1997 | 20121.91] loss=2.61 avg=1.69\n",
      "[1998 | 20131.89] loss=1.50 avg=1.68\n",
      "[1999 | 20141.92] loss=2.02 avg=1.69\n",
      "[2000 | 20151.89] loss=1.47 avg=1.69\n",
      "Saving checkpoint/QGen_QUAC_test/model-2000\n",
      "======== SAMPLE 1 ========\n",
      " his own family.  After taking the reins, he led a successful and sometimes controversial course in public schools, earning a B.A. with a concentration in philosophy, in which his students included a group that included writers like Sigmund Freud and George Peirse. In 1952, he attended Yale Law School, where his classmates included scholars such as John Jowett and Norman Cohn. After graduating, he continued his academic work, becoming a law professor at the University of Chicago and subsequently the United States District Court for the Eastern District of New York. The following year, he taught at the University of Chicago until his retirement from law.  In 1962, his wife, Janet Ann, had a second child, and by late 1963 she had moved back into marriage.  He had been appointed a senior professor of mathematics at the United States Naval Postgraduate School.  He had been active in the civil rights struggle when he was a student at Columbia University.  After leaving Princeton for Yale in 1964, Gould received a teaching position at the Department of Economics of the University of Chicago's Stern School of Business. CANNOTANSWER [QUESTION]: What was his relationship with Janet? <|endoftext|>\n",
      "<|startoftext|>\n",
      "[CONTEXT]: From his 1960s onwards, Gould remained a popular academic, appearing in radio and television documentaries, and contributing to several books, including Gould's 1986 film In Search of Meaning: A New American History from the First Revolution to the Second Civil Rights Movement (with his son, Steven Gould), and The End of American Radicalism: The Rise of the Radical Right (in which his son was the featured character), and The Long Peace, with whom he is closely associated. It was in this period Gould's greatest intellectual work. Gould also worked to expand his readership of philosophy but this was done by building increasingly large, intricate studies which combined philosophical, sociological, biographical and linguistic elements.  At this point Gould left the Department of Economics, and during his career taught several courses at Princeton University. Gould did not withdraw from teaching, and was active politically in the 1960s. He wrote a book, The Unfinished Revolution: The Rise of the National Revolution in 1961-62. Although not the title of his own documentary, The Rise of the National Revolution, the book was a work of \"pure intellectual curiosity.\" In it, Gould argues that despite the election of Lyndon B. Johnson as President of the United States in 1964 and the subsequent civil rights movement, the conservative president was likely to continue the conservative drift, with little prospect for change.  Gould's third documentary, The Last Resort: The Final Days of the Conservative Movement (1993), was the final segment of his career. The film showed him, in a final confrontation with a conservative professor at Harvard, explaining why he had been so \"satisfied\" with conservatism. The film was largely filmed in the basement of his home, and it has been described as his most personal work. In addition, Gould claimed to have written the book for the film. It was released in 1995, when it was reported to be the most expensive documentary ever made - at a total value of US$10 million. With over $2 million and four studios, it proved impossible for Gould to make it look professional or convincing, and several critics rated it less than professional, but Gould said \"I felt if you've got the right stuff, you know it.\"  Gould's final documentary was the 1994 film The Great Divided: A Year on the Campaign Trail (about the 1996 election that saw the Republican candidate, Bill Kristol, accused of racial politics and inciting violence at the University of Virginia) but without his son, Steven. CANNOTANSWER [QUESTION]: What else do we know about Gould's career? <|endoftext|>\n",
      "<|startoftext|>\n",
      "[CONTEXT]: After leaving Princeton for Yale in 1964, Gould received a teaching position at the Department of Economics of the University of Chicago's Stern School of Business. CANNOTANSWER [QUESTION]: What was Gould's position in the college? <|endoftext|>\n",
      "<|startoftext|>\n",
      "[CONTEXT]: From his 1960s onwards, Gould remained a popular academic, appearing in radio and television documentaries, and contributing to several books, including Gould's 1986 film In Search of Meaning: A New American History from the First Revolution to the Second Civil Rights Movement (with his son, Steven Gould), and The End of American Radicalism: The Rise of the Radical Right (in which his son was the featured character), and The Long Peace, with whom he is closely associated. It was in this period Gould's greatest intellectual work. Gould also worked to expand his readership of philosophy but this was done by building increasingly large, intricate studies which combined philosophical, sociological, biographical and linguistic elements.  At this point Gould left the Department of Economics, and during his career taught several courses at Princeton University. Gould did not withdraw\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2001 | 20318.73] loss=1.85 avg=1.69\n",
      "[2002 | 20328.12] loss=1.39 avg=1.68\n",
      "[2003 | 20337.70] loss=2.55 avg=1.69\n",
      "[2004 | 20347.38] loss=1.64 avg=1.69\n",
      "[2005 | 20356.76] loss=1.96 avg=1.69\n",
      "[2006 | 20366.15] loss=1.18 avg=1.69\n",
      "[2007 | 20375.55] loss=1.49 avg=1.69\n",
      "[2008 | 20385.19] loss=1.28 avg=1.68\n",
      "[2009 | 20394.59] loss=2.45 avg=1.69\n",
      "[2010 | 20403.97] loss=1.62 avg=1.69\n",
      "[2011 | 20413.36] loss=1.33 avg=1.69\n",
      "[2012 | 20422.79] loss=1.79 avg=1.69\n",
      "[2013 | 20432.19] loss=1.30 avg=1.68\n",
      "[2014 | 20441.59] loss=2.32 avg=1.69\n",
      "[2015 | 20450.96] loss=1.75 avg=1.69\n",
      "[2016 | 20460.37] loss=0.96 avg=1.68\n",
      "[2017 | 20469.76] loss=2.40 avg=1.69\n",
      "[2018 | 20479.14] loss=1.74 avg=1.69\n",
      "[2019 | 20488.55] loss=1.70 avg=1.69\n",
      "[2020 | 20497.95] loss=1.75 avg=1.69\n",
      "[2021 | 20507.36] loss=2.12 avg=1.70\n",
      "[2022 | 20516.78] loss=1.81 avg=1.70\n",
      "[2023 | 20526.37] loss=1.45 avg=1.70\n",
      "[2024 | 20536.34] loss=1.25 avg=1.69\n",
      "[2025 | 20546.30] loss=1.81 avg=1.69\n",
      "[2026 | 20556.28] loss=1.99 avg=1.69\n",
      "[2027 | 20566.51] loss=2.71 avg=1.70\n",
      "[2028 | 20576.57] loss=1.57 avg=1.70\n",
      "[2029 | 20586.89] loss=1.47 avg=1.70\n",
      "[2030 | 20597.26] loss=1.72 avg=1.70\n",
      "[2031 | 20607.63] loss=1.88 avg=1.70\n",
      "[2032 | 20617.89] loss=1.54 avg=1.70\n",
      "[2033 | 20628.21] loss=0.87 avg=1.69\n",
      "[2034 | 20638.30] loss=1.41 avg=1.69\n",
      "[2035 | 20648.26] loss=1.50 avg=1.69\n",
      "[2036 | 20658.22] loss=0.89 avg=1.68\n",
      "[2037 | 20668.18] loss=0.95 avg=1.67\n",
      "[2038 | 20678.38] loss=3.04 avg=1.69\n",
      "[2039 | 20688.77] loss=2.87 avg=1.70\n",
      "[2040 | 20699.10] loss=1.99 avg=1.70\n",
      "[2041 | 20709.49] loss=1.16 avg=1.70\n",
      "[2042 | 20719.85] loss=2.50 avg=1.70\n",
      "[2043 | 20730.20] loss=1.21 avg=1.70\n",
      "[2044 | 20740.44] loss=2.68 avg=1.71\n",
      "[2045 | 20750.39] loss=1.90 avg=1.71\n",
      "[2046 | 20760.37] loss=1.68 avg=1.71\n",
      "[2047 | 20770.32] loss=0.72 avg=1.70\n",
      "[2048 | 20780.31] loss=1.54 avg=1.70\n",
      "[2049 | 20790.34] loss=2.94 avg=1.71\n",
      "[2050 | 20800.34] loss=2.09 avg=1.72\n",
      "[2051 | 20810.36] loss=1.07 avg=1.71\n",
      "[2052 | 20820.36] loss=2.29 avg=1.71\n",
      "[2053 | 20830.33] loss=1.36 avg=1.71\n",
      "[2054 | 20840.28] loss=1.38 avg=1.71\n",
      "[2055 | 20850.26] loss=2.32 avg=1.71\n",
      "[2056 | 20860.27] loss=1.16 avg=1.71\n",
      "[2057 | 20870.22] loss=2.39 avg=1.72\n",
      "[2058 | 20880.21] loss=1.36 avg=1.71\n",
      "[2059 | 20890.17] loss=1.67 avg=1.71\n",
      "[2060 | 20900.14] loss=1.39 avg=1.71\n",
      "[2061 | 20910.12] loss=1.06 avg=1.70\n",
      "[2062 | 20920.36] loss=1.06 avg=1.70\n",
      "[2063 | 20930.36] loss=1.40 avg=1.69\n",
      "[2064 | 20940.34] loss=1.27 avg=1.69\n",
      "[2065 | 20950.30] loss=1.12 avg=1.68\n",
      "[2066 | 20960.26] loss=2.55 avg=1.69\n",
      "[2067 | 20970.23] loss=2.41 avg=1.70\n",
      "[2068 | 20980.20] loss=2.29 avg=1.70\n",
      "[2069 | 20990.17] loss=1.45 avg=1.70\n",
      "[2070 | 21000.13] loss=3.16 avg=1.72\n",
      "[2071 | 21010.08] loss=1.38 avg=1.71\n",
      "[2072 | 21020.06] loss=1.05 avg=1.71\n",
      "[2073 | 21030.11] loss=1.54 avg=1.70\n",
      "[2074 | 21040.55] loss=1.04 avg=1.70\n",
      "[2075 | 21050.98] loss=1.54 avg=1.70\n",
      "[2076 | 21061.40] loss=1.33 avg=1.69\n",
      "[2077 | 21071.81] loss=2.35 avg=1.70\n",
      "[2078 | 21082.23] loss=1.20 avg=1.69\n",
      "[2079 | 21092.66] loss=1.88 avg=1.70\n",
      "[2080 | 21103.32] loss=1.71 avg=1.70\n",
      "[2081 | 21113.71] loss=1.55 avg=1.69\n",
      "[2082 | 21124.15] loss=0.75 avg=1.69\n",
      "[2083 | 21134.48] loss=0.76 avg=1.68\n",
      "[2084 | 21144.91] loss=0.98 avg=1.67\n",
      "[2085 | 21155.30] loss=2.90 avg=1.68\n",
      "[2086 | 21165.72] loss=1.45 avg=1.68\n",
      "[2087 | 21176.14] loss=1.19 avg=1.67\n",
      "[2088 | 21186.58] loss=1.33 avg=1.67\n",
      "[2089 | 21196.97] loss=1.93 avg=1.67\n",
      "[2090 | 21207.41] loss=2.05 avg=1.68\n",
      "[2091 | 21217.83] loss=1.45 avg=1.67\n",
      "[2092 | 21228.23] loss=1.33 avg=1.67\n",
      "[2093 | 21238.62] loss=1.94 avg=1.67\n",
      "[2094 | 21249.00] loss=1.95 avg=1.68\n",
      "[2095 | 21259.41] loss=1.31 avg=1.67\n",
      "[2096 | 21269.82] loss=1.03 avg=1.67\n",
      "[2097 | 21280.47] loss=2.22 avg=1.67\n",
      "[2098 | 21290.88] loss=1.83 avg=1.67\n",
      "[2099 | 21301.28] loss=2.43 avg=1.68\n",
      "[2100 | 21311.68] loss=0.95 avg=1.67\n",
      "Saving checkpoint/QGen_QUAC_test/model-2100\n",
      "[2101 | 21325.20] loss=1.73 avg=1.67\n",
      "[2102 | 21335.56] loss=2.07 avg=1.68\n",
      "[2103 | 21345.93] loss=1.57 avg=1.68\n",
      "[2104 | 21356.32] loss=1.80 avg=1.68\n",
      "[2105 | 21366.59] loss=1.92 avg=1.68\n",
      "[2106 | 21376.88] loss=1.64 avg=1.68\n",
      "[2107 | 21387.24] loss=2.76 avg=1.69\n",
      "[2108 | 21397.58] loss=2.28 avg=1.70\n",
      "[2109 | 21407.90] loss=1.57 avg=1.70\n",
      "[2110 | 21418.27] loss=1.65 avg=1.70\n",
      "[2111 | 21428.56] loss=1.32 avg=1.69\n",
      "[2112 | 21438.89] loss=1.52 avg=1.69\n",
      "[2113 | 21449.26] loss=2.31 avg=1.70\n",
      "[2114 | 21459.84] loss=2.97 avg=1.71\n",
      "[2115 | 21470.20] loss=1.62 avg=1.71\n",
      "[2116 | 21480.55] loss=1.10 avg=1.70\n",
      "[2117 | 21490.85] loss=1.91 avg=1.70\n",
      "[2118 | 21501.07] loss=3.13 avg=1.72\n",
      "[2119 | 21511.43] loss=2.24 avg=1.72\n",
      "[2120 | 21521.80] loss=1.13 avg=1.72\n",
      "[2121 | 21532.17] loss=2.26 avg=1.72\n",
      "[2122 | 21542.48] loss=2.02 avg=1.73\n",
      "[2123 | 21552.82] loss=2.41 avg=1.73\n",
      "[2124 | 21563.18] loss=1.30 avg=1.73\n",
      "[2125 | 21573.55] loss=0.92 avg=1.72\n",
      "[2126 | 21583.91] loss=1.80 avg=1.72\n",
      "[2127 | 21594.26] loss=3.08 avg=1.73\n",
      "[2128 | 21604.57] loss=1.59 avg=1.73\n",
      "[2129 | 21614.88] loss=1.10 avg=1.73\n",
      "[2130 | 21625.27] loss=1.86 avg=1.73\n",
      "[2131 | 21635.67] loss=1.45 avg=1.73\n",
      "[2132 | 21646.38] loss=0.94 avg=1.72\n",
      "[2133 | 21656.73] loss=1.72 avg=1.72\n",
      "[2134 | 21667.08] loss=2.53 avg=1.73\n",
      "[2135 | 21677.37] loss=1.80 avg=1.73\n",
      "[2136 | 21687.72] loss=1.86 avg=1.73\n",
      "[2137 | 21698.06] loss=2.80 avg=1.74\n",
      "[2138 | 21708.38] loss=1.64 avg=1.74\n",
      "[2139 | 21718.73] loss=2.05 avg=1.74\n",
      "[2140 | 21729.01] loss=1.86 avg=1.74\n",
      "[2141 | 21739.40] loss=2.28 avg=1.75\n",
      "[2142 | 21749.81] loss=1.43 avg=1.74\n",
      "[2143 | 21760.20] loss=2.96 avg=1.76\n",
      "[2144 | 21770.59] loss=1.68 avg=1.76\n",
      "[2145 | 21780.95] loss=1.88 avg=1.76\n",
      "[2146 | 21791.25] loss=1.83 avg=1.76\n",
      "[2147 | 21801.60] loss=2.56 avg=1.77\n",
      "[2148 | 21811.99] loss=1.59 avg=1.76\n",
      "[2149 | 21822.54] loss=1.69 avg=1.76\n",
      "[2150 | 21832.50] loss=1.42 avg=1.76\n",
      "[2151 | 21842.48] loss=1.44 avg=1.76\n",
      "[2152 | 21852.43] loss=0.82 avg=1.75\n",
      "[2153 | 21862.38] loss=1.06 avg=1.74\n",
      "[2154 | 21872.33] loss=1.83 avg=1.74\n",
      "[2155 | 21882.30] loss=2.27 avg=1.75\n",
      "[2156 | 21892.25] loss=1.83 avg=1.75\n",
      "[2157 | 21902.19] loss=2.11 avg=1.75\n",
      "[2158 | 21912.11] loss=2.96 avg=1.76\n",
      "[2159 | 21922.06] loss=1.48 avg=1.76\n",
      "[2160 | 21932.02] loss=0.92 avg=1.75\n",
      "[2161 | 21941.97] loss=1.28 avg=1.75\n",
      "[2162 | 21951.89] loss=1.30 avg=1.74\n",
      "[2163 | 21961.86] loss=2.22 avg=1.75\n",
      "[2164 | 21971.80] loss=1.84 avg=1.75\n",
      "[2165 | 21981.72] loss=1.22 avg=1.74\n",
      "[2166 | 21991.68] loss=1.79 avg=1.74\n",
      "[2167 | 22001.90] loss=2.30 avg=1.75\n",
      "[2168 | 22011.83] loss=1.06 avg=1.74\n",
      "[2169 | 22021.79] loss=0.99 avg=1.73\n",
      "[2170 | 22031.75] loss=1.94 avg=1.74\n",
      "[2171 | 22041.70] loss=0.79 avg=1.73\n",
      "[2172 | 22051.63] loss=2.18 avg=1.73\n",
      "[2173 | 22061.59] loss=1.46 avg=1.73\n",
      "[2174 | 22071.54] loss=1.55 avg=1.73\n",
      "[2175 | 22081.51] loss=1.84 avg=1.73\n",
      "[2176 | 22091.46] loss=1.31 avg=1.72\n",
      "[2177 | 22101.43] loss=1.63 avg=1.72\n",
      "[2178 | 22111.39] loss=1.22 avg=1.72\n",
      "[2179 | 22121.32] loss=1.22 avg=1.71\n",
      "[2180 | 22131.49] loss=1.49 avg=1.71\n",
      "[2181 | 22141.44] loss=1.33 avg=1.71\n",
      "[2182 | 22151.40] loss=1.10 avg=1.70\n",
      "[2183 | 22161.35] loss=2.07 avg=1.70\n",
      "[2184 | 22171.34] loss=1.86 avg=1.71\n",
      "[2185 | 22181.52] loss=1.25 avg=1.70\n",
      "[2186 | 22191.45] loss=2.08 avg=1.71\n",
      "[2187 | 22201.40] loss=1.69 avg=1.71\n",
      "[2188 | 22211.33] loss=1.14 avg=1.70\n",
      "[2189 | 22221.27] loss=1.36 avg=1.70\n",
      "[2190 | 22231.21] loss=0.83 avg=1.69\n",
      "[2191 | 22241.17] loss=1.73 avg=1.69\n",
      "[2192 | 22251.10] loss=2.05 avg=1.69\n",
      "[2193 | 22261.06] loss=0.97 avg=1.68\n",
      "[2194 | 22271.01] loss=1.37 avg=1.68\n",
      "[2195 | 22280.94] loss=2.00 avg=1.68\n",
      "[2196 | 22290.89] loss=2.75 avg=1.69\n",
      "[2197 | 22300.82] loss=2.02 avg=1.70\n",
      "[2198 | 22310.78] loss=2.26 avg=1.70\n",
      "[2199 | 22320.73] loss=1.31 avg=1.70\n",
      "[2200 | 22330.66] loss=2.35 avg=1.71\n",
      "Saving checkpoint/QGen_QUAC_test/model-2200\n",
      "[2201 | 22343.57] loss=2.19 avg=1.71\n",
      "[2202 | 22353.54] loss=1.77 avg=1.71\n",
      "[2203 | 22363.77] loss=1.19 avg=1.71\n",
      "[2204 | 22373.79] loss=3.14 avg=1.72\n",
      "[2205 | 22383.77] loss=1.86 avg=1.72\n",
      "[2206 | 22393.73] loss=1.22 avg=1.72\n",
      "[2207 | 22403.70] loss=1.90 avg=1.72\n",
      "[2208 | 22413.67] loss=0.89 avg=1.71\n",
      "[2209 | 22423.64] loss=1.74 avg=1.71\n",
      "[2210 | 22433.82] loss=2.22 avg=1.72\n",
      "[2211 | 22444.15] loss=2.00 avg=1.72\n",
      "[2212 | 22454.40] loss=2.19 avg=1.72\n",
      "[2213 | 22464.37] loss=1.61 avg=1.72\n",
      "[2214 | 22474.36] loss=1.28 avg=1.72\n",
      "[2215 | 22484.34] loss=2.46 avg=1.73\n",
      "[2216 | 22494.32] loss=1.47 avg=1.72\n",
      "[2217 | 22504.33] loss=0.98 avg=1.72\n",
      "[2218 | 22514.31] loss=1.91 avg=1.72\n",
      "[2219 | 22524.28] loss=0.99 avg=1.71\n",
      "[2220 | 22534.25] loss=2.24 avg=1.72\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2221 | 22544.52] loss=1.37 avg=1.71\n",
      "[2222 | 22554.46] loss=1.76 avg=1.71\n",
      "[2223 | 22564.43] loss=1.67 avg=1.71\n",
      "[2224 | 22574.38] loss=1.46 avg=1.71\n",
      "[2225 | 22584.36] loss=1.41 avg=1.71\n",
      "[2226 | 22594.33] loss=2.10 avg=1.71\n",
      "[2227 | 22604.32] loss=1.47 avg=1.71\n",
      "[2228 | 22614.27] loss=2.42 avg=1.72\n",
      "[2229 | 22624.26] loss=0.77 avg=1.71\n",
      "[2230 | 22634.21] loss=1.78 avg=1.71\n",
      "[2231 | 22644.19] loss=1.70 avg=1.71\n",
      "[2232 | 22654.16] loss=2.45 avg=1.71\n",
      "[2233 | 22664.12] loss=1.40 avg=1.71\n",
      "[2234 | 22674.10] loss=1.50 avg=1.71\n",
      "[2235 | 22684.08] loss=1.58 avg=1.71\n",
      "[2236 | 22694.05] loss=0.82 avg=1.70\n",
      "[2237 | 22704.02] loss=1.17 avg=1.69\n",
      "[2238 | 22714.01] loss=2.61 avg=1.70\n",
      "[2239 | 22724.25] loss=2.68 avg=1.71\n",
      "[2240 | 22734.21] loss=2.45 avg=1.72\n",
      "[2241 | 22744.22] loss=2.94 avg=1.73\n",
      "[2242 | 22754.26] loss=1.46 avg=1.73\n",
      "[2243 | 22764.65] loss=0.98 avg=1.72\n",
      "[2244 | 22775.04] loss=1.06 avg=1.72\n",
      "[2245 | 22785.31] loss=0.89 avg=1.71\n",
      "[2246 | 22795.29] loss=1.12 avg=1.70\n",
      "[2247 | 22805.28] loss=1.70 avg=1.70\n",
      "[2248 | 22815.27] loss=2.14 avg=1.71\n",
      "[2249 | 22825.22] loss=2.22 avg=1.71\n",
      "[2250 | 22835.16] loss=1.86 avg=1.71\n",
      "[2251 | 22845.10] loss=1.80 avg=1.71\n",
      "[2252 | 22855.05] loss=1.92 avg=1.72\n",
      "[2253 | 22864.99] loss=3.55 avg=1.73\n",
      "[2254 | 22874.94] loss=1.53 avg=1.73\n",
      "[2255 | 22884.89] loss=2.02 avg=1.73\n",
      "[2256 | 22894.85] loss=1.61 avg=1.73\n",
      "[2257 | 22905.12] loss=2.26 avg=1.74\n",
      "[2258 | 22915.08] loss=1.27 avg=1.73\n",
      "[2259 | 22925.03] loss=1.56 avg=1.73\n",
      "[2260 | 22934.99] loss=1.74 avg=1.73\n",
      "[2261 | 22944.93] loss=2.20 avg=1.74\n",
      "[2262 | 22954.88] loss=1.44 avg=1.73\n",
      "[2263 | 22964.83] loss=2.06 avg=1.74\n",
      "[2264 | 22974.78] loss=0.99 avg=1.73\n",
      "[2265 | 22984.73] loss=1.06 avg=1.72\n",
      "[2266 | 22994.68] loss=0.88 avg=1.71\n",
      "[2267 | 23004.61] loss=0.91 avg=1.71\n",
      "[2268 | 23014.56] loss=1.39 avg=1.70\n",
      "[2269 | 23024.50] loss=1.47 avg=1.70\n",
      "[2270 | 23034.46] loss=0.73 avg=1.69\n",
      "[2271 | 23044.40] loss=1.41 avg=1.69\n",
      "[2272 | 23054.33] loss=2.05 avg=1.69\n",
      "[2273 | 23064.27] loss=1.08 avg=1.69\n",
      "[2274 | 23074.21] loss=1.36 avg=1.68\n",
      "[2275 | 23084.44] loss=1.31 avg=1.68\n",
      "[2276 | 23094.39] loss=2.27 avg=1.68\n",
      "[2277 | 23104.36] loss=1.22 avg=1.68\n",
      "[2278 | 23114.31] loss=1.21 avg=1.68\n",
      "[2279 | 23124.26] loss=1.77 avg=1.68\n",
      "[2280 | 23133.88] loss=1.32 avg=1.67\n",
      "[2281 | 23143.73] loss=1.17 avg=1.67\n",
      "[2282 | 23153.14] loss=1.18 avg=1.66\n",
      "[2283 | 23162.53] loss=1.76 avg=1.66\n",
      "[2284 | 23171.93] loss=1.19 avg=1.66\n",
      "[2285 | 23181.31] loss=2.27 avg=1.67\n",
      "[2286 | 23190.71] loss=1.68 avg=1.67\n",
      "[2287 | 23200.09] loss=2.01 avg=1.67\n",
      "[2288 | 23209.49] loss=2.48 avg=1.68\n",
      "[2289 | 23218.89] loss=1.13 avg=1.67\n",
      "[2290 | 23228.26] loss=2.39 avg=1.68\n",
      "[2291 | 23237.64] loss=1.55 avg=1.68\n",
      "[2292 | 23247.03] loss=1.26 avg=1.67\n",
      "[2293 | 23256.48] loss=1.03 avg=1.67\n",
      "[2294 | 23266.08] loss=1.06 avg=1.66\n",
      "[2295 | 23275.46] loss=1.41 avg=1.66\n",
      "[2296 | 23284.88] loss=0.87 avg=1.65\n",
      "[2297 | 23294.27] loss=2.25 avg=1.66\n",
      "[2298 | 23303.64] loss=1.05 avg=1.65\n",
      "[2299 | 23313.46] loss=1.96 avg=1.65\n",
      "[2300 | 23323.40] loss=1.88 avg=1.66\n",
      "Saving checkpoint/QGen_QUAC_test/model-2300\n",
      "[2301 | 23336.27] loss=1.41 avg=1.65\n",
      "[2302 | 23346.20] loss=1.28 avg=1.65\n",
      "[2303 | 23356.15] loss=1.24 avg=1.64\n",
      "[2304 | 23366.07] loss=1.43 avg=1.64\n",
      "[2305 | 23376.00] loss=1.68 avg=1.64\n",
      "[2306 | 23385.92] loss=1.11 avg=1.64\n",
      "[2307 | 23395.85] loss=1.25 avg=1.63\n",
      "[2308 | 23405.77] loss=1.03 avg=1.63\n",
      "[2309 | 23415.70] loss=1.64 avg=1.63\n",
      "[2310 | 23425.61] loss=1.32 avg=1.62\n",
      "[2311 | 23435.55] loss=1.71 avg=1.63\n",
      "[2312 | 23445.78] loss=2.22 avg=1.63\n",
      "[2313 | 23455.69] loss=1.59 avg=1.63\n",
      "[2314 | 23465.60] loss=1.76 avg=1.63\n",
      "[2315 | 23475.50] loss=3.18 avg=1.65\n",
      "[2316 | 23485.44] loss=1.27 avg=1.64\n",
      "[2317 | 23495.38] loss=1.40 avg=1.64\n",
      "[2318 | 23505.34] loss=1.44 avg=1.64\n",
      "[2319 | 23515.30] loss=2.17 avg=1.65\n",
      "[2320 | 23525.29] loss=3.20 avg=1.66\n",
      "[2321 | 23535.25] loss=2.77 avg=1.67\n",
      "[2322 | 23545.20] loss=1.45 avg=1.67\n",
      "[2323 | 23555.12] loss=1.95 avg=1.67\n",
      "[2324 | 23565.44] loss=1.61 avg=1.67\n",
      "[2325 | 23575.73] loss=1.06 avg=1.67\n",
      "[2326 | 23585.95] loss=2.56 avg=1.67\n",
      "[2327 | 23596.26] loss=1.88 avg=1.68\n",
      "[2328 | 23606.56] loss=1.29 avg=1.67\n",
      "[2329 | 23616.91] loss=1.34 avg=1.67\n",
      "[2330 | 23627.43] loss=2.18 avg=1.67\n",
      "[2331 | 23637.66] loss=1.40 avg=1.67\n",
      "[2332 | 23647.90] loss=2.90 avg=1.68\n",
      "[2333 | 23658.09] loss=1.20 avg=1.68\n",
      "[2334 | 23668.38] loss=1.19 avg=1.67\n",
      "[2335 | 23678.60] loss=1.10 avg=1.67\n",
      "[2336 | 23688.88] loss=2.92 avg=1.68\n",
      "[2337 | 23699.07] loss=1.40 avg=1.68\n",
      "[2338 | 23709.33] loss=1.51 avg=1.68\n",
      "[2339 | 23719.61] loss=1.08 avg=1.67\n",
      "[2340 | 23729.91] loss=1.94 avg=1.67\n",
      "[2341 | 23740.13] loss=2.09 avg=1.68\n",
      "[2342 | 23750.33] loss=1.52 avg=1.68\n",
      "[2343 | 23760.61] loss=0.96 avg=1.67\n",
      "[2345 | 23781.12] loss=1.36 avg=1.67\n",
      "[2346 | 23791.37] loss=2.42 avg=1.68\n",
      "[2348 | 23812.18] loss=1.94 avg=1.68\n",
      "[2349 | 23822.67] loss=2.33 avg=1.68\n",
      "[2350 | 23833.06] loss=0.87 avg=1.68\n",
      "[2351 | 23843.44] loss=1.12 avg=1.67\n",
      "[2352 | 23853.60] loss=2.32 avg=1.68\n",
      "[2353 | 23863.87] loss=1.92 avg=1.68\n",
      "[2354 | 23873.95] loss=2.33 avg=1.69\n",
      "[2355 | 23884.11] loss=1.76 avg=1.69\n",
      "[2356 | 23894.33] loss=2.40 avg=1.69\n",
      "[2357 | 23904.53] loss=3.16 avg=1.71\n",
      "[2448 | 24838.78] loss=2.33 avg=1.73\n",
      "[2449 | 24848.91] loss=2.23 avg=1.73\n",
      "[2450 | 24859.22] loss=1.64 avg=1.73\n",
      "[2451 | 24869.47] loss=2.12 avg=1.74\n",
      "[2452 | 24880.20] loss=0.66 avg=1.73\n",
      "[2453 | 24890.40] loss=1.08 avg=1.72\n",
      "[2454 | 24900.58] loss=1.35 avg=1.72\n",
      "[2455 | 24910.73] loss=3.28 avg=1.73\n",
      "[2456 | 24920.93] loss=1.85 avg=1.73\n",
      "[2457 | 24931.15] loss=1.46 avg=1.73\n",
      "[2458 | 24941.36] loss=1.65 avg=1.73\n",
      "[2684 | 27255.30] loss=2.36 avg=1.65\n",
      "[2685 | 27265.47] loss=2.51 avg=1.66\n",
      "[2686 | 27275.63] loss=1.35 avg=1.66\n",
      "[2952 | 29967.36] loss=1.82 avg=1.65\n",
      "[2953 | 29977.26] loss=1.05 avg=1.65\n",
      "[2954 | 29987.16] loss=1.85 avg=1.65\n",
      "[2955 | 29997.09] loss=1.37 avg=1.65\n"
     ]
    }
   ],
   "source": [
    "gpt2.finetune(qsess,\n",
    "              dataset= \"data/quac_train.txt\", \n",
    "              model_name='345M', \n",
    "              steps=4000, \n",
    "              restore_from='fresh', \n",
    "              run_name='QGen_QUAC_test', \n",
    "              print_every=1, \n",
    "              sample_every=2000, \n",
    "              save_every=100  \n",
    "             )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Pre-trained Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = open('data/quac_test.txt')\n",
    "testset = file.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_lst = testset.split('<|endoftext|>\\n')\n",
    "\n",
    "test_str = test_lst[0].split('[QUESTION]:')[0]\n",
    "\n",
    "ctx = test_str\n",
    "pre = ctx + \" [QUESTION]:\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading checkpoint checkpoint/QGen_QUAC_test/model-4000\n",
      "INFO:tensorflow:Restoring parameters from checkpoint/QGen_QUAC_test/model-4000\n"
     ]
    }
   ],
   "source": [
    "qsess = gpt2.start_tf_sess()\n",
    "gpt2.load_gpt2(qsess, run_name='QGen_QUAC_test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|startoftext|>\n",
      "[CONTEXT]: In May 1983, she married Nikos Karvelas, a composer, with whom she collaborated in 1975 and in November she gave birth to her daughter Sofia. After their marriage, she started a close collaboration with Karvelas. Since 1975, all her releases have become gold or platinum and have included songs by Karvelas. In 1986, she participated at the Cypriot National Final for Eurovision Song Contest with the song Thelo Na Gino Star (\"I Want To Be A Star\"), taking second place. This song is still unreleased up to date. In 1984, Vissi left her record company EMI Greece and signed with CBS Records Greece, which later became Sony Music Greece, a collaboration that lasted until 2013. In March 1984, she released Na 'Hes Kardia (\"If You Had a Heart\"). The album was certified gold. The following year her seventh album Kati Simveni (\"Something Is Happening\") was released which included one of her most famous songs, titled \"Dodeka\" [\"Twelve (O'Clock)\"] and reached gold status selling 80.000 units. In 1986 I Epomeni Kinisi (\"The Next Move\") was released. The album included the hit Pragmata (\"Things\") and went platinum, becoming the best selling record of the year. In February 1988 she released her ninth album Tora (\"Now\") and in December the album Empnefsi! (\"Inspiration!\") which went gold. In 1988, she made her debut as a radio producer on ANT1 Radio. Her radio program was titled after one of her songs Ta Koritsia Einai Atakta (\"Girls Are Naughty\") and was aired every weekend. In the same year, she participated with the song Klaio (\"I'm Crying\") at the Greek National Final for Eurovision Song Contest, finishing third. In 1989, she released the highly successful studio album Fotia (Fire), being one of the first albums to feature western sounds. The lead single Pseftika (\"Fake\") became a big hit and the album reached platinum status, selling 180.000 copies and becoming the second best selling record of 1990. She performed at \"Diogenis Palace\" in that same year, Athens's biggest nightclub/music hall at the time. CANNOTANSWER  [QUESTION]: Did she participate in any other events? <|endoftext|>\n",
      "<|startoftext|>\n",
      "[CONTEXT]: In May 1983, she married Nikos Karvelas, a composer, with whom she collaborated in 1975 and in November she gave birth to her daughter Sofia. After their marriage, she started a close collaboration with Karvelas. Since 1975, all her releases have become gold or platinum and have included songs by Karvelas. In 1986, she participated at the Cypriot National Final for Eurovision Song Contest with the song Thelo Na Gino Star (\"I Want To Be A Star\"), taking second place. This song is still unreleased up to date. In 1984, Vissi left her record company EMI Greece and signed with CBS Records Greece, which later became Sony Music Greece, a collaboration that lasted until 2013. In March 1984, she released Na 'Hes Kardia (\"If You Had a Heart\"). The album was certified gold. The following year her seventh album Kati Simveni (\"The Next Move\") was released which included one of her most famous songs, titled \"Dodeka\" [\"Twelve (O'Clock)\"] and reached gold status selling 80.000 units. In 1986 I Epomeni Kinisi (\"The Next Move\") was released which included the hit Pragmata (\"Things\") and went platinum, becoming the best selling record of the year. In February 1988 she released her ninth album Tora (\"Now\") and in December the album Empnefsi! (\"Inspiration!\") which went gold. In 1988, she made her debut as a radio producer on ANT1 Radio. Her radio program was titled after one of her songs Ta Koritsia Einai Atakta (\"Girls Are Naughty\") and was aired every weekend. In the same year, she participated with the song Klaio (\"I'm Crying\") at the Greek National Final for Eurovision Song Contest, finishing third. In 1989, she released the highly successful studio album Fotia (Fire), being one of the first albums to feature western sounds. The lead single Pseftika (\"Fake\") became a big hit and the album reached platinum status, selling 180.000 copies and becoming the second best selling record of 1990. She performed at \"Diogenis Palace\" in that same year, Athens's biggest nightclub/music hall at the time. CANNOTANSWER [QUESTION]: What did he do after that? <|endoftext|>\n",
      "<|startoftext|>\n",
      "[CONTEXT]: In May\n",
      "====================\n",
      "<|startoftext|>\n",
      "[CONTEXT]: In May 1983, she married Nikos Karvelas, a composer, with whom she collaborated in 1975 and in November she gave birth to her daughter Sofia. After their marriage, she started a close collaboration with Karvelas. Since 1975, all her releases have become gold or platinum and have included songs by Karvelas. In 1986, she participated at the Cypriot National Final for Eurovision Song Contest with the song Thelo Na Gino Star (\"I Want To Be A Star\"), taking second place. This song is still unreleased up to date. In 1984, Vissi left her record company EMI Greece and signed with CBS Records Greece, which later became Sony Music Greece, a collaboration that lasted until 2013. In March 1984, she released Na 'Hes Kardia (\"If You Had a Heart\"). The album was certified gold. The following year her seventh album Kati Simveni (\"Something Is Happening\") was released which included one of her most famous songs, titled \"Dodeka\" [\"Twelve (O'Clock)\"] and reached gold status selling 80.000 units. In 1986 I Epomeni Kinisi (\"The Next Move\") was released. The album included the hit Pragmata (\"Things\") and went platinum, becoming the best selling record of the year. In February 1988 she released her ninth album Tora (\"Now\") and in December the album Empnefsi! (\"Inspiration!\") which went gold. In 1988, she made her debut as a radio producer on ANT1 Radio. Her radio program was titled after one of her songs Ta Koritsia Einai Atakta (\"Girls Are Naughty\") and was aired every weekend. In the same year, she participated with the song Klaio (\"I'm Crying\") at the Greek National Final for Eurovision Song Contest, finishing third. In 1989, she released the highly successful studio album Fotia (Fire), being one of the first albums to feature western sounds. The lead single Pseftika (\"Fake\") became a big hit and the album reached platinum status, selling 180.000 copies and becoming the second best selling record of 1990. She performed at \"Diogenis Palace\" in that same year, Athens's biggest nightclub/music hall at the time. CANNOTANSWER  [QUESTION]: Did she continue recording when she left EMI? <|endoftext|>\n",
      "<|startoftext|>\n",
      "[CONTEXT]: In May 1983, she married Nikos Karvelas, a composer, with whom she collaborated in 1975 and in November she gave birth to her daughter Sofia. After their marriage, she started a close collaboration with Karvelas. Since 1975, all her releases have become gold or platinum and have included songs by Karvelas. In 1986, she participated at the Cypriot National Final for Eurovision Song Contest with the song Thelo Na Gino Star (\"I Want To Be A Star\"), taking second place. This song is still unreleased up to date. In 1984, Vissi left her record company EMI Greece and signed with CBS Records Greece, which later became Sony Music Greece, a collaboration that lasted until 2013. In March 1984, she released Na 'Hes Kardia (\"If You Had a Heart\"). The album was certified gold. The following year her seventh album Kati Simveni (\"The Next Move\") was released which included one of her most famous songs, titled \"Dodeka\" [\"Twelve (O'Clock)\"] and reached gold status selling 80.000 units. In 1986 I Epomeni Kinisi (\"The Next Move\") was released which included the hit Pragmata (\"Things\") and went platinum, becoming the best selling record of the year. In February 1988 she released her ninth album Tora (\"Now\") and in December the album Empnefsi! (\"Inspiration!\") which went gold. In 1988, she made her debut as a radio producer on ANT1 Radio. Her radio program was titled after one of her songs Ta Koritsia Einai Atakta (\"Girls Are Naughty\") and was aired every weekend. In the same year, she participated with the song Klaio (\"I'm Crying\") at the Greek National Final for Eurovision Song Contest, finishing third. In 1989, she released the highly successful studio album Fotia (Fire), being one of the first albums to feature western sounds. The lead single Pseftika (\"Fake\") became a big hit and the album reached platinum status, selling 180.000 copies and becoming the second best selling record of 1990. She performed at \"Diogenis Palace\" in that same year, Athens's biggest nightclub/music hall at the time. CANNOTANSWER [QUESTION]: What was her next album? <|endoftext|>\n",
      "<|startoftext|>\n",
      "[CONTEXT]: In\n",
      "====================\n",
      "<|startoftext|>\n",
      "[CONTEXT]: In May 1983, she married Nikos Karvelas, a composer, with whom she collaborated in 1975 and in November she gave birth to her daughter Sofia. After their marriage, she started a close collaboration with Karvelas. Since 1975, all her releases have become gold or platinum and have included songs by Karvelas. In 1986, she participated at the Cypriot National Final for Eurovision Song Contest with the song Thelo Na Gino Star (\"I Want To Be A Star\"), taking second place. This song is still unreleased up to date. In 1984, Vissi left her record company EMI Greece and signed with CBS Records Greece, which later became Sony Music Greece, a collaboration that lasted until 2013. In March 1984, she released Na 'Hes Kardia (\"If You Had a Heart\"). The album was certified gold. The following year her seventh album Kati Simveni (\"Something Is Happening\") was released which included one of her most famous songs, titled \"Dodeka\" [\"Twelve (O'Clock)\"] and reached gold status selling 80.000 units. In 1986 I Epomeni Kinisi (\"The Next Move\") was released. The album included the hit Pragmata (\"Things\") and went platinum, becoming the best selling record of the year. In February 1988 she released her ninth album Tora (\"Now\") and in December the album Empnefsi! (\"Inspiration!\") which went gold. In 1988, she made her debut as a radio producer on ANT1 Radio. Her radio program was titled after one of her songs Ta Koritsia Einai Atakta (\"Girls Are Naughty\") and was aired every weekend. In the same year, she participated with the song Klaio (\"I'm Crying\") at the Greek National Final for Eurovision Song Contest, finishing third. In 1989, she released the highly successful studio album Fotia (Fire), being one of the first albums to feature western sounds. The lead single Pseftika (\"Fake\") became a big hit and the album reached platinum status, selling 180.000 copies and becoming the second best selling record of 1990. She performed at \"Diogenis Palace\" in that same year, Athens's biggest nightclub/music hall at the time. CANNOTANSWER  [QUESTION]: what was the most important aspect of her career? <|endoftext|>\n",
      "<|startoftext|>\n",
      "[CONTEXT]: In May 1983, she married Nikos Karvelas, a composer, with whom she collaborated in 1975 and in November she gave birth to her daughter Sofia. After their marriage, she started a close collaboration with Karvelas. Since 1975, all her releases have become gold or platinum and have included songs by Karvelas. In 1986, she participated at the Cypriot National Final for Eurovision Song Contest with the song Thelo Na Gino Star (\"I Want To Be A Star\"), taking second place. This song is still unreleased up to date. In 1984, Vissi left her record company EMI Greece and signed with CBS Records, which later became Sony Music Greece, a collaboration that lasted until 2013. In March 1984, she released Na 'Hes Kardia (\"If You Had a Heart\"). The album was certified gold. The following year her seventh album Kati Simveni (\"The Next Move\") was released which included one of her most famous songs, titled \"Dodeka\" [\"Twelve (O'Clock)\"] and reached gold status selling 80.000 units. In 1986 I Epomeni Kinisi (\"The Next Move\") was released which included the hit Pragmata (\"Things\") and went platinum, becoming the best selling record of the year. In February 1988 she released her ninth album Tora (\"Now\") and in December the album Empnefsi! (\"Inspiration!\") which went gold. In 1988, she made her debut as a radio producer on ANT1 Radio. Her radio program was titled after one of her songs Ta Koritsia Einai Atakta (\"Girls Are Naughty\") and was aired every weekend. In the same year, she participated with the song Klaio (\"I'm Crying\") at the Greek National Final for Eurovision Song Contest, finishing third. In 1989, she released the highly successful studio album Fotia (Fire), being one of the first albums to feature western sounds. The lead single Pseftika (\"Fake\") became a big hit and the album reached platinum status, selling 180.000 copies and becoming the second best selling record of 1990. She performed at \"Diogenis Palace\" in that same year, Athens's biggest nightclub/music hall at the time. CANNOTANSWER [QUESTION]: did she produce any other songs? <|endoftext|>\n",
      "<|startoftext|>\n",
      "[CONTEXT]: In\n",
      "====================\n",
      "<|startoftext|>\n",
      "[CONTEXT]: In May 1983, she married Nikos Karvelas, a composer, with whom she collaborated in 1975 and in November she gave birth to her daughter Sofia. After their marriage, she started a close collaboration with Karvelas. Since 1975, all her releases have become gold or platinum and have included songs by Karvelas. In 1986, she participated at the Cypriot National Final for Eurovision Song Contest with the song Thelo Na Gino Star (\"I Want To Be A Star\"), taking second place. This song is still unreleased up to date. In 1984, Vissi left her record company EMI Greece and signed with CBS Records Greece, which later became Sony Music Greece, a collaboration that lasted until 2013. In March 1984, she released Na 'Hes Kardia (\"If You Had a Heart\"). The album was certified gold. The following year her seventh album Kati Simveni (\"Something Is Happening\") was released which included one of her most famous songs, titled \"Dodeka\" [\"Twelve (O'Clock)\"] and reached gold status selling 80.000 units. In 1986 I Epomeni Kinisi (\"The Next Move\") was released. The album included the hit Pragmata (\"Things\") and went platinum, becoming the best selling record of the year. In February 1988 she released her ninth album Tora (\"Now\") and in December the album Empnefsi! (\"Inspiration!\") which went gold. In 1988, she made her debut as a radio producer on ANT1 Radio. Her radio program was titled after one of her songs Ta Koritsia Einai Atakta (\"Girls Are Naughty\") and was aired every weekend. In the same year, she participated with the song Klaio (\"I'm Crying\") at the Greek National Final for Eurovision Song Contest, finishing third. In 1989, she released the highly successful studio album Fotia (Fire), being one of the first albums to feature western sounds. The lead single Pseftika (\"Fake\") became a big hit and the album reached platinum status, selling 180.000 copies and becoming the second best selling record of 1990. She performed at \"Diogenis Palace\" in that same year, Athens's biggest nightclub/music hall at the time. CANNOTANSWER  [QUESTION]: What did she do in 1982? <|endoftext|>\n",
      "<|startoftext|>\n",
      "[CONTEXT]: In May 1983, she married Nikos Karvelas, a composer, with whom she collaborated in 1975 and in November she gave birth to her daughter Sofia. After their marriage, she started a close collaboration with Karvelas. Since 1975, all her releases have become gold or platinum and have included songs by Karvelas. In 1986, she participated at the Cypriot National Final for Eurovision Song Contest with the song Thelo Na Gino Star (\"I Want To Be A Star\"), taking second place. This song is still unreleased up to date. In 1984, Vissi left her record company EMI Greece and signed with CBS Records Greece, which later became Sony Music Greece, a collaboration that lasted until 2013. In March 1984, she released Na 'Hes Kardia (\"If You Had a Heart\"). The album was certified gold. The following year her seventh album Kati Simveni (\"The Next Move\") was released which included one of her most famous songs, titled \"Dodeka\" [\"Twelve (O'Clock)\"] and reached gold status selling 80.000 units. In 1986 I Epomeni Kinisi (\"The Next Move\") was released which included the hit Pragmata (\"Things\") and went platinum, becoming the best selling record of the year. In February 1988 she released her ninth album Tora (\"Now\") and in December the album Empnefsi! (\"Inspiration!\") which went gold. In 1988, she made her debut as a radio producer on ANT1 Radio. Her radio program was titled after one of her songs Ta Koritsia Einai Atakta (\"Girls Are Naughty\") and was aired every weekend. In the same year, she participated with the song Klaio (\"I'm Crying\") at the Greek National Final for Eurovision Song Contest, finishing third. In 1989, she released the highly successful studio album Fotia (Fire), being one of the first albums to feature western sounds. The lead single Pseftika (\"Fake\") became a big hit and the album reached platinum status, selling 180.000 copies and becoming the second best selling record of 1990. She performed at \"Diogenis Palace\" in that same year, Athens's biggest nightclub/music hall at the time. CANNOTANSWER [QUESTION]: What is the name of the album which she made? <|endoftext|>\n",
      "<|startoftext|>\n",
      "[CONTEXT\n",
      "====================\n",
      "<|startoftext|>\n",
      "[CONTEXT]: In May 1983, she married Nikos Karvelas, a composer, with whom she collaborated in 1975 and in November she gave birth to her daughter Sofia. After their marriage, she started a close collaboration with Karvelas. Since 1975, all her releases have become gold or platinum and have included songs by Karvelas. In 1986, she participated at the Cypriot National Final for Eurovision Song Contest with the song Thelo Na Gino Star (\"I Want To Be A Star\"), taking second place. This song is still unreleased up to date. In 1984, Vissi left her record company EMI Greece and signed with CBS Records Greece, which later became Sony Music Greece, a collaboration that lasted until 2013. In March 1984, she released Na 'Hes Kardia (\"If You Had a Heart\"). The album was certified gold. The following year her seventh album Kati Simveni (\"Something Is Happening\") was released which included one of her most famous songs, titled \"Dodeka\" [\"Twelve (O'Clock)\"] and reached gold status selling 80.000 units. In 1986 I Epomeni Kinisi (\"The Next Move\") was released. The album included the hit Pragmata (\"Things\") and went platinum, becoming the best selling record of the year. In February 1988 she released her ninth album Tora (\"Now\") and in December the album Empnefsi! (\"Inspiration!\") which went gold. In 1988, she made her debut as a radio producer on ANT1 Radio. Her radio program was titled after one of her songs Ta Koritsia Einai Atakta (\"Girls Are Naughty\") and was aired every weekend. In the same year, she participated with the song Klaio (\"I'm Crying\") at the Greek National Final for Eurovision Song Contest, finishing third. In 1989, she released the highly successful studio album Fotia (Fire), being one of the first albums to feature western sounds. The lead single Pseftika (\"Fake\") became a big hit and the album reached platinum status, selling 180.000 copies and becoming the second best selling record of 1990. She performed at \"Diogenis Palace\" in that same year, Athens's biggest nightclub/music hall at the time. CANNOTANSWER  [QUESTION]: Did she win any awards? <|endoftext|>\n",
      "<|startoftext|>\n",
      "[CONTEXT]: In May 1983, she married Nikos Karvelas, a composer, with whom she collaborated in 1975 and in November she gave birth to her daughter Sofia. After their marriage, she started a close collaboration with Karvelas. Since 1975, all her releases have become gold or platinum and have included songs by Karvelas. In 1986, she participated at the Cypriot National Final for Eurovision Song Contest with the song Thelo Na Gino Star (\"I Want To Be A Star\"), taking second place. This song is still unreleased up to date. In 1984, Vissi left her record company EMI Greece and signed with CBS Records, which later became Sony Music Greece, a collaboration that lasted until 2013. In March 1984, she released Na 'Hes Kardia (\"If You Had a Heart\"). The album was certified gold. The following year her seventh album Kati Simveni (\"The Next Move\") was released which included one of her most famous songs, titled \"Dodeka\" [\"Twelve (O'Clock)\"] and reached gold status selling 80.000 units. In 1986 I Epomeni Kinisi (\"The Next Move\") was released which included the hit Pragmata (\"Things\") and went platinum, becoming the best selling record of the year. In February 1988 she released her ninth album Tora (\"Now\") and in December the album Empnefsi! (\"Inspiration!\") which went gold. In 1988, she made her debut as a radio producer on ANT1 Radio. Her radio program was titled after one of her songs Ta Koritsia Einai Atakta (\"Girls Are Naughty\") and was aired every weekend. In the same year, she participated with the song Klaio (\"I'm Crying\") at the Greek National Final for Eurovision Song Contest, finishing third. In 1989, she released the highly successful studio album Fotia (Fire), being one of the first albums to feature western sounds. The lead single Pseftika (\"Fake\") became a big hit and the album reached platinum status, selling 180.000 copies and becoming the second best selling record of 1990. She performed at \"Diogenis Palace\" in that same year, Athens's biggest nightclub/music hall at the time. CANNOTANSWER [QUESTION]: What music album was she working with? <|endoftext|>\n",
      "<|startoftext|>\n",
      "[CONTEXT]: In May 1983,\n",
      "====================\n",
      "<|startoftext|>\n",
      "[CONTEXT]: In May 1983, she married Nikos Karvelas, a composer, with whom she collaborated in 1975 and in November she gave birth to her daughter Sofia. After their marriage, she started a close collaboration with Karvelas. Since 1975, all her releases have become gold or platinum and have included songs by Karvelas. In 1986, she participated at the Cypriot National Final for Eurovision Song Contest with the song Thelo Na Gino Star (\"I Want To Be A Star\"), taking second place. This song is still unreleased up to date. In 1984, Vissi left her record company EMI Greece and signed with CBS Records Greece, which later became Sony Music Greece, a collaboration that lasted until 2013. In March 1984, she released Na 'Hes Kardia (\"If You Had a Heart\"). The album was certified gold. The following year her seventh album Kati Simveni (\"Something Is Happening\") was released which included one of her most famous songs, titled \"Dodeka\" [\"Twelve (O'Clock)\"] and reached gold status selling 80.000 units. In 1986 I Epomeni Kinisi (\"The Next Move\") was released. The album included the hit Pragmata (\"Things\") and went platinum, becoming the best selling record of the year. In February 1988 she released her ninth album Tora (\"Now\") and in December the album Empnefsi! (\"Inspiration!\") which went gold. In 1988, she made her debut as a radio producer on ANT1 Radio. Her radio program was titled after one of her songs Ta Koritsia Einai Atakta (\"Girls Are Naughty\") and was aired every weekend. In the same year, she participated with the song Klaio (\"I'm Crying\") at the Greek National Final for Eurovision Song Contest, finishing third. In 1989, she released the highly successful studio album Fotia (Fire), being one of the first albums to feature western sounds. The lead single Pseftika (\"Fake\") became a big hit and the album reached platinum status, selling 180.000 copies and becoming the second best selling record of 1990. She performed at \"Diogenis Palace\" in that same year, Athens's biggest nightclub/music hall at the time. CANNOTANSWER  [QUESTION]: What was her biggest hit to date? <|endoftext|>\n",
      "<|startoftext|>\n",
      "[CONTEXT]: In May 1983, she married Nikos Karvelas, a composer, with whom she collaborated in 1975 and in November she gave birth to her daughter Sofia. After their marriage, she started a close collaboration with Karvelas. Since 1975, all her releases have become gold or platinum and have included songs by Karvelas. In 1986, she participated at the Cypriot National Final for Eurovision Song Contest with the song Thelo Na Gino Star (\"I Want To Be A Star\"), taking second place. This song is still unreleased up to date. In 1984, Vissi left her record company EMI Greece and signed with CBS Records, which later became Sony Music Greece, a collaboration that lasted until 2013. In March 1984, she released Na 'Hes Kardia (\"If You Had a Heart\"). The album was certified gold. The following year her seventh album Kati Simveni (\"The Next Move\") was released which included one of her most famous songs, titled \"Dodeka\" [\"Twelve (O'Clock)\"] and reached gold status selling 80.000 units. In 1986 I Epomeni Kinisi (\"The Next Move\") was released which included the hit Pragmata (\"Things\") and went platinum, becoming the best selling record of the year. In February 1988 she released her ninth album Tora (\"Now\") and in December the album Empnefsi! (\"Inspiration!\") which went gold. In 1988, she made her debut as a radio producer on ANT1 Radio. Her radio program was titled after one of her songs Ta Koritsia Einai Atakta (\"Girls Are Naughty\") and was aired every weekend. In the same year, she participated with the song Klaio (\"I'm Crying\") at the Greek National Final for Eurovision Song Contest, finishing third. In 1989, she released the highly successful studio album Fotia (Fire), being one of the first albums to feature western sounds. The lead single Pseftika (\"Fake\") became a big hit and the album reached platinum status, selling 180.000 copies and becoming the second best selling record of 1990. She performed at \"Diogenis Palace\" in that same year, Athens's biggest nightclub/music hall at the time. CANNOTANSWER [QUESTION]: What became of his musical career after leaving EMI? <|endoftext|>\n",
      "<|startoftext|>\n",
      "[CONTEXT\n",
      "====================\n",
      "<|startoftext|>\n",
      "[CONTEXT]: In May 1983, she married Nikos Karvelas, a composer, with whom she collaborated in 1975 and in November she gave birth to her daughter Sofia. After their marriage, she started a close collaboration with Karvelas. Since 1975, all her releases have become gold or platinum and have included songs by Karvelas. In 1986, she participated at the Cypriot National Final for Eurovision Song Contest with the song Thelo Na Gino Star (\"I Want To Be A Star\"), taking second place. This song is still unreleased up to date. In 1984, Vissi left her record company EMI Greece and signed with CBS Records Greece, which later became Sony Music Greece, a collaboration that lasted until 2013. In March 1984, she released Na 'Hes Kardia (\"If You Had a Heart\"). The album was certified gold. The following year her seventh album Kati Simveni (\"Something Is Happening\") was released which included one of her most famous songs, titled \"Dodeka\" [\"Twelve (O'Clock)\"] and reached gold status selling 80.000 units. In 1986 I Epomeni Kinisi (\"The Next Move\") was released. The album included the hit Pragmata (\"Things\") and went platinum, becoming the best selling record of the year. In February 1988 she released her ninth album Tora (\"Now\") and in December the album Empnefsi! (\"Inspiration!\") which went gold. In 1988, she made her debut as a radio producer on ANT1 Radio. Her radio program was titled after one of her songs Ta Koritsia Einai Atakta (\"Girls Are Naughty\") and was aired every weekend. In the same year, she participated with the song Klaio (\"I'm Crying\") at the Greek National Final for Eurovision Song Contest, finishing third. In 1989, she released the highly successful studio album Fotia (Fire), being one of the first albums to feature western sounds. The lead single Pseftika (\"Fake\") became a big hit and the album reached platinum status, selling 180.000 copies and becoming the second best selling record of 1990. She performed at \"Diogenis Palace\" in that same year, Athens's biggest nightclub/music hall at the time. CANNOTANSWER  [QUESTION]: What was the situation for in 1986? <|endoftext|>\n",
      "<|startoftext|>\n",
      "[CONTEXT]: In May 1983, she married Nikos Karvelas, a composer, with whom she collaborated in 1975 and in November she gave birth to her daughter Sofia. After their marriage, she started a close collaboration with Karvelas. Since 1975, all her releases have become gold or platinum and have included songs by Karvelas. In 1986, she participated at the Cypriot National Final for Eurovision Song Contest with the song Thelo Na Gino Star (\"I Want To Be A Star\"), taking second place. This song is still unreleased up to date. In 1984, Vissi left her record company EMI Greece and signed with CBS Records, which later became Sony Music Greece, a collaboration that lasted until 2013. In March 1984, she released Na 'Hes Kardia (\"If You Had a Heart\"). The album was certified gold. The following year her seventh album Kati Simveni (\"The Next Move\") was released which included one of her most famous songs, titled \"Dodeka\" [\"Twelve (O'Clock)\"] and reached gold status selling 80.000 units. In 1986 I Epomeni Kinisi (\"The Next Move\") was released which included the hit Pragmata (\"Things\") and went platinum, becoming the best selling record of the year. In February 1988 she released her ninth album Tora (\"Now\") and in December the album Empnefsi! (\"Inspiration!\") which went gold. In 1988, she made her debut as a radio producer on ANT1 Radio. Her radio program was titled after one of her songs Ta Koritsia Einai Atakta (\"Girls Are Naughty\") and was aired every weekend. In the same year, she participated with the song Klaio (\"I'm Crying\") at the Greek National Final for Eurovision Song Contest, finishing third. In 1989, she released the highly successful studio album Fotia (Fire), being one of the first albums to feature western sounds. The lead single Pseftika (\"Fake\") became a big hit and the album reached platinum status, selling 180.000 copies and becoming the second best selling record of 1990. She performed at \"Diogenis Palace\" in that same year, Athens's biggest nightclub/music hall at the time. CANNOTANSWER [QUESTION]: Did she have a third album in 1986? <|endoftext|>\n",
      "<|startoftext|>\n",
      "[CONTEXT]: In\n",
      "====================\n",
      "<|startoftext|>\n",
      "[CONTEXT]: In May 1983, she married Nikos Karvelas, a composer, with whom she collaborated in 1975 and in November she gave birth to her daughter Sofia. After their marriage, she started a close collaboration with Karvelas. Since 1975, all her releases have become gold or platinum and have included songs by Karvelas. In 1986, she participated at the Cypriot National Final for Eurovision Song Contest with the song Thelo Na Gino Star (\"I Want To Be A Star\"), taking second place. This song is still unreleased up to date. In 1984, Vissi left her record company EMI Greece and signed with CBS Records Greece, which later became Sony Music Greece, a collaboration that lasted until 2013. In March 1984, she released Na 'Hes Kardia (\"If You Had a Heart\"). The album was certified gold. The following year her seventh album Kati Simveni (\"Something Is Happening\") was released which included one of her most famous songs, titled \"Dodeka\" [\"Twelve (O'Clock)\"] and reached gold status selling 80.000 units. In 1986 I Epomeni Kinisi (\"The Next Move\") was released. The album included the hit Pragmata (\"Things\") and went platinum, becoming the best selling record of the year. In February 1988 she released her ninth album Tora (\"Now\") and in December the album Empnefsi! (\"Inspiration!\") which went gold. In 1988, she made her debut as a radio producer on ANT1 Radio. Her radio program was titled after one of her songs Ta Koritsia Einai Atakta (\"Girls Are Naughty\") and was aired every weekend. In the same year, she participated with the song Klaio (\"I'm Crying\") at the Greek National Final for Eurovision Song Contest, finishing third. In 1989, she released the highly successful studio album Fotia (Fire), being one of the first albums to feature western sounds. The lead single Pseftika (\"Fake\") became a big hit and the album reached platinum status, selling 180.000 copies and becoming the second best selling record of 1990. She performed at \"Diogenis Palace\" in that same year, Athens's biggest nightclub/music hall at the time. CANNOTANSWER  [QUESTION]: Was she a member of the other girl groups or was she solo? <|endoftext|>\n",
      "<|startoftext|>\n",
      "[CONTEXT]: In May 1983, she married Nikos Karvelas, a composer, with whom she collaborated in 1975 and in November she gave birth to her daughter Sofia. After their marriage, she started a close collaboration with Karvelas. Since 1975, all her releases have become gold or platinum and have included songs by Karvelas. In 1986, she participated at the Cypriot National Final for Eurovision Song Contest with the song Thelo Na Gino Star (\"I Want To Be A Star\"), taking second place. This song is still unreleased up to date. In 1984, Vissi left her record company EMI Greece and signed with CBS Records, which later became Sony Music Greece, a collaboration that lasted until 2013. In March 1984, she released Na 'Hes Kardia (\"If You Had a Heart\"). The album was certified gold. The following year her seventh album Kati Simveni (\"The Next Move\") was released which included one of her most famous songs, titled \"Dodeka\" [\"Twelve (O'Clock)\"] and reached gold status selling 80.000 units. In 1986 I Epomeni Kinisi (\"The Next Move\") was released which included the hit Pragmata (\"Things\") and went platinum, becoming the best selling record of the year. In February 1988 she released her ninth album Tora (\"Now\") and in December the album Empnefsi! (\"Inspiration!\") which went gold. In 1988, she made her debut as a radio producer on ANT1 Radio. Her radio program was titled after one of her songs Ta Koritsia Einai Atakta (\"Girls Are Naughty\") and was aired every weekend. In the same year, she participated with the song Klaio (\"I'm Crying\") at the Greek National Final for Eurovision Song Contest, finishing third. In 1989, she released the highly successful studio album Fotia (Fire), being one of the first albums to feature western sounds. The lead single Pseftika (\"Fake\") became a big hit and the album reached platinum status, selling 180.000 copies and becoming the second best selling record of 1990. She performed at \"Diogenis Palace\" in that same year, Athens's biggest nightclub/music hall at the time. CANNOTANSWER [QUESTION]: Did she win any awards? <|endoftext|>\n",
      "<|startoftext|>\n",
      "[CON\n",
      "====================\n",
      "<|startoftext|>\n",
      "[CONTEXT]: In May 1983, she married Nikos Karvelas, a composer, with whom she collaborated in 1975 and in November she gave birth to her daughter Sofia. After their marriage, she started a close collaboration with Karvelas. Since 1975, all her releases have become gold or platinum and have included songs by Karvelas. In 1986, she participated at the Cypriot National Final for Eurovision Song Contest with the song Thelo Na Gino Star (\"I Want To Be A Star\"), taking second place. This song is still unreleased up to date. In 1984, Vissi left her record company EMI Greece and signed with CBS Records Greece, which later became Sony Music Greece, a collaboration that lasted until 2013. In March 1984, she released Na 'Hes Kardia (\"If You Had a Heart\"). The album was certified gold. The following year her seventh album Kati Simveni (\"Something Is Happening\") was released which included one of her most famous songs, titled \"Dodeka\" [\"Twelve (O'Clock)\"] and reached gold status selling 80.000 units. In 1986 I Epomeni Kinisi (\"The Next Move\") was released. The album included the hit Pragmata (\"Things\") and went platinum, becoming the best selling record of the year. In February 1988 she released her ninth album Tora (\"Now\") and in December the album Empnefsi! (\"Inspiration!\") which went gold. In 1988, she made her debut as a radio producer on ANT1 Radio. Her radio program was titled after one of her songs Ta Koritsia Einai Atakta (\"Girls Are Naughty\") and was aired every weekend. In the same year, she participated with the song Klaio (\"I'm Crying\") at the Greek National Final for Eurovision Song Contest, finishing third. In 1989, she released the highly successful studio album Fotia (Fire), being one of the first albums to feature western sounds. The lead single Pseftika (\"Fake\") became a big hit and the album reached platinum status, selling 180.000 copies and becoming the second best selling record of 1990. She performed at \"Diogenis Palace\" in that same year, Athens's biggest nightclub/music hall at the time. CANNOTANSWER  [QUESTION]: how did she do on the charts? <|endoftext|>\n",
      "<|startoftext|>\n",
      "[CONTEXT]: In May 1983, she married Nikos Karvelas, a composer, with whom she collaborated in 1975 and in November she gave birth to her daughter Sofia. After their marriage, she started a close collaboration with Karvelas. Since 1975, all her releases have become gold or platinum and have included songs by Karvelas. In 1986, she participated at the Cypriot National Final for Eurovision Song Contest with the song Thelo Na Gino Star (\"I Want To Be A Star\"), taking second place. This song is still unreleased up to date. In 1984, Vissi left her record company EMI Greece and signed with CBS Records, which later became Sony Music Greece, a collaboration that lasted until 2013. In March 1984, she released Na 'Hes Kardia (\"If You Had a Heart\"). The album was certified gold. The following year her seventh album Kati Simveni (\"The Next Move\") was released which included one of her most famous songs, titled \"Dodeka\" [\"Twelve (O'Clock)\"] and reached gold status selling 80.000 units. In 1986 I Epomeni Kinisi (\"The Next Move\") was released which included the hit Pragmata (\"Things\") and went platinum, becoming the best selling record of the year. In February 1988 she released her ninth album Tora (\"Now\") and in December the album Empnefsi! (\"Inspiration!\") which went gold. In 1988, she made her debut as a radio producer on ANT1 Radio. Her radio program was titled after one of her songs Ta Koritsia Einai Atakta (\"Girls Are Naughty\") and was aired every weekend. In the same year, she participated with the song Klaio (\"I'm Crying\") at the Greek National Final for Eurovision Song Contest, finishing third. In 1989, she released the highly successful studio album Fotia (Fire), being one of the first albums to feature western sounds. The lead single Pseftika (\"Fake\") became a big hit and the album reached platinum status, selling 180.000 copies and becoming the second best selling record of 1990. She performed at \"Diogenis Palace\" in that same year, Athens's biggest nightclub/music hall at the time. CANNOTANSWER [QUESTION]: Did she have any hits? <|endoftext|>\n",
      "<|startoftext|>\n",
      "[CONTEXT]: In May 1983,\n",
      "====================\n",
      "<|startoftext|>\n",
      "[CONTEXT]: In May 1983, she married Nikos Karvelas, a composer, with whom she collaborated in 1975 and in November she gave birth to her daughter Sofia. After their marriage, she started a close collaboration with Karvelas. Since 1975, all her releases have become gold or platinum and have included songs by Karvelas. In 1986, she participated at the Cypriot National Final for Eurovision Song Contest with the song Thelo Na Gino Star (\"I Want To Be A Star\"), taking second place. This song is still unreleased up to date. In 1984, Vissi left her record company EMI Greece and signed with CBS Records Greece, which later became Sony Music Greece, a collaboration that lasted until 2013. In March 1984, she released Na 'Hes Kardia (\"If You Had a Heart\"). The album was certified gold. The following year her seventh album Kati Simveni (\"Something Is Happening\") was released which included one of her most famous songs, titled \"Dodeka\" [\"Twelve (O'Clock)\"] and reached gold status selling 80.000 units. In 1986 I Epomeni Kinisi (\"The Next Move\") was released. The album included the hit Pragmata (\"Things\") and went platinum, becoming the best selling record of the year. In February 1988 she released her ninth album Tora (\"Now\") and in December the album Empnefsi! (\"Inspiration!\") which went gold. In 1988, she made her debut as a radio producer on ANT1 Radio. Her radio program was titled after one of her songs Ta Koritsia Einai Atakta (\"Girls Are Naughty\") and was aired every weekend. In the same year, she participated with the song Klaio (\"I'm Crying\") at the Greek National Final for Eurovision Song Contest, finishing third. In 1989, she released the highly successful studio album Fotia (Fire), being one of the first albums to feature western sounds. The lead single Pseftika (\"Fake\") became a big hit and the album reached platinum status, selling 180.000 copies and becoming the second best selling record of 1990. She performed at \"Diogenis Palace\" in that same year, Athens's biggest nightclub/music hall at the time. CANNOTANSWER  [QUESTION]: What was the first single released from her debut album? <|endoftext|>\n",
      "<|startoftext|>\n",
      "[CONTEXT]: In May 1983, she married Nikos Karvelas, a composer, with whom she collaborated in 1975 and in November she gave birth to her daughter Sofia. After their marriage, she started a close collaboration with Karvelas. Since 1975, all her releases have become gold or platinum and have included songs by Karvelas. In 1986, she participated at the Cypriot National Final for Eurovision Song Contest with the song Thelo Na Gino Star (\"I Want To Be A Star\"), taking second place. This song is still unreleased up to date. In 1984, Vissi left her record company EMI Greece and signed with CBS Records Greece, which later became Sony Music Greece, a collaboration that lasted until 2013. In March 1984, she released Na 'Hes Kardia (\"If You Had a Heart\"). The album was certified gold. The following year her seventh album Kati Simveni (\"The Next Move\") was released which included one of her most famous songs, titled \"Dodeka\" [\"Twelve (O'Clock)\"] and reached gold status selling 80.000 units. In 1986 I Epomeni Kinisi (\"The Next Move\") was released which included the hit Pragmata (\"Things\") and went platinum, becoming the best selling record of the year. In February 1988 she released her ninth album Tora (\"Now\") and in December the album Empnefsi! (\"Inspiration!\") which went gold. In 1988, she made her debut as a radio producer on ANT1 Radio. Her radio program was titled after one of her songs Ta Koritsia Einai Atakta (\"Girls Are Naughty\") and was aired every weekend. In the same year, she participated with the song Klaio (\"I'm Crying\") at the Greek National Final for Eurovision Song Contest, finishing third. In 1989, she released the highly successful studio album Fotia (Fire), being one of the first albums to feature western sounds. The lead single Pseftika (\"Fake\") became a big hit and the album reached platinum status, selling 180.000 copies and becoming the second best selling record of 1990. She performed at \"Diogenis Palace\" in that same year, Athens's biggest nightclub/music hall at the time. CANNOTANSWER [QUESTION]: Was her debut album released with the help of her husband? <|endoftext|>\n",
      "<|startoftext|\n",
      "====================\n"
     ]
    }
   ],
   "source": [
    "gpt2.generate(qsess,\n",
    "              temperature=0.7,\n",
    "              prefix=pre,\n",
    "              nsamples=10,\n",
    "              batch_size=10,\n",
    "              run_name=\"QGen_QUAC_test\",\n",
    "              )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hypertuning GPT-2 QuAC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# future development - understand benefit in running time, considering the loss function result and/or learning rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"124M\"\n",
    "if not os.path.isdir(os.path.join(\"models\", model_name)):\n",
    "    print(f\"Downloading {model_name} model...\")\n",
    "    gpt2.download_gpt2(model_name=model_name)   # model is saved into current directory under /models/<size>/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import ParameterGrid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default batch size is 8 therefore search around it \n",
    "# default lr is 1e-5 therefore search around that \n",
    "\n",
    "param_grid = {\"batch_size\": [6, 8],\n",
    "              \"learning_rate\": [1e-4, 1e-5, 1e-6],\n",
    "              \"optimizer\": ['adam'],\n",
    "              \"steps\": [2000,4000]\n",
    "             }\n",
    "\n",
    "grid = ParameterGrid(param_grid)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading checkpoint models/124M/model.ckpt\n",
      "INFO:tensorflow:Restoring parameters from models/124M/model.ckpt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [03:40<00:00, 220.48s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset has 46016379 tokens\n",
      "Training...\n",
      "[10 | 14.31] loss=1.96 avg=1.96\n",
      "[20 | 21.78] loss=2.32 avg=2.14\n",
      "[30 | 29.25] loss=2.03 avg=2.10\n",
      "[40 | 36.70] loss=1.77 avg=2.02\n",
      "[50 | 44.16] loss=1.98 avg=2.01\n",
      "[60 | 51.61] loss=1.75 avg=1.96\n",
      "[70 | 59.08] loss=1.78 avg=1.94\n",
      "[80 | 66.53] loss=1.85 avg=1.93\n",
      "[90 | 73.99] loss=1.98 avg=1.93\n",
      "[100 | 81.45] loss=2.60 avg=2.00\n",
      "[110 | 88.91] loss=1.63 avg=1.97\n",
      "[120 | 96.39] loss=1.88 avg=1.96\n",
      "[130 | 103.87] loss=2.22 avg=1.98\n",
      "[140 | 111.35] loss=1.41 avg=1.94\n",
      "[150 | 118.83] loss=2.03 avg=1.94\n",
      "[160 | 126.32] loss=2.18 avg=1.96\n",
      "[170 | 133.80] loss=1.65 avg=1.94\n",
      "[180 | 141.29] loss=1.47 avg=1.91\n",
      "[190 | 148.79] loss=1.80 avg=1.90\n",
      "[200 | 156.28] loss=1.87 avg=1.90\n",
      "[210 | 163.77] loss=2.08 avg=1.91\n",
      "[220 | 171.29] loss=1.95 avg=1.91\n",
      "[230 | 178.78] loss=1.92 avg=1.91\n",
      "[240 | 186.28] loss=2.34 avg=1.93\n",
      "[250 | 193.79] loss=1.91 avg=1.93\n",
      "[260 | 201.32] loss=1.79 avg=1.93\n",
      "[270 | 208.83] loss=2.06 avg=1.93\n",
      "[280 | 216.33] loss=1.71 avg=1.92\n",
      "[290 | 223.82] loss=2.03 avg=1.93\n",
      "[300 | 231.32] loss=2.13 avg=1.93\n",
      "[310 | 238.84] loss=1.91 avg=1.93\n",
      "[320 | 246.35] loss=1.83 avg=1.93\n",
      "[330 | 253.86] loss=1.95 avg=1.93\n",
      "[340 | 261.38] loss=1.74 avg=1.92\n",
      "[350 | 268.88] loss=1.74 avg=1.92\n",
      "[360 | 276.37] loss=2.05 avg=1.92\n",
      "[370 | 283.87] loss=2.31 avg=1.94\n",
      "[380 | 291.39] loss=1.87 avg=1.93\n",
      "[390 | 298.91] loss=1.53 avg=1.92\n",
      "[400 | 306.43] loss=2.35 avg=1.93\n",
      "[410 | 313.95] loss=2.03 avg=1.94\n",
      "[420 | 321.49] loss=1.95 avg=1.94\n",
      "[430 | 329.01] loss=1.84 avg=1.93\n",
      "[440 | 336.53] loss=1.71 avg=1.93\n",
      "[450 | 344.05] loss=1.92 avg=1.93\n",
      "[460 | 351.56] loss=1.79 avg=1.92\n",
      "[470 | 359.08] loss=2.12 avg=1.93\n",
      "[480 | 366.62] loss=2.05 avg=1.93\n",
      "[490 | 374.14] loss=1.91 avg=1.93\n",
      "[500 | 381.66] loss=1.28 avg=1.92\n",
      "Saving checkpoint/quac_ht_7/model-500\n",
      "[510 | 391.29] loss=1.72 avg=1.91\n",
      "[520 | 398.81] loss=2.03 avg=1.91\n",
      "[530 | 406.33] loss=1.97 avg=1.91\n",
      "[540 | 413.89] loss=2.01 avg=1.92\n",
      "[550 | 421.40] loss=1.51 avg=1.91\n",
      "[560 | 428.92] loss=1.63 avg=1.90\n",
      "[570 | 436.45] loss=1.74 avg=1.90\n",
      "[580 | 443.98] loss=1.88 avg=1.90\n",
      "[590 | 451.50] loss=2.23 avg=1.90\n",
      "[600 | 459.02] loss=1.70 avg=1.90\n",
      "[610 | 466.56] loss=1.73 avg=1.90\n",
      "[620 | 474.07] loss=1.86 avg=1.90\n",
      "[630 | 481.59] loss=2.09 avg=1.90\n",
      "[640 | 489.11] loss=1.94 avg=1.90\n",
      "[650 | 496.64] loss=1.58 avg=1.89\n",
      "[660 | 504.16] loss=2.42 avg=1.90\n",
      "[670 | 511.68] loss=2.13 avg=1.91\n",
      "[680 | 519.20] loss=2.26 avg=1.92\n",
      "[690 | 526.71] loss=1.90 avg=1.92\n",
      "[700 | 534.24] loss=1.56 avg=1.91\n",
      "[710 | 541.77] loss=1.87 avg=1.91\n",
      "[720 | 549.28] loss=1.72 avg=1.90\n",
      "[730 | 556.81] loss=1.70 avg=1.90\n",
      "[740 | 564.33] loss=1.36 avg=1.89\n",
      "[750 | 571.85] loss=2.33 avg=1.90\n",
      "[760 | 579.37] loss=2.04 avg=1.90\n",
      "[770 | 586.89] loss=1.32 avg=1.89\n",
      "[780 | 594.42] loss=2.04 avg=1.89\n",
      "[790 | 601.94] loss=2.17 avg=1.90\n",
      "[800 | 609.46] loss=2.15 avg=1.90\n",
      "[810 | 616.98] loss=1.82 avg=1.90\n",
      "[820 | 624.50] loss=1.60 avg=1.90\n",
      "[830 | 632.02] loss=1.66 avg=1.89\n",
      "[840 | 639.53] loss=1.65 avg=1.89\n",
      "[850 | 647.04] loss=1.67 avg=1.88\n",
      "[860 | 654.56] loss=2.00 avg=1.89\n",
      "[870 | 662.07] loss=2.08 avg=1.89\n",
      "[880 | 669.60] loss=2.34 avg=1.90\n",
      "[890 | 677.13] loss=1.67 avg=1.89\n",
      "[900 | 684.65] loss=1.66 avg=1.89\n",
      "[910 | 692.17] loss=1.81 avg=1.89\n",
      "[920 | 699.69] loss=1.56 avg=1.88\n",
      "[930 | 707.20] loss=2.08 avg=1.89\n",
      "[940 | 714.73] loss=1.74 avg=1.88\n",
      "[950 | 722.26] loss=1.95 avg=1.88\n",
      "[960 | 729.78] loss=1.99 avg=1.89\n",
      "[970 | 737.32] loss=1.62 avg=1.88\n",
      "[980 | 744.84] loss=1.64 avg=1.88\n",
      "[990 | 752.35] loss=1.77 avg=1.88\n",
      "[1000 | 759.85] loss=2.02 avg=1.88\n",
      "Saving checkpoint/quac_ht_7/model-1000\n",
      "[1010 | 768.94] loss=1.95 avg=1.88\n",
      "[1020 | 776.51] loss=2.42 avg=1.89\n",
      "[1030 | 784.03] loss=1.90 avg=1.89\n",
      "[1040 | 791.54] loss=1.21 avg=1.88\n",
      "[1050 | 799.06] loss=2.06 avg=1.88\n",
      "[1060 | 806.57] loss=1.78 avg=1.88\n",
      "[1070 | 814.08] loss=1.33 avg=1.87\n",
      "[1080 | 821.60] loss=1.51 avg=1.87\n",
      "[1090 | 829.12] loss=1.89 avg=1.87\n",
      "[1100 | 836.64] loss=1.66 avg=1.86\n",
      "[1110 | 844.17] loss=1.48 avg=1.86\n",
      "[1120 | 851.69] loss=1.91 avg=1.86\n",
      "[1130 | 859.20] loss=1.82 avg=1.86\n",
      "[1140 | 866.70] loss=1.57 avg=1.85\n",
      "[1150 | 874.21] loss=1.68 avg=1.85\n",
      "[1160 | 881.73] loss=1.93 avg=1.85\n",
      "[1170 | 889.25] loss=2.02 avg=1.85\n",
      "[1180 | 896.76] loss=2.17 avg=1.86\n",
      "[1190 | 904.27] loss=1.71 avg=1.86\n",
      "[1200 | 911.80] loss=2.48 avg=1.87\n",
      "[1210 | 919.30] loss=1.89 avg=1.87\n",
      "[1220 | 926.80] loss=1.56 avg=1.86\n",
      "[1230 | 934.32] loss=1.86 avg=1.86\n",
      "[1240 | 941.84] loss=1.64 avg=1.86\n",
      "[1250 | 949.35] loss=1.74 avg=1.86\n",
      "[1260 | 956.92] loss=1.94 avg=1.86\n",
      "[1270 | 964.44] loss=1.59 avg=1.85\n",
      "[1280 | 971.96] loss=1.76 avg=1.85\n",
      "[1290 | 979.48] loss=1.90 avg=1.85\n",
      "[1300 | 987.00] loss=1.99 avg=1.86\n",
      "[1310 | 994.54] loss=1.79 avg=1.85\n",
      "[1320 | 1002.06] loss=1.30 avg=1.85\n",
      "[1330 | 1009.58] loss=2.06 avg=1.85\n",
      "[1340 | 1017.11] loss=1.76 avg=1.85\n",
      "[1350 | 1024.63] loss=1.39 avg=1.84\n",
      "[1360 | 1032.15] loss=1.48 avg=1.84\n",
      "[1370 | 1039.69] loss=1.50 avg=1.83\n",
      "[1380 | 1047.21] loss=1.78 avg=1.83\n",
      "[1390 | 1054.72] loss=1.59 avg=1.83\n",
      "[1400 | 1062.24] loss=1.93 avg=1.83\n",
      "[1410 | 1069.76] loss=1.87 avg=1.83\n",
      "[1420 | 1077.29] loss=1.60 avg=1.83\n",
      "[1430 | 1084.81] loss=1.92 avg=1.83\n",
      "[1440 | 1092.33] loss=1.70 avg=1.83\n",
      "[1450 | 1099.87] loss=1.75 avg=1.83\n",
      "[1460 | 1107.38] loss=2.32 avg=1.83\n",
      "[1470 | 1114.89] loss=2.42 avg=1.84\n",
      "[1480 | 1122.42] loss=1.74 avg=1.84\n",
      "[1490 | 1129.95] loss=2.05 avg=1.84\n",
      "[1500 | 1137.51] loss=1.52 avg=1.84\n",
      "Saving checkpoint/quac_ht_7/model-1500\n",
      "[1510 | 1146.51] loss=1.59 avg=1.83\n",
      "[1520 | 1154.01] loss=1.29 avg=1.83\n",
      "[1530 | 1161.53] loss=1.58 avg=1.82\n",
      "[1540 | 1169.05] loss=2.08 avg=1.83\n",
      "[1550 | 1176.57] loss=1.60 avg=1.82\n",
      "[1560 | 1184.07] loss=1.51 avg=1.82\n",
      "[1570 | 1191.57] loss=1.59 avg=1.82\n",
      "[1580 | 1199.07] loss=1.78 avg=1.82\n",
      "[1590 | 1206.59] loss=1.70 avg=1.82\n",
      "[1600 | 1214.11] loss=2.12 avg=1.82\n",
      "[1610 | 1221.62] loss=1.97 avg=1.82\n",
      "[1620 | 1229.13] loss=1.82 avg=1.82\n",
      "[1630 | 1236.66] loss=2.36 avg=1.83\n",
      "[1640 | 1244.17] loss=1.44 avg=1.82\n",
      "[1650 | 1251.69] loss=1.30 avg=1.82\n",
      "[1660 | 1259.22] loss=1.55 avg=1.81\n",
      "[1670 | 1266.74] loss=1.43 avg=1.81\n",
      "[1680 | 1274.26] loss=1.82 avg=1.81\n",
      "[1690 | 1281.79] loss=2.30 avg=1.82\n",
      "[1700 | 1289.33] loss=1.60 avg=1.81\n",
      "[1710 | 1296.85] loss=1.88 avg=1.81\n",
      "[1720 | 1304.37] loss=1.77 avg=1.81\n",
      "[1730 | 1311.90] loss=2.13 avg=1.82\n",
      "[1740 | 1319.44] loss=2.21 avg=1.82\n",
      "[1750 | 1326.96] loss=1.57 avg=1.82\n",
      "[1760 | 1334.48] loss=1.60 avg=1.82\n",
      "[1770 | 1342.00] loss=1.58 avg=1.81\n",
      "[1780 | 1349.52] loss=1.86 avg=1.81\n",
      "[1790 | 1357.05] loss=1.67 avg=1.81\n",
      "[1800 | 1364.58] loss=1.94 avg=1.81\n",
      "[1810 | 1372.11] loss=1.69 avg=1.81\n",
      "[1820 | 1379.64] loss=1.32 avg=1.81\n",
      "[1830 | 1387.16] loss=1.66 avg=1.80\n",
      "[1840 | 1394.68] loss=1.60 avg=1.80\n",
      "[1850 | 1402.20] loss=1.68 avg=1.80\n",
      "[1860 | 1409.72] loss=1.63 avg=1.80\n",
      "[1870 | 1417.23] loss=2.25 avg=1.80\n",
      "[1880 | 1424.75] loss=1.76 avg=1.80\n",
      "[1890 | 1432.28] loss=1.39 avg=1.80\n",
      "[1900 | 1439.80] loss=1.50 avg=1.79\n",
      "[1910 | 1447.32] loss=1.78 avg=1.79\n",
      "[1920 | 1454.85] loss=1.29 avg=1.79\n",
      "[1930 | 1462.38] loss=1.67 avg=1.79\n",
      "[1940 | 1469.92] loss=1.37 avg=1.78\n",
      "[1950 | 1477.45] loss=1.18 avg=1.78\n",
      "[1960 | 1484.97] loss=1.64 avg=1.77\n",
      "[1970 | 1492.49] loss=1.61 avg=1.77\n",
      "[1980 | 1500.05] loss=1.19 avg=1.77\n",
      "[1990 | 1507.56] loss=1.58 avg=1.76\n",
      "[2000 | 1515.06] loss=1.77 avg=1.76\n",
      "Saving checkpoint/quac_ht_7/model-2000\n",
      "======== SAMPLE 1 ========\n",
      " 16th ranked on ESPN Radio and 2nd on SportsCenter.  On November 8, 2008, the team was named in the ESPN 50-50 list.  On September 11, 2008, the Los Angeles Clippers announced that they would host a preseason scrimmage on February 7, 2009, at the Lakers' training camp facility. On August 26, 2009, the team announced that the first pre-season practice for its preseason roster would be held on September 13 at Brooklyn's Barclays Center. One scrimmage was scheduled for January 22, but was moved to February 25 after a number of meetings with the team. This scrimmage was hosted by The Star on October 28, 2009. The scrimmage opened with a scrimmage on March 28. On April 1 the Clippers announced their selection to play against the Chicago Bulls in the 2012 NBA All-Star Game. This was later confirmed a day later. On June 16, 2009, the team announced that coach Brown would be reassigned to free agency.  On July 6, 2009, the Los Angeles Clippers released guard J.T. Brown. \"I can't do it,\" said Clippers owner Don King. The final NBA trade was traded with Miami over the summer. Two days later, the Clippers released guard Paul D'Angelo. On August 13, 2009, the Los Angeles Clippers released center Donnie DiPietro. CANNOTANSWER [QUESTION]: was there anything else notable about the group? <|endoftext|>\n",
      "<|startoftext|>\n",
      "[CONTEXT]: The first season of the Clippers started off with a 4-12 record and a .938 winning percentage. They finished the season by averaging 101.8 points and shooting 38.7% from the field, 54.4% from 3-point range and 40% from the 3-point line. The team improved after the season by hitting an 11-6 stretch. In the first part of the improvement, the team shot 51.1% from the free-throw line and 53% from the free-throw line as a team, but they went to 51.3% from the line in the second half and 55.6% from the line in the third half. The Clippers were then defeated by the Milwaukee Bucks and New Jersey Nets in the Eastern Conference semifinals, where they lost in six games. They finished 9-3 when losing in the semifinals.  The team went undefeated in the East with an 8-2 record and a .938 winning percentage. They qualified for the playoffs with an 8-2 record and a .928 winning percentage. They were defeated by the Atlanta Hawks and Boston Celtics in the third round of the playoffs. The team finished the season with an 8-5 record and a .927 winning percentage. The Celtics went undefeated in the conference semifinals with an 8-7 record and a .928 winning percentage. They were defeated by Indiana Pacers and Dallas Mavericks in the NBA Finals.  The Clippers were in playoff contention each and home court in 2009 with a 16.3 points per game average, but lost in the fourth round of the playoffs to the defending champions the Oklahoma City Thunder and the Los Angeles Clippers. They were defeated in the post-season by the Atlanta Hawks and Boston Celtics in the 2009 postseason. The team finished the regular season with a 17-12 record. When the Clippers were ranked 11th in the league, head coach Doc Rivers called a press conference to announce he was leaving the team to focus on his personal studies and coaching. The team went undefeated in the conference with an 8-2 record. The Clippers' team also qualified for the post-season with a 4-0 record and a .931 winning percentage. The team won the conference tournament in the conference finals.  After the 2011-12 season, the new owners of the team announced that they would purchase the team from the Atlanta Hawks for $100 million. The team improved a lot with Rivers and his team. In the playoff series on the fourth quarter, the team was one of the contenders for the NBA championship. In the conference finals, the team advanced to the Eastern Conference semifinals with a 17-11 record.  The Los Angeles Clippers finished in fifth place, in 12th place, and 19th place overall. The team advanced to the second round of the playoffs with a 16-7 record and a .914 winning percentage. Their playoff record was 2-2; in the Semifinals, they were only 2-1 in the regular season.  The team finished with a record that stood at 34 wins. However, this was not a typical season for the team. In the regular season the Clippers went 8-3 and had a .904 winning percentage. The team went undefeated in the conference finals.  The team went undefeated in the post-season with a 9-4 record and a .944 winning percentage. They qualified for the playoffs with a 9-4 record and an 8-7 record and a .904 winning percentage. The team won the postseason with the 7th seed Memphis\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2010 | 1538.11] loss=1.60 avg=1.76\n",
      "[2020 | 1545.59] loss=1.76 avg=1.76\n",
      "[2030 | 1553.08] loss=1.41 avg=1.76\n",
      "[2040 | 1560.55] loss=1.84 avg=1.76\n",
      "[2050 | 1568.02] loss=1.72 avg=1.76\n",
      "[2060 | 1575.50] loss=1.67 avg=1.76\n",
      "[2070 | 1582.98] loss=1.78 avg=1.76\n",
      "[2080 | 1590.46] loss=1.42 avg=1.75\n",
      "[2090 | 1597.94] loss=1.44 avg=1.75\n",
      "[2100 | 1605.42] loss=1.56 avg=1.75\n",
      "[2110 | 1612.92] loss=1.33 avg=1.74\n",
      "[2120 | 1620.40] loss=1.22 avg=1.74\n",
      "[2130 | 1627.91] loss=1.65 avg=1.74\n",
      "[2140 | 1635.43] loss=1.77 avg=1.74\n",
      "[2150 | 1642.92] loss=1.66 avg=1.74\n",
      "[2160 | 1650.40] loss=1.86 avg=1.74\n",
      "[2170 | 1657.91] loss=1.25 avg=1.73\n",
      "[2180 | 1665.42] loss=1.40 avg=1.73\n",
      "[2190 | 1672.98] loss=1.61 avg=1.73\n",
      "[2200 | 1680.46] loss=1.32 avg=1.72\n",
      "[2210 | 1687.99] loss=1.65 avg=1.72\n",
      "[2220 | 1695.50] loss=1.69 avg=1.72\n",
      "[2230 | 1703.02] loss=2.21 avg=1.73\n",
      "[2240 | 1710.54] loss=1.69 avg=1.73\n",
      "[2250 | 1718.06] loss=1.34 avg=1.72\n",
      "[2260 | 1725.58] loss=1.68 avg=1.72\n",
      "[2270 | 1733.09] loss=1.46 avg=1.72\n",
      "[2280 | 1740.61] loss=1.48 avg=1.72\n",
      "[2290 | 1748.12] loss=1.36 avg=1.71\n",
      "[2300 | 1755.64] loss=1.59 avg=1.71\n",
      "[2310 | 1763.15] loss=1.25 avg=1.70\n",
      "[2320 | 1770.66] loss=1.70 avg=1.70\n",
      "[2330 | 1778.16] loss=1.33 avg=1.70\n",
      "[2340 | 1785.68] loss=1.82 avg=1.70\n",
      "[2350 | 1793.20] loss=1.65 avg=1.70\n",
      "[2360 | 1800.73] loss=1.90 avg=1.70\n",
      "[2370 | 1808.25] loss=1.41 avg=1.70\n",
      "[2380 | 1815.79] loss=1.10 avg=1.69\n",
      "[2390 | 1823.30] loss=1.19 avg=1.69\n",
      "[2400 | 1830.81] loss=1.68 avg=1.69\n",
      "[2410 | 1838.31] loss=1.57 avg=1.69\n",
      "[2420 | 1845.83] loss=1.60 avg=1.69\n",
      "[2430 | 1853.35] loss=2.12 avg=1.69\n",
      "[2440 | 1860.86] loss=1.86 avg=1.69\n",
      "[2450 | 1868.37] loss=1.54 avg=1.69\n",
      "[2460 | 1875.89] loss=1.39 avg=1.69\n",
      "[2470 | 1883.42] loss=1.25 avg=1.68\n",
      "[2480 | 1890.93] loss=1.64 avg=1.68\n",
      "[2490 | 1898.45] loss=1.94 avg=1.69\n",
      "[2500 | 1905.97] loss=2.00 avg=1.69\n",
      "Saving checkpoint/quac_ht_7/model-2500\n",
      "[2510 | 1914.99] loss=1.65 avg=1.69\n",
      "[2520 | 1922.52] loss=1.84 avg=1.69\n",
      "[2530 | 1930.03] loss=1.65 avg=1.69\n",
      "[2540 | 1937.54] loss=1.43 avg=1.69\n",
      "[2550 | 1945.06] loss=1.46 avg=1.68\n",
      "[2560 | 1952.57] loss=1.71 avg=1.68\n",
      "[2570 | 1960.11] loss=1.85 avg=1.69\n",
      "[2580 | 1967.62] loss=1.62 avg=1.69\n",
      "[2590 | 1975.14] loss=1.82 avg=1.69\n",
      "[2600 | 1982.67] loss=1.78 avg=1.69\n",
      "[2610 | 1990.19] loss=1.41 avg=1.68\n",
      "[2620 | 1997.72] loss=1.49 avg=1.68\n",
      "[2630 | 2005.22] loss=1.69 avg=1.68\n",
      "[2640 | 2012.73] loss=1.44 avg=1.68\n",
      "[2650 | 2020.23] loss=1.46 avg=1.68\n",
      "[2660 | 2027.76] loss=1.86 avg=1.68\n",
      "[2670 | 2035.28] loss=1.66 avg=1.68\n",
      "[2680 | 2042.78] loss=1.44 avg=1.68\n",
      "[2690 | 2050.28] loss=1.47 avg=1.67\n",
      "[2700 | 2057.78] loss=1.21 avg=1.67\n",
      "[2710 | 2065.28] loss=1.64 avg=1.67\n",
      "[2720 | 2072.78] loss=1.51 avg=1.67\n",
      "[2730 | 2080.29] loss=1.36 avg=1.66\n",
      "[2740 | 2087.79] loss=1.84 avg=1.67\n",
      "[2750 | 2095.29] loss=1.24 avg=1.66\n",
      "[2760 | 2102.80] loss=1.77 avg=1.66\n",
      "[2770 | 2110.32] loss=1.81 avg=1.66\n",
      "[2780 | 2117.84] loss=1.08 avg=1.66\n",
      "[2790 | 2125.37] loss=1.05 avg=1.65\n",
      "[2800 | 2132.89] loss=1.50 avg=1.65\n",
      "[2810 | 2140.41] loss=1.55 avg=1.65\n",
      "[2820 | 2147.94] loss=1.72 avg=1.65\n",
      "[2830 | 2155.47] loss=1.41 avg=1.65\n",
      "[2840 | 2163.00] loss=1.16 avg=1.64\n",
      "[2850 | 2170.52] loss=1.90 avg=1.65\n",
      "[2860 | 2178.04] loss=1.31 avg=1.64\n",
      "[2870 | 2185.54] loss=1.23 avg=1.64\n",
      "[2880 | 2193.05] loss=1.24 avg=1.63\n",
      "[2890 | 2200.56] loss=1.27 avg=1.63\n",
      "[2900 | 2208.05] loss=1.82 avg=1.63\n",
      "[2910 | 2215.58] loss=1.54 avg=1.63\n",
      "[2920 | 2223.09] loss=1.87 avg=1.63\n",
      "[2930 | 2230.60] loss=1.65 avg=1.63\n",
      "[2940 | 2238.11] loss=1.40 avg=1.63\n",
      "[2950 | 2245.62] loss=1.16 avg=1.63\n",
      "[2960 | 2253.13] loss=1.41 avg=1.62\n",
      "[2970 | 2260.63] loss=1.08 avg=1.62\n",
      "[2980 | 2268.14] loss=1.46 avg=1.62\n",
      "[2990 | 2275.65] loss=1.46 avg=1.61\n",
      "[3000 | 2283.14] loss=1.70 avg=1.62\n",
      "Saving checkpoint/quac_ht_7/model-3000\n",
      "[3010 | 2292.12] loss=1.75 avg=1.62\n",
      "[3020 | 2299.64] loss=1.97 avg=1.62\n",
      "[3030 | 2307.14] loss=1.16 avg=1.62\n",
      "[3680 | 2797.23] loss=1.28 avg=1.50\n"
     ]
    }
   ],
   "source": [
    "for idx, combination in enumerate(grid): \n",
    "    \n",
    "    if(idx>0):\n",
    "        while(os.path.isfile('results/Hypertuning/QUAC/done.txt') != True):\n",
    "            print(\"Done file does not exist\")\n",
    "            Event().wait(2.0) \n",
    "            \n",
    "    if(os.path.isfile('results/Hypertuning/QUAC/done.txt')):\n",
    "        os.remove('results/Hypertuning/QUAC/done.txt')\n",
    "    \n",
    "    tf.reset_default_graph()\n",
    "\n",
    "    sess = gpt2.start_tf_sess()\n",
    "    \n",
    "    run_iter_name=\"quac_ht_\"+str(idx)\n",
    "    \n",
    "    gpt2.finetune(sess,\n",
    "                  dataset= \"Data/quac_train.txt\", \n",
    "                  model_name='124M', \n",
    "                  restore_from='fresh', \n",
    "                  run_name= run_iter_name, \n",
    "                  print_every=10, \n",
    "                  sample_every=2000, \n",
    "                  save_every=500,\n",
    "                  #batch_size = combination['batch_size'],\n",
    "                  learning_rate= combination['learning_rate'],\n",
    "                  optimizer = combination['optimizer'],\n",
    "                  steps = combination['steps']\n",
    "             )\n",
    "    \n",
    "    \n",
    "    results_file = 'results/Hypertuning/QUAC/'+run_iter_name+'.txt'\n",
    "\n",
    "    #create results file and write out the hyperparams used in this iteration\n",
    "    with open(results_file, 'w') as f:\n",
    "        f.write(\"{}\\n\".format(combination))\n",
    "    f.close()\n",
    "    \n",
    "    with open('results/Hypertuning/QUAC/done.txt', 'w') as f:\n",
    "        f.write(\"Done\\n\".format(combination))\n",
    "    f.close()\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
