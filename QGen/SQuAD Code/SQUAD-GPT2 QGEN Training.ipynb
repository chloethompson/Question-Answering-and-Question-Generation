{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#download model \n",
    "#!python download.py 345M"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: dataclasses in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (0.8)\n",
      "Collecting datasets\n",
      "  Downloading datasets-1.4.1-py3-none-any.whl (186 kB)\n",
      "\u001b[K     |████████████████████████████████| 186 kB 15.4 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting multiprocess\n",
      "  Downloading multiprocess-0.70.11.1-py36-none-any.whl (101 kB)\n",
      "\u001b[K     |████████████████████████████████| 101 kB 9.9 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: dill in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from datasets) (0.3.3)\n",
      "Collecting tqdm<4.50.0,>=4.27\n",
      "  Downloading tqdm-4.49.0-py2.py3-none-any.whl (69 kB)\n",
      "\u001b[K     |████████████████████████████████| 69 kB 14.2 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: fsspec in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from datasets) (0.8.7)\n",
      "Requirement already satisfied: dataclasses in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from datasets) (0.8)\n",
      "Requirement already satisfied: requests>=2.19.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from datasets) (2.25.1)\n",
      "Requirement already satisfied: importlib-metadata in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from datasets) (3.7.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from datasets) (1.19.5)\n",
      "Collecting huggingface-hub==0.0.2\n",
      "  Downloading huggingface_hub-0.0.2-py3-none-any.whl (24 kB)\n",
      "Collecting xxhash\n",
      "  Downloading xxhash-2.0.0-cp36-cp36m-manylinux2010_x86_64.whl (242 kB)\n",
      "\u001b[K     |████████████████████████████████| 242 kB 67.4 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: pyarrow>=0.17.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from datasets) (3.0.0)\n",
      "Requirement already satisfied: pandas in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from datasets) (1.1.5)\n",
      "Requirement already satisfied: filelock in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from huggingface-hub==0.0.2->datasets) (3.0.12)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from requests>=2.19.0->datasets) (2.10)\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from requests>=2.19.0->datasets) (3.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from requests>=2.19.0->datasets) (2020.12.5)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from requests>=2.19.0->datasets) (1.26.3)\n",
      "Requirement already satisfied: typing-extensions>=3.6.4 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from importlib-metadata->datasets) (3.7.4.3)\n",
      "Requirement already satisfied: zipp>=0.5 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from importlib-metadata->datasets) (3.4.0)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from pandas->datasets) (2.8.1)\n",
      "Requirement already satisfied: pytz>=2017.2 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from pandas->datasets) (2021.1)\n",
      "Requirement already satisfied: six>=1.5 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from python-dateutil>=2.7.3->pandas->datasets) (1.15.0)\n",
      "Installing collected packages: tqdm, xxhash, multiprocess, huggingface-hub, datasets\n",
      "Successfully installed datasets-1.4.1 huggingface-hub-0.0.2 multiprocess-0.70.11.1 tqdm-4.49.0 xxhash-2.0.0\n",
      "Collecting tensorflow-gpu==1.15.2\n",
      "  Downloading tensorflow_gpu-1.15.2-cp36-cp36m-manylinux2010_x86_64.whl (411.0 MB)\n",
      "\u001b[K     |████████████████████████████████| 411.0 MB 23 kB/s s eta 0:00:01    |████                            | 50.9 MB 18.3 MB/s eta 0:00:20\n",
      "\u001b[?25hCollecting opt-einsum>=2.3.2\n",
      "  Downloading opt_einsum-3.3.0-py3-none-any.whl (65 kB)\n",
      "\u001b[K     |████████████████████████████████| 65 kB 881 kB/s  eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: numpy<2.0,>=1.16.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from tensorflow-gpu==1.15.2) (1.19.5)\n",
      "Requirement already satisfied: wrapt>=1.11.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from tensorflow-gpu==1.15.2) (1.12.1)\n",
      "Requirement already satisfied: six>=1.10.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from tensorflow-gpu==1.15.2) (1.15.0)\n",
      "Collecting tensorflow-estimator==1.15.1\n",
      "  Downloading tensorflow_estimator-1.15.1-py2.py3-none-any.whl (503 kB)\n",
      "\u001b[K     |████████████████████████████████| 503 kB 78.2 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting grpcio>=1.8.6\n",
      "  Downloading grpcio-1.36.1-cp36-cp36m-manylinux2014_x86_64.whl (4.1 MB)\n",
      "\u001b[K     |████████████████████████████████| 4.1 MB 27.6 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting termcolor>=1.1.0\n",
      "  Downloading termcolor-1.1.0.tar.gz (3.9 kB)\n",
      "Collecting tensorboard<1.16.0,>=1.15.0\n",
      "  Downloading tensorboard-1.15.0-py3-none-any.whl (3.8 MB)\n",
      "\u001b[K     |████████████████████████████████| 3.8 MB 24.1 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting keras-preprocessing>=1.0.5\n",
      "  Downloading Keras_Preprocessing-1.1.2-py2.py3-none-any.whl (42 kB)\n",
      "\u001b[K     |████████████████████████████████| 42 kB 229 kB/s  eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: wheel>=0.26 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from tensorflow-gpu==1.15.2) (0.36.2)\n",
      "Collecting absl-py>=0.7.0\n",
      "  Downloading absl_py-0.12.0-py3-none-any.whl (129 kB)\n",
      "\u001b[K     |████████████████████████████████| 129 kB 78.1 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: google-pasta>=0.1.6 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from tensorflow-gpu==1.15.2) (0.2.0)\n",
      "Collecting gast==0.2.2\n",
      "  Downloading gast-0.2.2.tar.gz (10 kB)\n",
      "Collecting keras-applications>=1.0.8\n",
      "  Downloading Keras_Applications-1.0.8-py3-none-any.whl (50 kB)\n",
      "\u001b[K     |████████████████████████████████| 50 kB 1.4 MB/s  eta 0:00:01\n",
      "\u001b[?25hCollecting astor>=0.6.0\n",
      "  Downloading astor-0.8.1-py2.py3-none-any.whl (27 kB)\n",
      "Requirement already satisfied: protobuf>=3.6.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from tensorflow-gpu==1.15.2) (3.15.2)\n",
      "Requirement already satisfied: h5py in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from keras-applications>=1.0.8->tensorflow-gpu==1.15.2) (3.1.0)\n",
      "Requirement already satisfied: setuptools>=41.0.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from tensorboard<1.16.0,>=1.15.0->tensorflow-gpu==1.15.2) (49.6.0.post20210108)\n",
      "Collecting markdown>=2.6.8\n",
      "  Downloading Markdown-3.3.4-py3-none-any.whl (97 kB)\n",
      "\u001b[K     |████████████████████████████████| 97 kB 9.9 MB/s  eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: werkzeug>=0.11.15 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from tensorboard<1.16.0,>=1.15.0->tensorflow-gpu==1.15.2) (1.0.1)\n",
      "Requirement already satisfied: importlib-metadata in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from markdown>=2.6.8->tensorboard<1.16.0,>=1.15.0->tensorflow-gpu==1.15.2) (3.7.0)\n",
      "Requirement already satisfied: cached-property in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from h5py->keras-applications>=1.0.8->tensorflow-gpu==1.15.2) (1.5.1)\n",
      "Requirement already satisfied: zipp>=0.5 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from importlib-metadata->markdown>=2.6.8->tensorboard<1.16.0,>=1.15.0->tensorflow-gpu==1.15.2) (3.4.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.4 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from importlib-metadata->markdown>=2.6.8->tensorboard<1.16.0,>=1.15.0->tensorflow-gpu==1.15.2) (3.7.4.3)\n",
      "Building wheels for collected packages: gast, termcolor\n",
      "  Building wheel for gast (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for gast: filename=gast-0.2.2-py3-none-any.whl size=7538 sha256=a2121544a3880b3270821197678709fb87d1437addcc0419202c5e51a381b114\n",
      "  Stored in directory: /home/ec2-user/.cache/pip/wheels/19/a7/b9/0740c7a3a7d1d348f04823339274b90de25fbcd217b2ee1fbe\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Building wheel for termcolor (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for termcolor: filename=termcolor-1.1.0-py3-none-any.whl size=4829 sha256=fd4fb2e1ad8ea81083e16eafd56f490b46196e3394cc9c29ea820291c671f4d3\n",
      "  Stored in directory: /home/ec2-user/.cache/pip/wheels/93/2a/eb/e58dbcbc963549ee4f065ff80a59f274cc7210b6eab962acdc\n",
      "Successfully built gast termcolor\n",
      "Installing collected packages: markdown, grpcio, absl-py, termcolor, tensorflow-estimator, tensorboard, opt-einsum, keras-preprocessing, keras-applications, gast, astor, tensorflow-gpu\n",
      "Successfully installed absl-py-0.12.0 astor-0.8.1 gast-0.2.2 grpcio-1.36.1 keras-applications-1.0.8 keras-preprocessing-1.1.2 markdown-3.3.4 opt-einsum-3.3.0 tensorboard-1.15.0 tensorflow-estimator-1.15.1 tensorflow-gpu-1.15.2 termcolor-1.1.0\n"
     ]
    }
   ],
   "source": [
    "!pip install dataclasses\n",
    "!pip install datasets \n",
    "!pip install tensorflow-gpu==1.15.2\n",
    "#!pip install transformers==3.5.0\n",
    "\n",
    "#!pip install git+https://github.com/huggingface/transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q gpt-2-simple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:\n",
      "The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
      "For more information, please see:\n",
      "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
      "  * https://github.com/tensorflow/addons\n",
      "  * https://github.com/tensorflow/io (for I/O related ops)\n",
      "If you depend on functionality not listed there, please file an issue.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import gpt_2_simple as gpt2\n",
    "import os\n",
    "from threading import Event"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'!python run_clm.py     --model_name_or_path models/345M/     --train_file data/squad_train.txt     --validation_file data/squad_test.txt     --do_train     --do_eval     --output_dir Results/test_squad/     --evaluate_during_training     --learning_rate 5e-5     --num_train_epochs=3.0'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"!python run_clm.py \\\n",
    "    --model_name_or_path models/345M/ \\\n",
    "    --train_file data/squad_train.txt \\\n",
    "    --validation_file data/squad_test.txt \\\n",
    "    --do_train \\\n",
    "    --do_eval \\\n",
    "    --output_dir Results/test_squad/ \\\n",
    "    --evaluate_during_training \\\n",
    "    --learning_rate 5e-5 \\\n",
    "    --num_train_epochs=3.0\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GPT2 Simple Version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"345M\"\n",
    "if not os.path.isdir(os.path.join(\"models\", model_name)):\n",
    "    print(f\"Downloading {model_name} model...\")\n",
    "    gpt2.download_gpt2(model_name=model_name)   # model is saved into current directory under /models/<size>/\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SQUAD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess = gpt2.start_tf_sess()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/gpt_2_simple/src/sample.py:17: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "WARNING:tensorflow:From /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/gpt_2_simple/src/memory_saving_gradients.py:62: get_backward_walk_ops (from tensorflow.contrib.graph_editor.select) is deprecated and will be removed after 2019-06-06.\n",
      "Instructions for updating:\n",
      "Please use tensorflow.python.ops.op_selector.get_backward_walk_ops.\n",
      "Loading checkpoint models/345M/model.ckpt\n",
      "INFO:tensorflow:Restoring parameters from models/345M/model.ckpt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [01:28<00:00, 88.84s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset has 16793911 tokens\n",
      "Training...\n",
      "[1 | 23.66] loss=1.27 avg=1.27\n",
      "[2 | 33.28] loss=1.08 avg=1.18\n",
      "[3 | 42.93] loss=1.48 avg=1.28\n",
      "[4 | 52.40] loss=1.25 avg=1.27\n",
      "[5 | 61.94] loss=1.09 avg=1.23\n",
      "[6 | 71.50] loss=1.25 avg=1.24\n",
      "[7 | 80.99] loss=1.20 avg=1.23\n",
      "[8 | 90.48] loss=1.48 avg=1.26\n",
      "[9 | 99.96] loss=0.82 avg=1.21\n",
      "[10 | 109.45] loss=0.79 avg=1.17\n",
      "[11 | 118.93] loss=0.94 avg=1.15\n",
      "[12 | 128.41] loss=0.82 avg=1.12\n",
      "[13 | 137.86] loss=1.50 avg=1.15\n",
      "[14 | 147.33] loss=0.95 avg=1.13\n",
      "[15 | 156.86] loss=1.62 avg=1.17\n",
      "[16 | 166.37] loss=1.06 avg=1.16\n",
      "[17 | 175.87] loss=0.96 avg=1.15\n",
      "[18 | 185.32] loss=1.29 avg=1.16\n",
      "[19 | 194.79] loss=1.13 avg=1.16\n",
      "[20 | 204.26] loss=0.84 avg=1.14\n",
      "[21 | 213.81] loss=1.50 avg=1.16\n",
      "[22 | 223.50] loss=1.82 avg=1.19\n",
      "[23 | 232.96] loss=1.31 avg=1.20\n",
      "[24 | 242.42] loss=0.93 avg=1.18\n",
      "[25 | 251.86] loss=1.12 avg=1.18\n",
      "[26 | 261.38] loss=1.04 avg=1.18\n",
      "[27 | 270.84] loss=0.88 avg=1.16\n",
      "[28 | 280.38] loss=1.10 avg=1.16\n",
      "[29 | 289.87] loss=0.89 avg=1.15\n",
      "[30 | 299.38] loss=0.99 avg=1.14\n",
      "[31 | 308.89] loss=1.00 avg=1.14\n",
      "[32 | 318.34] loss=1.16 avg=1.14\n",
      "[33 | 327.84] loss=0.87 avg=1.13\n",
      "[34 | 337.40] loss=0.97 avg=1.12\n",
      "[35 | 346.88] loss=1.10 avg=1.12\n",
      "[36 | 356.35] loss=0.58 avg=1.11\n",
      "[37 | 365.84] loss=1.24 avg=1.11\n",
      "[38 | 375.36] loss=1.19 avg=1.11\n",
      "[39 | 384.88] loss=1.29 avg=1.12\n",
      "[40 | 394.41] loss=1.19 avg=1.12\n",
      "[41 | 404.00] loss=0.72 avg=1.11\n",
      "[42 | 413.48] loss=1.18 avg=1.11\n",
      "[43 | 422.96] loss=1.31 avg=1.12\n",
      "[44 | 432.45] loss=1.33 avg=1.12\n",
      "[45 | 441.93] loss=1.15 avg=1.12\n",
      "[46 | 451.41] loss=0.79 avg=1.11\n",
      "[47 | 460.91] loss=1.02 avg=1.11\n",
      "[48 | 470.37] loss=1.73 avg=1.13\n",
      "[49 | 479.86] loss=1.69 avg=1.14\n",
      "[50 | 489.33] loss=1.46 avg=1.15\n",
      "[51 | 498.83] loss=1.32 avg=1.15\n",
      "[52 | 508.33] loss=1.17 avg=1.15\n",
      "[53 | 517.82] loss=1.03 avg=1.15\n",
      "[54 | 527.34] loss=1.12 avg=1.15\n",
      "[55 | 536.90] loss=0.85 avg=1.14\n",
      "[56 | 546.45] loss=1.19 avg=1.14\n",
      "[57 | 555.96] loss=1.83 avg=1.16\n",
      "[58 | 565.48] loss=0.88 avg=1.15\n",
      "[59 | 575.04] loss=1.32 avg=1.16\n",
      "[60 | 584.60] loss=1.35 avg=1.16\n",
      "[61 | 594.12] loss=1.24 avg=1.16\n",
      "[62 | 603.64] loss=1.07 avg=1.16\n",
      "[63 | 613.15] loss=1.38 avg=1.17\n",
      "[64 | 622.65] loss=1.15 avg=1.17\n",
      "[65 | 632.12] loss=0.66 avg=1.16\n",
      "[66 | 641.59] loss=0.91 avg=1.15\n",
      "[67 | 651.07] loss=0.86 avg=1.14\n",
      "[68 | 660.56] loss=1.04 avg=1.14\n",
      "[69 | 670.06] loss=0.72 avg=1.13\n",
      "[70 | 679.64] loss=1.22 avg=1.14\n",
      "[71 | 689.17] loss=1.17 avg=1.14\n",
      "[72 | 698.68] loss=2.05 avg=1.15\n",
      "[73 | 708.20] loss=0.88 avg=1.15\n",
      "[74 | 717.73] loss=1.27 avg=1.15\n",
      "[75 | 727.19] loss=1.32 avg=1.15\n",
      "[76 | 736.69] loss=1.35 avg=1.16\n",
      "[77 | 746.17] loss=0.88 avg=1.15\n",
      "[78 | 755.80] loss=0.97 avg=1.15\n",
      "[79 | 765.26] loss=0.87 avg=1.14\n",
      "[80 | 774.77] loss=1.55 avg=1.15\n",
      "[81 | 784.24] loss=1.87 avg=1.16\n",
      "[82 | 793.69] loss=1.23 avg=1.17\n",
      "[83 | 803.13] loss=0.89 avg=1.16\n",
      "[84 | 812.67] loss=0.87 avg=1.16\n",
      "[85 | 822.17] loss=0.97 avg=1.15\n",
      "[86 | 831.64] loss=1.01 avg=1.15\n",
      "[87 | 841.18] loss=1.05 avg=1.15\n",
      "[88 | 850.65] loss=0.79 avg=1.14\n",
      "[89 | 860.13] loss=0.82 avg=1.14\n",
      "[90 | 869.66] loss=1.69 avg=1.15\n",
      "[91 | 879.12] loss=0.95 avg=1.14\n",
      "[92 | 888.61] loss=1.02 avg=1.14\n",
      "[93 | 898.09] loss=0.91 avg=1.14\n",
      "[94 | 907.60] loss=1.02 avg=1.13\n",
      "[95 | 917.15] loss=1.02 avg=1.13\n",
      "[96 | 926.62] loss=1.00 avg=1.13\n",
      "[97 | 936.28] loss=1.16 avg=1.13\n",
      "[98 | 945.76] loss=1.14 avg=1.13\n",
      "[99 | 955.26] loss=0.80 avg=1.13\n",
      "[100 | 964.72] loss=1.40 avg=1.13\n",
      "Saving checkpoint/QGen_SQUAD_test/model-100\n",
      "[101 | 987.97] loss=1.65 avg=1.14\n",
      "[102 | 997.41] loss=1.20 avg=1.14\n",
      "[104 | 1016.34] loss=0.97 avg=1.13\n",
      "[105 | 1025.74] loss=0.79 avg=1.13\n",
      "[106 | 1035.18] loss=1.07 avg=1.12\n",
      "[107 | 1044.62] loss=1.21 avg=1.13\n",
      "[108 | 1054.06] loss=1.03 avg=1.12\n",
      "[109 | 1063.53] loss=0.88 avg=1.12\n",
      "[110 | 1073.00] loss=1.37 avg=1.12\n",
      "[364 | 3479.72] loss=0.46 avg=1.12\n",
      "[365 | 3489.15] loss=0.76 avg=1.12\n",
      "[366 | 3498.82] loss=1.10 avg=1.12\n",
      "[367 | 3508.22] loss=1.31 avg=1.12\n",
      "[675 | 6425.79] loss=1.04 avg=1.10\n",
      "[1440 | 13671.82] loss=0.95 avg=1.09\n",
      "[1441 | 13681.27] loss=0.87 avg=1.09\n",
      "[1442 | 13690.75] loss=1.08 avg=1.09\n",
      "[1443 | 13700.22] loss=0.84 avg=1.09\n",
      "[1444 | 13709.66] loss=0.75 avg=1.08\n",
      "[1445 | 13719.27] loss=1.17 avg=1.09\n",
      "[1446 | 13728.69] loss=0.88 avg=1.08\n",
      "[1447 | 13738.12] loss=1.02 avg=1.08\n",
      "[1448 | 13747.57] loss=0.99 avg=1.08\n",
      "[1449 | 13757.00] loss=1.35 avg=1.08\n",
      "[1450 | 13766.45] loss=1.08 avg=1.08\n",
      "[1451 | 13775.89] loss=1.13 avg=1.08\n",
      "[1452 | 13785.35] loss=1.08 avg=1.08\n",
      "[1453 | 13794.78] loss=1.52 avg=1.09\n",
      "[1454 | 13804.19] loss=1.03 avg=1.09\n",
      "[1455 | 13813.63] loss=2.08 avg=1.10\n",
      "[1456 | 13823.07] loss=1.37 avg=1.10\n",
      "[1457 | 13832.52] loss=0.91 avg=1.10\n",
      "[1458 | 13841.98] loss=1.17 avg=1.10\n",
      "[1459 | 13851.41] loss=0.93 avg=1.10\n",
      "[1460 | 13860.89] loss=1.09 avg=1.10\n",
      "[1461 | 13870.33] loss=0.64 avg=1.09\n",
      "[1462 | 13879.76] loss=1.62 avg=1.10\n",
      "[1463 | 13889.21] loss=1.41 avg=1.10\n",
      "[1464 | 13898.81] loss=1.29 avg=1.10\n",
      "[1465 | 13908.24] loss=1.05 avg=1.10\n",
      "[1466 | 13917.68] loss=0.71 avg=1.10\n",
      "[1467 | 13927.13] loss=1.12 avg=1.10\n",
      "[1468 | 13936.57] loss=1.08 avg=1.10\n",
      "[1469 | 13946.01] loss=1.37 avg=1.10\n",
      "[1470 | 13955.45] loss=0.79 avg=1.10\n",
      "[1471 | 13964.86] loss=1.27 avg=1.10\n",
      "[1472 | 13974.30] loss=1.25 avg=1.10\n",
      "[1473 | 13983.73] loss=1.50 avg=1.11\n",
      "[1474 | 13993.16] loss=0.95 avg=1.10\n",
      "[1475 | 14002.62] loss=0.99 avg=1.10\n",
      "[1476 | 14012.07] loss=1.02 avg=1.10\n",
      "[1477 | 14021.53] loss=0.99 avg=1.10\n",
      "[1478 | 14030.98] loss=0.72 avg=1.10\n",
      "[1479 | 14040.45] loss=0.65 avg=1.09\n",
      "[1480 | 14049.89] loss=1.02 avg=1.09\n",
      "[1481 | 14059.32] loss=1.02 avg=1.09\n",
      "[1482 | 14068.76] loss=1.44 avg=1.10\n",
      "[1483 | 14078.34] loss=0.96 avg=1.09\n",
      "[1484 | 14087.81] loss=1.18 avg=1.09\n",
      "[1485 | 14097.25] loss=0.57 avg=1.09\n",
      "[1486 | 14106.68] loss=0.77 avg=1.09\n",
      "[1487 | 14116.11] loss=1.18 avg=1.09\n",
      "[1488 | 14125.55] loss=1.51 avg=1.09\n",
      "[1489 | 14134.97] loss=1.14 avg=1.09\n",
      "[1490 | 14144.45] loss=0.98 avg=1.09\n",
      "[1491 | 14153.89] loss=0.96 avg=1.09\n",
      "[1492 | 14163.30] loss=1.39 avg=1.09\n",
      "[1493 | 14172.73] loss=1.50 avg=1.10\n",
      "[1494 | 14182.15] loss=0.98 avg=1.10\n",
      "[1495 | 14191.59] loss=0.73 avg=1.09\n",
      "[1496 | 14201.04] loss=1.18 avg=1.09\n",
      "[1497 | 14210.48] loss=0.79 avg=1.09\n",
      "[1498 | 14219.90] loss=1.00 avg=1.09\n",
      "[1499 | 14229.34] loss=1.28 avg=1.09\n",
      "[1500 | 14238.81] loss=1.27 avg=1.09\n",
      "Saving checkpoint/QGen_SQUAD_test/model-1500\n",
      "[1501 | 14251.77] loss=0.46 avg=1.09\n",
      "[1502 | 14261.35] loss=0.67 avg=1.08\n",
      "[1503 | 14270.80] loss=0.99 avg=1.08\n",
      "[1504 | 14280.34] loss=1.32 avg=1.08\n",
      "[1505 | 14289.80] loss=0.92 avg=1.08\n",
      "[1506 | 14299.22] loss=1.12 avg=1.08\n",
      "[1507 | 14308.63] loss=0.57 avg=1.08\n",
      "[1508 | 14318.08] loss=1.09 avg=1.08\n",
      "[1509 | 14327.50] loss=2.03 avg=1.09\n",
      "[1510 | 14336.89] loss=0.67 avg=1.08\n",
      "[1511 | 14346.29] loss=0.68 avg=1.08\n",
      "[1512 | 14355.78] loss=0.95 avg=1.08\n",
      "[1513 | 14365.18] loss=0.83 avg=1.07\n",
      "[1514 | 14374.58] loss=1.23 avg=1.08\n",
      "[1515 | 14383.96] loss=1.66 avg=1.08\n",
      "[1516 | 14393.37] loss=1.00 avg=1.08\n",
      "[1517 | 14402.80] loss=0.98 avg=1.08\n",
      "[1518 | 14412.21] loss=0.99 avg=1.08\n",
      "[1519 | 14421.66] loss=0.80 avg=1.08\n",
      "[1520 | 14431.07] loss=1.04 avg=1.08\n",
      "[1521 | 14440.69] loss=1.53 avg=1.08\n",
      "[1522 | 14450.09] loss=1.12 avg=1.08\n",
      "[1523 | 14459.49] loss=1.22 avg=1.08\n",
      "[1524 | 14468.89] loss=1.01 avg=1.08\n",
      "[1525 | 14478.30] loss=0.61 avg=1.08\n",
      "[1526 | 14487.73] loss=1.08 avg=1.08\n",
      "[1527 | 14497.14] loss=0.92 avg=1.08\n",
      "[1528 | 14506.53] loss=1.28 avg=1.08\n",
      "[1529 | 14515.95] loss=1.13 avg=1.08\n",
      "[1530 | 14525.39] loss=1.12 avg=1.08\n",
      "[1531 | 14534.79] loss=1.01 avg=1.08\n",
      "[1532 | 14544.20] loss=0.65 avg=1.07\n",
      "[1533 | 14553.60] loss=1.56 avg=1.08\n",
      "[1534 | 14563.00] loss=1.04 avg=1.08\n",
      "[1535 | 14572.44] loss=1.84 avg=1.09\n",
      "[1536 | 14581.85] loss=1.37 avg=1.09\n",
      "[1537 | 14591.25] loss=0.97 avg=1.09\n",
      "[1538 | 14600.64] loss=1.64 avg=1.09\n",
      "[1539 | 14610.04] loss=0.77 avg=1.09\n",
      "[1540 | 14619.61] loss=0.71 avg=1.09\n",
      "[1541 | 14629.05] loss=1.29 avg=1.09\n",
      "[1542 | 14638.47] loss=1.10 avg=1.09\n",
      "[1543 | 14647.86] loss=0.92 avg=1.09\n",
      "[1544 | 14657.26] loss=0.92 avg=1.08\n",
      "[1545 | 14666.66] loss=1.05 avg=1.08\n",
      "[1546 | 14676.06] loss=0.93 avg=1.08\n",
      "[1547 | 14685.48] loss=1.20 avg=1.08\n",
      "[1548 | 14694.89] loss=1.27 avg=1.09\n",
      "[1549 | 14704.30] loss=1.12 avg=1.09\n",
      "[1550 | 14713.69] loss=1.07 avg=1.09\n",
      "[1551 | 14723.15] loss=1.06 avg=1.09\n",
      "[1552 | 14732.55] loss=1.17 avg=1.09\n",
      "[1553 | 14741.96] loss=1.40 avg=1.09\n",
      "[1554 | 14751.36] loss=1.12 avg=1.09\n",
      "[1555 | 14760.77] loss=1.08 avg=1.09\n",
      "[1556 | 14770.17] loss=2.19 avg=1.10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1557 | 14779.58] loss=0.52 avg=1.10\n",
      "[1558 | 14789.00] loss=1.25 avg=1.10\n",
      "[1559 | 14798.57] loss=1.27 avg=1.10\n",
      "[1560 | 14807.99] loss=1.20 avg=1.10\n",
      "[1561 | 14817.39] loss=0.71 avg=1.10\n",
      "[1562 | 14826.85] loss=1.04 avg=1.10\n",
      "[1563 | 14836.26] loss=0.72 avg=1.09\n",
      "[1564 | 14845.69] loss=0.99 avg=1.09\n",
      "[1565 | 14855.10] loss=1.63 avg=1.10\n",
      "[1566 | 14864.50] loss=0.57 avg=1.09\n",
      "[1567 | 14873.91] loss=0.74 avg=1.09\n",
      "[1568 | 14883.29] loss=0.70 avg=1.08\n",
      "[1569 | 14892.69] loss=0.84 avg=1.08\n",
      "[1570 | 14902.11] loss=0.89 avg=1.08\n",
      "[1571 | 14911.51] loss=1.04 avg=1.08\n",
      "[1572 | 14920.92] loss=0.99 avg=1.08\n",
      "[1573 | 14930.32] loss=0.70 avg=1.07\n",
      "[1574 | 14939.84] loss=1.18 avg=1.07\n",
      "[1575 | 14949.25] loss=1.37 avg=1.08\n",
      "[1576 | 14958.66] loss=0.92 avg=1.08\n",
      "[1577 | 14968.08] loss=1.54 avg=1.08\n",
      "[1578 | 14977.69] loss=1.51 avg=1.09\n",
      "[1579 | 14987.09] loss=1.08 avg=1.09\n",
      "[1580 | 14996.52] loss=1.03 avg=1.08\n",
      "[1581 | 15005.95] loss=0.85 avg=1.08\n",
      "[1582 | 15015.35] loss=0.84 avg=1.08\n",
      "[1583 | 15024.77] loss=1.23 avg=1.08\n",
      "[1584 | 15034.23] loss=0.77 avg=1.08\n",
      "[1585 | 15043.64] loss=1.56 avg=1.08\n",
      "[1586 | 15053.05] loss=0.93 avg=1.08\n",
      "[1587 | 15062.47] loss=1.42 avg=1.08\n",
      "[1588 | 15071.86] loss=0.89 avg=1.08\n",
      "[1589 | 15081.27] loss=0.65 avg=1.08\n",
      "[1590 | 15090.69] loss=0.62 avg=1.07\n",
      "[1591 | 15100.09] loss=1.66 avg=1.08\n",
      "[1592 | 15109.49] loss=1.12 avg=1.08\n",
      "[1593 | 15118.92] loss=1.37 avg=1.08\n",
      "[1594 | 15128.32] loss=1.30 avg=1.09\n",
      "[1595 | 15137.72] loss=0.94 avg=1.08\n",
      "[1596 | 15147.14] loss=0.78 avg=1.08\n",
      "[1597 | 15156.67] loss=1.53 avg=1.09\n",
      "[1598 | 15166.06] loss=0.76 avg=1.08\n",
      "[1599 | 15175.47] loss=0.64 avg=1.08\n",
      "[1600 | 15184.93] loss=0.76 avg=1.07\n",
      "Saving checkpoint/QGen_SQUAD_test/model-1600\n",
      "[1601 | 15197.87] loss=1.32 avg=1.08\n",
      "[1602 | 15207.30] loss=2.05 avg=1.09\n",
      "[1603 | 15216.77] loss=2.27 avg=1.10\n",
      "[1604 | 15226.33] loss=1.16 avg=1.10\n",
      "[1605 | 15235.77] loss=1.14 avg=1.10\n",
      "[1606 | 15245.20] loss=1.14 avg=1.10\n",
      "[1607 | 15254.68] loss=0.75 avg=1.10\n",
      "[1608 | 15264.09] loss=0.68 avg=1.09\n",
      "[1609 | 15273.53] loss=1.08 avg=1.09\n",
      "[1610 | 15282.98] loss=1.13 avg=1.09\n",
      "[1611 | 15292.42] loss=0.98 avg=1.09\n",
      "[1612 | 15301.90] loss=0.92 avg=1.09\n",
      "[1613 | 15311.37] loss=1.03 avg=1.09\n",
      "[1614 | 15320.80] loss=1.01 avg=1.09\n",
      "[1615 | 15330.24] loss=1.22 avg=1.09\n",
      "[1616 | 15339.89] loss=1.70 avg=1.10\n",
      "[1617 | 15349.31] loss=1.44 avg=1.10\n",
      "[1618 | 15358.73] loss=0.96 avg=1.10\n",
      "[1619 | 15368.18] loss=1.81 avg=1.11\n",
      "[1620 | 15377.63] loss=1.05 avg=1.10\n",
      "[1621 | 15387.10] loss=1.33 avg=1.11\n",
      "[1622 | 15396.52] loss=0.72 avg=1.10\n",
      "[1623 | 15405.94] loss=1.31 avg=1.11\n",
      "[1624 | 15415.39] loss=1.15 avg=1.11\n",
      "[1625 | 15424.83] loss=0.65 avg=1.10\n",
      "[1626 | 15434.30] loss=0.82 avg=1.10\n",
      "[1627 | 15443.76] loss=1.80 avg=1.11\n",
      "[1628 | 15453.33] loss=1.13 avg=1.11\n",
      "[1629 | 15462.82] loss=0.85 avg=1.10\n",
      "[1630 | 15472.26] loss=1.01 avg=1.10\n",
      "[1631 | 15481.70] loss=1.13 avg=1.10\n",
      "[1632 | 15491.14] loss=0.82 avg=1.10\n",
      "[1633 | 15500.55] loss=1.22 avg=1.10\n",
      "[1634 | 15510.03] loss=1.06 avg=1.10\n",
      "[1635 | 15519.64] loss=0.75 avg=1.10\n",
      "[1636 | 15529.06] loss=1.04 avg=1.10\n",
      "[1637 | 15538.53] loss=0.87 avg=1.09\n",
      "[1638 | 15547.96] loss=0.65 avg=1.09\n",
      "[1639 | 15557.41] loss=1.03 avg=1.09\n",
      "[1640 | 15566.90] loss=0.78 avg=1.09\n",
      "[1641 | 15576.44] loss=1.34 avg=1.09\n",
      "[1642 | 15585.91] loss=1.32 avg=1.09\n",
      "[1643 | 15595.37] loss=0.96 avg=1.09\n",
      "[1644 | 15604.83] loss=0.54 avg=1.08\n",
      "[1645 | 15614.26] loss=1.21 avg=1.09\n",
      "[1646 | 15623.71] loss=1.06 avg=1.08\n",
      "[1647 | 15633.16] loss=1.21 avg=1.09\n",
      "[1648 | 15642.59] loss=1.02 avg=1.09\n",
      "[1649 | 15652.02] loss=0.96 avg=1.08\n",
      "[1650 | 15661.46] loss=1.08 avg=1.08\n",
      "[1651 | 15670.90] loss=1.18 avg=1.09\n",
      "[1652 | 15680.34] loss=0.92 avg=1.08\n",
      "[1653 | 15689.81] loss=0.82 avg=1.08\n",
      "[1654 | 15699.42] loss=0.71 avg=1.08\n",
      "[1655 | 15708.87] loss=1.08 avg=1.08\n",
      "[1656 | 15718.34] loss=1.11 avg=1.08\n",
      "[1657 | 15727.78] loss=1.55 avg=1.08\n",
      "[1658 | 15737.22] loss=1.01 avg=1.08\n",
      "[1659 | 15746.67] loss=1.10 avg=1.08\n",
      "[1660 | 15756.11] loss=1.04 avg=1.08\n",
      "[1661 | 15765.54] loss=0.62 avg=1.08\n",
      "[1662 | 15774.97] loss=0.38 avg=1.07\n",
      "[1663 | 15784.41] loss=1.25 avg=1.07\n",
      "[1664 | 15793.84] loss=1.16 avg=1.07\n",
      "[1665 | 15803.28] loss=0.75 avg=1.07\n",
      "[1666 | 15812.74] loss=1.56 avg=1.07\n",
      "[1667 | 15822.19] loss=0.66 avg=1.07\n",
      "[1668 | 15831.62] loss=1.16 avg=1.07\n",
      "[1669 | 15841.09] loss=0.84 avg=1.07\n",
      "[1670 | 15850.52] loss=1.09 avg=1.07\n",
      "[1671 | 15860.00] loss=0.95 avg=1.07\n",
      "[1672 | 15869.50] loss=1.23 avg=1.07\n",
      "[1673 | 15879.15] loss=1.28 avg=1.07\n",
      "[1674 | 15888.59] loss=1.19 avg=1.07\n",
      "[1826 | 17329.23] loss=0.85 avg=1.11\n",
      "[2576 | 24581.73] loss=1.14 avg=1.09\n",
      "[2888 | 27523.00] loss=1.27 avg=1.08\n",
      "[3103 | 29553.55] loss=1.34 avg=1.04\n",
      "[3104 | 29563.22] loss=0.75 avg=1.04\n",
      "[3105 | 29572.67] loss=0.73 avg=1.03\n",
      "[3106 | 29582.06] loss=1.07 avg=1.03\n",
      "[3107 | 29591.70] loss=1.47 avg=1.04\n",
      "[3108 | 29601.19] loss=1.21 avg=1.04\n",
      "[3109 | 29610.86] loss=0.59 avg=1.03\n",
      "[3110 | 29620.50] loss=0.89 avg=1.03\n",
      "[3111 | 29630.10] loss=1.90 avg=1.04\n",
      "[3112 | 29639.56] loss=0.84 avg=1.04\n",
      "[3113 | 29649.21] loss=1.05 avg=1.04\n",
      "[3114 | 29658.73] loss=0.81 avg=1.04\n",
      "[3115 | 29668.38] loss=1.49 avg=1.04\n",
      "[3116 | 29677.95] loss=1.63 avg=1.05\n",
      "[3117 | 29687.57] loss=0.96 avg=1.05\n",
      "[3118 | 29697.14] loss=0.78 avg=1.04\n",
      "[3119 | 29706.75] loss=0.97 avg=1.04\n",
      "[3120 | 29716.33] loss=0.82 avg=1.04\n",
      "[3121 | 29725.92] loss=0.96 avg=1.04\n",
      "[3122 | 29735.64] loss=1.14 avg=1.04\n",
      "[3123 | 29745.14] loss=0.99 avg=1.04\n",
      "[3124 | 29754.81] loss=0.89 avg=1.04\n",
      "[3125 | 29764.30] loss=0.82 avg=1.04\n",
      "[3126 | 29773.96] loss=0.94 avg=1.04\n",
      "[3127 | 29783.49] loss=1.10 avg=1.04\n",
      "[3128 | 29793.18] loss=0.36 avg=1.03\n",
      "[3129 | 29802.74] loss=1.14 avg=1.03\n",
      "[3130 | 29812.36] loss=0.77 avg=1.03\n",
      "[3131 | 29821.97] loss=1.30 avg=1.03\n",
      "[3132 | 29831.56] loss=0.88 avg=1.03\n",
      "[3133 | 29841.13] loss=1.47 avg=1.03\n",
      "[3134 | 29850.72] loss=1.25 avg=1.04\n",
      "[3135 | 29860.30] loss=0.82 avg=1.03\n",
      "[3136 | 29869.82] loss=0.85 avg=1.03\n",
      "[3137 | 29879.57] loss=0.91 avg=1.03\n",
      "[3138 | 29889.15] loss=1.45 avg=1.04\n",
      "[3139 | 29898.65] loss=1.51 avg=1.04\n",
      "[3140 | 29908.30] loss=1.21 avg=1.04\n",
      "[3141 | 29917.97] loss=0.75 avg=1.04\n",
      "[3142 | 29927.65] loss=0.86 avg=1.04\n",
      "[3143 | 29937.22] loss=1.04 avg=1.04\n",
      "[3144 | 29946.77] loss=1.15 avg=1.04\n",
      "[3145 | 29956.16] loss=0.92 avg=1.04\n",
      "[3146 | 29965.55] loss=1.22 avg=1.04\n",
      "[3147 | 29974.97] loss=0.81 avg=1.04\n",
      "[3148 | 29984.37] loss=1.19 avg=1.04\n",
      "[3149 | 29993.80] loss=0.65 avg=1.03\n",
      "[3150 | 30003.22] loss=0.61 avg=1.03\n",
      "[3151 | 30012.62] loss=1.02 avg=1.03\n",
      "[3152 | 30022.02] loss=0.55 avg=1.03\n",
      "[3153 | 30031.45] loss=0.58 avg=1.02\n",
      "[3154 | 30040.85] loss=0.86 avg=1.02\n",
      "[3155 | 30050.25] loss=0.90 avg=1.02\n",
      "[3156 | 30059.65] loss=1.14 avg=1.02\n",
      "[3157 | 30069.05] loss=0.65 avg=1.02\n",
      "[3158 | 30078.43] loss=1.00 avg=1.02\n",
      "[3159 | 30087.81] loss=1.18 avg=1.02\n",
      "[3160 | 30097.36] loss=0.58 avg=1.01\n",
      "[3161 | 30106.77] loss=0.60 avg=1.01\n",
      "[3162 | 30116.16] loss=0.87 avg=1.01\n",
      "[3163 | 30125.55] loss=1.06 avg=1.01\n",
      "[3164 | 30134.96] loss=2.02 avg=1.02\n",
      "[3165 | 30144.36] loss=0.56 avg=1.01\n",
      "[3166 | 30153.78] loss=1.64 avg=1.02\n",
      "[3167 | 30163.18] loss=1.16 avg=1.02\n",
      "[3168 | 30172.58] loss=0.59 avg=1.02\n",
      "[3169 | 30182.01] loss=1.28 avg=1.02\n",
      "[3170 | 30191.45] loss=0.86 avg=1.02\n",
      "[3171 | 30200.84] loss=1.01 avg=1.02\n",
      "[3172 | 30210.24] loss=1.45 avg=1.02\n",
      "[3173 | 30219.65] loss=0.98 avg=1.02\n",
      "[3174 | 30229.04] loss=1.25 avg=1.02\n",
      "[3175 | 30238.44] loss=0.76 avg=1.02\n",
      "[3176 | 30247.83] loss=0.85 avg=1.02\n",
      "[3177 | 30257.23] loss=0.76 avg=1.02\n",
      "[3178 | 30266.64] loss=0.68 avg=1.01\n",
      "[3179 | 30276.14] loss=0.92 avg=1.01\n",
      "[3180 | 30285.54] loss=1.04 avg=1.01\n",
      "[3181 | 30294.93] loss=0.97 avg=1.01\n",
      "[3182 | 30304.31] loss=0.82 avg=1.01\n",
      "[3183 | 30313.71] loss=1.06 avg=1.01\n",
      "[3184 | 30323.11] loss=1.75 avg=1.02\n",
      "[3185 | 30332.52] loss=1.11 avg=1.02\n",
      "[3186 | 30341.92] loss=0.73 avg=1.02\n",
      "[3187 | 30351.32] loss=1.46 avg=1.02\n",
      "[3188 | 30360.72] loss=1.04 avg=1.02\n",
      "[3189 | 30370.14] loss=1.41 avg=1.02\n",
      "[3190 | 30379.53] loss=0.98 avg=1.02\n",
      "[3191 | 30388.91] loss=0.96 avg=1.02\n",
      "[3192 | 30398.29] loss=1.28 avg=1.03\n",
      "[3193 | 30407.69] loss=0.87 avg=1.02\n",
      "[3194 | 30417.08] loss=0.98 avg=1.02\n",
      "[3195 | 30426.49] loss=0.83 avg=1.02\n",
      "[3196 | 30435.89] loss=1.60 avg=1.03\n",
      "[3197 | 30445.32] loss=1.13 avg=1.03\n",
      "[3198 | 30454.81] loss=1.41 avg=1.03\n",
      "[3199 | 30464.32] loss=1.16 avg=1.03\n",
      "[3200 | 30473.71] loss=0.96 avg=1.03\n",
      "Saving checkpoint/QGen_SQUAD_test/model-3200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3201 | 30486.62] loss=0.97 avg=1.03\n",
      "[3202 | 30496.00] loss=0.65 avg=1.03\n",
      "[3203 | 30505.40] loss=1.10 avg=1.03\n",
      "[3204 | 30514.91] loss=1.61 avg=1.04\n",
      "[3205 | 30524.33] loss=0.92 avg=1.03\n",
      "[3206 | 30533.71] loss=1.62 avg=1.04\n",
      "[3207 | 30543.09] loss=1.42 avg=1.04\n",
      "[3208 | 30552.49] loss=1.68 avg=1.05\n",
      "[3209 | 30561.88] loss=1.17 avg=1.05\n",
      "[3210 | 30571.26] loss=1.19 avg=1.05\n",
      "[3211 | 30580.64] loss=0.70 avg=1.05\n",
      "[3212 | 30590.03] loss=0.71 avg=1.05\n",
      "[3213 | 30599.40] loss=1.16 avg=1.05\n",
      "[3214 | 30608.79] loss=1.47 avg=1.05\n",
      "[3215 | 30618.18] loss=1.28 avg=1.05\n",
      "[3216 | 30627.57] loss=1.07 avg=1.05\n",
      "[3217 | 30637.08] loss=0.97 avg=1.05\n",
      "[3218 | 30646.47] loss=1.35 avg=1.06\n",
      "[3219 | 30655.86] loss=1.05 avg=1.06\n",
      "[3220 | 30665.23] loss=0.99 avg=1.06\n",
      "[3221 | 30674.60] loss=0.83 avg=1.05\n",
      "[3222 | 30683.99] loss=0.60 avg=1.05\n",
      "[3223 | 30693.39] loss=0.99 avg=1.05\n",
      "[3224 | 30702.75] loss=1.48 avg=1.05\n",
      "[3225 | 30712.13] loss=1.73 avg=1.06\n",
      "[3226 | 30721.51] loss=1.40 avg=1.06\n",
      "[3227 | 30730.89] loss=0.56 avg=1.06\n",
      "[3228 | 30740.26] loss=1.20 avg=1.06\n",
      "[3229 | 30749.65] loss=1.52 avg=1.06\n",
      "[3230 | 30759.02] loss=0.84 avg=1.06\n",
      "[3231 | 30768.42] loss=1.07 avg=1.06\n",
      "[3232 | 30777.80] loss=1.52 avg=1.07\n",
      "[3233 | 30787.19] loss=1.38 avg=1.07\n",
      "[3234 | 30796.58] loss=0.90 avg=1.07\n",
      "[3235 | 30805.98] loss=1.52 avg=1.07\n",
      "[3236 | 30815.46] loss=1.79 avg=1.08\n",
      "[3237 | 30824.93] loss=0.69 avg=1.08\n",
      "[3238 | 30834.32] loss=1.50 avg=1.08\n",
      "[3239 | 30843.68] loss=1.57 avg=1.08\n",
      "[3240 | 30853.06] loss=1.26 avg=1.09\n",
      "[3241 | 30862.44] loss=1.12 avg=1.09\n",
      "[3242 | 30871.84] loss=1.03 avg=1.09\n",
      "[3243 | 30881.21] loss=0.65 avg=1.08\n",
      "[3244 | 30890.62] loss=0.88 avg=1.08\n",
      "[3245 | 30900.00] loss=1.41 avg=1.08\n",
      "[3246 | 30909.37] loss=0.82 avg=1.08\n",
      "[3247 | 30918.73] loss=1.05 avg=1.08\n",
      "[3248 | 30928.11] loss=0.88 avg=1.08\n",
      "[3249 | 30937.50] loss=1.01 avg=1.08\n",
      "[3250 | 30946.88] loss=1.30 avg=1.08\n",
      "[3251 | 30956.30] loss=1.08 avg=1.08\n",
      "[3252 | 30965.69] loss=1.07 avg=1.08\n",
      "[3253 | 30975.04] loss=0.75 avg=1.08\n",
      "[3254 | 30984.43] loss=0.62 avg=1.07\n",
      "[3255 | 30993.87] loss=0.79 avg=1.07\n",
      "[3256 | 31003.35] loss=1.09 avg=1.07\n",
      "[3257 | 31012.72] loss=1.07 avg=1.07\n",
      "[3258 | 31022.09] loss=1.40 avg=1.07\n",
      "[3259 | 31031.46] loss=1.04 avg=1.07\n",
      "[3260 | 31040.83] loss=1.55 avg=1.08\n",
      "[3261 | 31050.22] loss=1.05 avg=1.08\n",
      "[3262 | 31059.60] loss=1.14 avg=1.08\n",
      "[3263 | 31069.00] loss=1.01 avg=1.08\n",
      "[3264 | 31078.38] loss=0.81 avg=1.07\n",
      "[3265 | 31087.76] loss=1.66 avg=1.08\n",
      "[3266 | 31097.16] loss=0.99 avg=1.08\n",
      "[3267 | 31106.55] loss=0.99 avg=1.08\n",
      "[3268 | 31115.94] loss=0.98 avg=1.08\n",
      "[3269 | 31125.33] loss=1.53 avg=1.08\n",
      "[3270 | 31134.69] loss=0.90 avg=1.08\n",
      "[3271 | 31144.05] loss=1.15 avg=1.08\n",
      "[3272 | 31153.41] loss=0.88 avg=1.08\n",
      "[3273 | 31162.80] loss=0.74 avg=1.07\n",
      "[3274 | 31172.19] loss=1.01 avg=1.07\n",
      "[3275 | 31181.76] loss=0.91 avg=1.07\n",
      "[3276 | 31191.14] loss=0.89 avg=1.07\n",
      "[3277 | 31200.51] loss=1.50 avg=1.08\n",
      "[3278 | 31209.89] loss=1.55 avg=1.08\n",
      "[3279 | 31219.27] loss=1.01 avg=1.08\n",
      "[3280 | 31228.65] loss=1.00 avg=1.08\n",
      "[3281 | 31238.03] loss=0.84 avg=1.08\n",
      "[3282 | 31247.42] loss=0.90 avg=1.07\n",
      "[3283 | 31256.79] loss=1.76 avg=1.08\n",
      "[3284 | 31266.17] loss=0.95 avg=1.08\n",
      "[3285 | 31275.55] loss=0.69 avg=1.08\n",
      "[3286 | 31284.93] loss=0.87 avg=1.07\n",
      "[3287 | 31294.31] loss=1.31 avg=1.08\n",
      "[3288 | 31303.71] loss=1.40 avg=1.08\n",
      "[3289 | 31313.07] loss=0.97 avg=1.08\n",
      "[3290 | 31322.46] loss=0.58 avg=1.07\n",
      "[3291 | 31331.85] loss=1.18 avg=1.07\n",
      "[3292 | 31341.23] loss=1.07 avg=1.07\n",
      "[3293 | 31350.62] loss=1.29 avg=1.08\n",
      "[3294 | 31360.20] loss=0.89 avg=1.07\n",
      "[3295 | 31369.58] loss=0.83 avg=1.07\n",
      "[3296 | 31378.95] loss=1.02 avg=1.07\n",
      "[3297 | 31388.33] loss=0.97 avg=1.07\n",
      "[3298 | 31397.70] loss=1.10 avg=1.07\n",
      "[3299 | 31407.07] loss=0.85 avg=1.07\n",
      "[3300 | 31416.44] loss=1.35 avg=1.07\n",
      "Saving checkpoint/QGen_SQUAD_test/model-3300\n",
      "[3301 | 31429.36] loss=1.55 avg=1.08\n",
      "[3302 | 31438.82] loss=0.62 avg=1.07\n",
      "[3303 | 31448.24] loss=0.89 avg=1.07\n",
      "[3304 | 31457.76] loss=0.81 avg=1.07\n",
      "[3305 | 31467.19] loss=0.91 avg=1.07\n",
      "[3306 | 31476.59] loss=1.16 avg=1.07\n",
      "[3307 | 31485.99] loss=0.95 avg=1.07\n",
      "[3308 | 31495.40] loss=2.30 avg=1.08\n",
      "[3309 | 31504.80] loss=0.92 avg=1.08\n",
      "[3310 | 31514.20] loss=1.20 avg=1.08\n",
      "[3311 | 31523.62] loss=1.24 avg=1.08\n",
      "[3312 | 31533.07] loss=1.06 avg=1.08\n",
      "[3313 | 31542.66] loss=1.20 avg=1.08\n",
      "[3314 | 31552.04] loss=0.94 avg=1.08\n",
      "[3315 | 31561.46] loss=1.17 avg=1.08\n",
      "[3316 | 31570.85] loss=1.10 avg=1.08\n",
      "[3317 | 31580.29] loss=0.56 avg=1.07\n",
      "[3318 | 31589.71] loss=0.70 avg=1.07\n",
      "[3319 | 31599.10] loss=0.82 avg=1.07\n",
      "[3320 | 31608.50] loss=1.03 avg=1.07\n",
      "[3321 | 31617.90] loss=1.32 avg=1.07\n",
      "[3322 | 31627.29] loss=1.21 avg=1.07\n",
      "[3323 | 31636.66] loss=0.61 avg=1.07\n",
      "[3324 | 31646.06] loss=0.89 avg=1.07\n",
      "[3325 | 31655.46] loss=0.98 avg=1.06\n",
      "[3326 | 31664.87] loss=1.63 avg=1.07\n",
      "[3327 | 31674.30] loss=0.76 avg=1.07\n",
      "[3328 | 31683.72] loss=1.04 avg=1.07\n",
      "[3329 | 31693.13] loss=0.00 avg=1.06\n",
      "[3330 | 31702.54] loss=1.09 avg=1.06\n",
      "[3331 | 31711.99] loss=1.18 avg=1.06\n",
      "[3332 | 31721.59] loss=0.74 avg=1.05\n",
      "[3333 | 31731.03] loss=0.98 avg=1.05\n",
      "[3334 | 31740.41] loss=0.95 avg=1.05\n",
      "[3335 | 31749.84] loss=0.85 avg=1.05\n",
      "[3336 | 31759.22] loss=0.92 avg=1.05\n",
      "[3337 | 31768.63] loss=1.29 avg=1.05\n",
      "[3338 | 31778.02] loss=0.84 avg=1.05\n",
      "[3339 | 31787.43] loss=0.81 avg=1.05\n",
      "[3340 | 31796.83] loss=1.12 avg=1.05\n",
      "[3341 | 31806.23] loss=1.31 avg=1.05\n",
      "[3342 | 31815.65] loss=1.12 avg=1.05\n",
      "[3343 | 31825.06] loss=0.66 avg=1.05\n",
      "[3344 | 31834.47] loss=1.21 avg=1.05\n",
      "[3345 | 31843.87] loss=0.59 avg=1.04\n",
      "[3346 | 31853.27] loss=1.00 avg=1.04\n",
      "[3347 | 31862.67] loss=1.96 avg=1.05\n",
      "[3348 | 31872.09] loss=1.48 avg=1.06\n",
      "[3349 | 31881.49] loss=0.89 avg=1.06\n",
      "[3350 | 31890.91] loss=1.57 avg=1.06\n",
      "[3351 | 31900.52] loss=1.10 avg=1.06\n",
      "[3352 | 31909.92] loss=1.15 avg=1.06\n",
      "[3353 | 31919.32] loss=1.30 avg=1.06\n",
      "[3354 | 31928.71] loss=0.77 avg=1.06\n",
      "[3355 | 31938.12] loss=1.45 avg=1.07\n",
      "[3356 | 31947.50] loss=0.95 avg=1.06\n",
      "[3357 | 31956.89] loss=1.51 avg=1.07\n",
      "[3358 | 31966.29] loss=0.86 avg=1.07\n",
      "[3359 | 31975.71] loss=1.41 avg=1.07\n",
      "[3360 | 31985.13] loss=0.53 avg=1.06\n",
      "[3361 | 31994.53] loss=1.34 avg=1.07\n",
      "[3362 | 32003.93] loss=0.75 avg=1.06\n",
      "[3363 | 32013.34] loss=1.00 avg=1.06\n",
      "[3364 | 32022.73] loss=0.69 avg=1.06\n",
      "[3365 | 32032.13] loss=0.71 avg=1.06\n",
      "[3366 | 32041.54] loss=1.55 avg=1.06\n",
      "[3367 | 32050.97] loss=0.76 avg=1.06\n",
      "[3368 | 32060.39] loss=0.85 avg=1.06\n",
      "[3369 | 32069.81] loss=0.99 avg=1.06\n",
      "[3370 | 32079.42] loss=0.90 avg=1.05\n",
      "[3371 | 32088.84] loss=1.43 avg=1.06\n",
      "[3372 | 32098.26] loss=0.57 avg=1.05\n",
      "[3373 | 32107.64] loss=1.24 avg=1.06\n",
      "[3374 | 32117.03] loss=1.05 avg=1.06\n",
      "[3375 | 32126.43] loss=1.54 avg=1.06\n",
      "[3376 | 32135.83] loss=0.91 avg=1.06\n",
      "[3377 | 32145.22] loss=2.15 avg=1.07\n",
      "[3378 | 32154.62] loss=0.72 avg=1.07\n",
      "[3379 | 32164.03] loss=1.16 avg=1.07\n",
      "[3380 | 32173.41] loss=1.34 avg=1.07\n",
      "[3381 | 32182.81] loss=0.78 avg=1.07\n",
      "[3382 | 32192.22] loss=0.69 avg=1.06\n",
      "[3383 | 32201.62] loss=1.05 avg=1.06\n",
      "[3384 | 32211.04] loss=1.00 avg=1.06\n",
      "[3385 | 32220.47] loss=1.04 avg=1.06\n",
      "[3386 | 32229.89] loss=1.16 avg=1.06\n",
      "[3387 | 32239.27] loss=1.20 avg=1.06\n",
      "[3388 | 32248.65] loss=1.17 avg=1.07\n",
      "[3389 | 32258.21] loss=1.14 avg=1.07\n",
      "[3390 | 32267.61] loss=1.10 avg=1.07\n",
      "[3391 | 32277.03] loss=0.83 avg=1.06\n",
      "[3392 | 32286.42] loss=0.98 avg=1.06\n",
      "[3393 | 32295.80] loss=0.83 avg=1.06\n",
      "[3394 | 32305.18] loss=1.06 avg=1.06\n",
      "[3395 | 32314.57] loss=1.38 avg=1.06\n",
      "[3396 | 32323.99] loss=1.00 avg=1.06\n",
      "[3397 | 32333.39] loss=1.55 avg=1.07\n",
      "[3398 | 32342.77] loss=1.04 avg=1.07\n",
      "[3399 | 32352.19] loss=0.82 avg=1.07\n",
      "[3400 | 32361.60] loss=0.88 avg=1.06\n",
      "Saving checkpoint/QGen_SQUAD_test/model-3400\n",
      "[3401 | 32374.56] loss=1.35 avg=1.07\n",
      "[3402 | 32383.92] loss=0.94 avg=1.07\n",
      "[3403 | 32393.32] loss=1.04 avg=1.06\n",
      "[3404 | 32402.80] loss=1.14 avg=1.07\n",
      "[3405 | 32412.18] loss=1.27 avg=1.07\n",
      "[3406 | 32421.59] loss=1.12 avg=1.07\n",
      "[3407 | 32430.99] loss=1.44 avg=1.07\n",
      "[3408 | 32440.55] loss=1.66 avg=1.08\n",
      "[3409 | 32449.92] loss=0.81 avg=1.08\n",
      "[3410 | 32459.31] loss=1.46 avg=1.08\n",
      "[3411 | 32468.69] loss=0.92 avg=1.08\n",
      "[3412 | 32478.08] loss=1.36 avg=1.08\n",
      "[3413 | 32487.47] loss=1.01 avg=1.08\n",
      "[3414 | 32496.82] loss=0.85 avg=1.08\n",
      "[3415 | 32506.21] loss=0.83 avg=1.08\n",
      "[3416 | 32515.58] loss=1.05 avg=1.07\n",
      "[3417 | 32524.99] loss=0.89 avg=1.07\n",
      "[3418 | 32534.37] loss=1.23 avg=1.07\n",
      "[3419 | 32543.76] loss=1.03 avg=1.07\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3420 | 32553.15] loss=0.69 avg=1.07\n",
      "[3421 | 32562.52] loss=1.85 avg=1.08\n",
      "[3422 | 32571.92] loss=0.82 avg=1.08\n",
      "[3423 | 32581.30] loss=16.88 avg=1.23\n",
      "[3424 | 32590.71] loss=0.65 avg=1.23\n",
      "[3425 | 32600.08] loss=1.43 avg=1.23\n",
      "[3426 | 32609.45] loss=0.85 avg=1.23\n",
      "[3427 | 32619.01] loss=0.93 avg=1.22\n",
      "[3428 | 32628.38] loss=1.08 avg=1.22\n",
      "[3429 | 32637.78] loss=0.95 avg=1.22\n",
      "[3430 | 32647.14] loss=1.22 avg=1.22\n",
      "[3431 | 32656.52] loss=1.18 avg=1.22\n",
      "[3432 | 32665.92] loss=1.16 avg=1.22\n",
      "[3433 | 32675.30] loss=1.03 avg=1.22\n",
      "[3434 | 32684.68] loss=1.10 avg=1.21\n",
      "[3435 | 32694.07] loss=1.20 avg=1.21\n",
      "[3436 | 32703.44] loss=1.04 avg=1.21\n",
      "[3437 | 32712.82] loss=1.25 avg=1.21\n",
      "[3438 | 32722.21] loss=1.61 avg=1.22\n",
      "[3439 | 32731.61] loss=1.19 avg=1.22\n",
      "[3440 | 32740.99] loss=1.17 avg=1.22\n",
      "[3441 | 32750.36] loss=1.24 avg=1.22\n",
      "[3442 | 32759.75] loss=0.95 avg=1.21\n",
      "[3443 | 32769.13] loss=1.44 avg=1.22\n",
      "[3444 | 32778.50] loss=1.33 avg=1.22\n",
      "[3445 | 32787.91] loss=1.26 avg=1.22\n",
      "[3446 | 32797.43] loss=0.79 avg=1.21\n",
      "[3447 | 32806.80] loss=1.27 avg=1.21\n",
      "[3448 | 32816.20] loss=1.52 avg=1.22\n",
      "[3449 | 32825.59] loss=0.77 avg=1.21\n",
      "[3450 | 32834.99] loss=1.46 avg=1.22\n",
      "[3451 | 32844.36] loss=1.13 avg=1.21\n",
      "[3452 | 32853.74] loss=1.14 avg=1.21\n",
      "[3453 | 32863.10] loss=0.66 avg=1.21\n",
      "[3454 | 32872.49] loss=1.09 avg=1.21\n",
      "[3455 | 32881.87] loss=0.70 avg=1.20\n",
      "[3456 | 32891.26] loss=0.72 avg=1.20\n",
      "[3457 | 32900.61] loss=0.92 avg=1.19\n",
      "[3458 | 32910.01] loss=0.73 avg=1.19\n",
      "[3459 | 32919.37] loss=0.85 avg=1.19\n",
      "[3460 | 32928.73] loss=0.56 avg=1.18\n",
      "[3461 | 32938.12] loss=0.86 avg=1.18\n",
      "[3462 | 32947.50] loss=0.85 avg=1.17\n",
      "[3463 | 32956.85] loss=0.90 avg=1.17\n",
      "[3464 | 32966.26] loss=0.88 avg=1.17\n",
      "[3465 | 32975.80] loss=1.21 avg=1.17\n",
      "[3466 | 32985.19] loss=0.61 avg=1.16\n",
      "[3467 | 32994.64] loss=1.01 avg=1.16\n",
      "[3468 | 33004.05] loss=0.93 avg=1.16\n",
      "[3469 | 33013.44] loss=1.15 avg=1.16\n",
      "[3470 | 33022.80] loss=0.95 avg=1.16\n",
      "[3471 | 33032.17] loss=1.17 avg=1.16\n",
      "[3472 | 33041.56] loss=1.18 avg=1.16\n",
      "[3473 | 33050.95] loss=1.06 avg=1.16\n",
      "[3474 | 33060.33] loss=0.76 avg=1.15\n",
      "[3475 | 33069.71] loss=0.63 avg=1.15\n",
      "[3476 | 33079.08] loss=0.98 avg=1.14\n",
      "[3477 | 33088.48] loss=1.17 avg=1.15\n",
      "[3478 | 33097.86] loss=0.96 avg=1.14\n",
      "[3479 | 33107.23] loss=1.21 avg=1.14\n",
      "[3480 | 33116.61] loss=1.13 avg=1.14\n",
      "[3481 | 33125.98] loss=0.87 avg=1.14\n",
      "[3482 | 33135.36] loss=1.12 avg=1.14\n",
      "[3483 | 33144.76] loss=0.68 avg=1.14\n",
      "[3484 | 33154.21] loss=0.91 avg=1.13\n",
      "[3485 | 33163.69] loss=1.41 avg=1.14\n",
      "[3486 | 33173.11] loss=0.65 avg=1.13\n",
      "[3487 | 33182.48] loss=0.36 avg=1.12\n",
      "[3488 | 33191.87] loss=0.82 avg=1.12\n",
      "[3489 | 33201.26] loss=1.22 avg=1.12\n",
      "[3490 | 33210.65] loss=0.54 avg=1.12\n",
      "[3491 | 33220.06] loss=1.55 avg=1.12\n",
      "[3492 | 33229.46] loss=1.34 avg=1.12\n",
      "[3493 | 33238.83] loss=1.18 avg=1.12\n",
      "[3494 | 33248.21] loss=0.87 avg=1.12\n",
      "[3495 | 33257.63] loss=1.16 avg=1.12\n",
      "[3496 | 33267.04] loss=0.79 avg=1.12\n",
      "[3497 | 33276.40] loss=1.35 avg=1.12\n",
      "[3498 | 33285.79] loss=0.87 avg=1.12\n",
      "[3499 | 33295.17] loss=0.89 avg=1.12\n",
      "[3500 | 33304.61] loss=0.77 avg=1.11\n",
      "Saving checkpoint/QGen_SQUAD_test/model-3500\n",
      "[3501 | 33317.54] loss=0.90 avg=1.11\n",
      "[3502 | 33326.95] loss=1.27 avg=1.11\n",
      "[3503 | 33336.51] loss=0.95 avg=1.11\n",
      "[3504 | 33346.01] loss=1.07 avg=1.11\n",
      "[3638 | 34610.50] loss=1.23 avg=1.06\n",
      "[3639 | 34619.91] loss=0.97 avg=1.06\n",
      "[3640 | 34629.27] loss=0.80 avg=1.06\n",
      "[3641 | 34638.65] loss=0.95 avg=1.05\n",
      "[3642 | 34648.04] loss=1.50 avg=1.06\n",
      "[3643 | 34657.43] loss=1.00 avg=1.06\n",
      "[3644 | 34666.80] loss=0.50 avg=1.05\n",
      "[3645 | 34676.17] loss=0.66 avg=1.05\n",
      "[3646 | 34685.54] loss=1.26 avg=1.05\n",
      "[3647 | 34694.91] loss=1.34 avg=1.05\n",
      "[3648 | 34704.31] loss=0.94 avg=1.05\n",
      "[3649 | 34713.69] loss=0.82 avg=1.05\n",
      "[3650 | 34723.08] loss=0.89 avg=1.05\n",
      "[3651 | 34732.46] loss=1.01 avg=1.05\n",
      "[3652 | 34741.85] loss=1.20 avg=1.05\n",
      "[3653 | 34751.24] loss=1.05 avg=1.05\n",
      "[3654 | 34760.61] loss=1.56 avg=1.06\n",
      "[3655 | 34770.01] loss=0.99 avg=1.05\n",
      "[3656 | 34779.57] loss=1.05 avg=1.05\n",
      "[3657 | 34788.94] loss=0.98 avg=1.05\n",
      "[3658 | 34798.31] loss=1.16 avg=1.05\n",
      "[3659 | 34807.68] loss=0.91 avg=1.05\n",
      "[3660 | 34817.06] loss=0.85 avg=1.05\n",
      "[3661 | 34826.46] loss=0.90 avg=1.05\n",
      "[3662 | 34835.84] loss=0.94 avg=1.05\n",
      "[3663 | 34845.20] loss=0.78 avg=1.05\n",
      "[3664 | 34854.58] loss=1.02 avg=1.05\n",
      "[3665 | 34864.01] loss=0.89 avg=1.04\n",
      "[3666 | 34873.37] loss=0.56 avg=1.04\n",
      "[3667 | 34882.74] loss=1.15 avg=1.04\n",
      "[3668 | 34892.15] loss=1.13 avg=1.04\n",
      "[3669 | 34901.53] loss=0.87 avg=1.04\n",
      "[3670 | 34910.92] loss=1.00 avg=1.04\n",
      "[3671 | 34920.28] loss=1.40 avg=1.04\n",
      "[3672 | 34929.67] loss=1.22 avg=1.04\n",
      "[3673 | 34939.08] loss=1.75 avg=1.05\n",
      "[3674 | 34948.48] loss=1.05 avg=1.05\n",
      "[3675 | 34958.01] loss=1.06 avg=1.05\n",
      "[3676 | 34967.38] loss=0.66 avg=1.05\n",
      "[3677 | 34976.76] loss=0.65 avg=1.04\n",
      "[3678 | 34986.14] loss=0.92 avg=1.04\n",
      "[3679 | 34995.51] loss=0.96 avg=1.04\n",
      "[3680 | 35004.94] loss=0.97 avg=1.04\n",
      "[3681 | 35014.32] loss=0.81 avg=1.04\n",
      "[3682 | 35023.71] loss=1.42 avg=1.04\n",
      "[3683 | 35033.09] loss=0.46 avg=1.04\n",
      "[3684 | 35042.47] loss=1.22 avg=1.04\n",
      "[3685 | 35051.83] loss=1.04 avg=1.04\n",
      "[3686 | 35061.22] loss=1.65 avg=1.04\n",
      "[3687 | 35070.64] loss=0.85 avg=1.04\n",
      "[3688 | 35080.03] loss=1.12 avg=1.04\n",
      "[3689 | 35089.42] loss=0.93 avg=1.04\n",
      "[3690 | 35098.79] loss=1.06 avg=1.04\n",
      "[3691 | 35108.16] loss=1.32 avg=1.05\n",
      "[3692 | 35117.53] loss=0.64 avg=1.04\n",
      "[3693 | 35126.91] loss=1.04 avg=1.04\n",
      "[3694 | 35136.40] loss=0.73 avg=1.04\n",
      "[3695 | 35145.80] loss=1.04 avg=1.04\n",
      "[3696 | 35155.17] loss=0.91 avg=1.04\n",
      "[3697 | 35164.57] loss=0.92 avg=1.04\n",
      "[3698 | 35173.98] loss=1.28 avg=1.04\n",
      "[3699 | 35183.35] loss=0.86 avg=1.04\n",
      "[3700 | 35192.76] loss=1.43 avg=1.04\n",
      "Saving checkpoint/QGen_SQUAD_test/model-3700\n",
      "[3701 | 35205.67] loss=0.75 avg=1.04\n",
      "[3702 | 35215.07] loss=1.00 avg=1.04\n",
      "[3703 | 35224.47] loss=1.28 avg=1.04\n",
      "[3704 | 35233.98] loss=1.12 avg=1.04\n",
      "[3705 | 35243.35] loss=1.25 avg=1.04\n",
      "[3706 | 35252.78] loss=1.01 avg=1.04\n",
      "[3707 | 35262.18] loss=1.31 avg=1.04\n",
      "[3708 | 35271.57] loss=1.09 avg=1.04\n",
      "[3709 | 35280.97] loss=1.58 avg=1.05\n",
      "[3710 | 35290.36] loss=1.15 avg=1.05\n",
      "[3711 | 35299.77] loss=0.46 avg=1.05\n",
      "[3712 | 35309.20] loss=0.80 avg=1.04\n",
      "[3713 | 35318.77] loss=0.95 avg=1.04\n",
      "[3714 | 35328.18] loss=1.01 avg=1.04\n",
      "[3715 | 35337.59] loss=0.95 avg=1.04\n",
      "[3716 | 35347.00] loss=0.69 avg=1.04\n",
      "[3717 | 35356.38] loss=1.14 avg=1.04\n",
      "[3718 | 35365.78] loss=1.11 avg=1.04\n",
      "[3719 | 35375.20] loss=0.97 avg=1.04\n",
      "[3720 | 35384.60] loss=1.37 avg=1.04\n",
      "[3721 | 35394.00] loss=1.07 avg=1.04\n",
      "[3722 | 35403.40] loss=1.09 avg=1.04\n",
      "[3723 | 35412.80] loss=0.64 avg=1.04\n",
      "[3724 | 35422.20] loss=0.89 avg=1.04\n"
     ]
    }
   ],
   "source": [
    "gpt2.finetune(sess,\n",
    "              dataset= \"data/squad_train.txt\", \n",
    "              model_name='345M', \n",
    "              steps=4000, \n",
    "              restore_from='fresh', \n",
    "              run_name='QGen_SQUAD_test', \n",
    "              print_every=1, \n",
    "              sample_every=2000, \n",
    "              save_every=100  \n",
    "             )\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing Trained Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = open('data/squad_test.txt')\n",
    "testset = file.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_lst = testset.split('<|endoftext|>\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_str = test_lst[0].split('[QUESTION]:')[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "ctx = test_str\n",
    "pre = ctx + \" [QUESTION]:\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading checkpoint checkpoint/QGen_SQUAD_test/model-4000\n",
      "INFO:tensorflow:Restoring parameters from checkpoint/QGen_SQUAD_test/model-4000\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "sess = gpt2.start_tf_sess()\n",
    "gpt2.load_gpt2(sess, run_name='QGen_SQUAD_test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|startoftext|>\n",
      "[CONTEXT]: The Normans (Norman: Nourmands; French: Normands; Latin: Normanni) were the people who in the 10th and 11th centuries gave their name to Normandy, a region in France. They were descended from Norse (\"Norman\" comes from \"Norseman\") raiders and pirates from Denmark, Iceland and Norway who, under their leader Rollo, agreed to swear fealty to King Charles III of West Francia. Through generations of assimilation and mixing with the native Frankish and Roman-Gaulish populations, their descendants would gradually merge with the Carolingian-based cultures of West Francia. The distinct cultural and ethnic identity of the Normans emerged initially in the first half of the 10th century, and it continued to evolve over the succeeding centuries.  [QUESTION]: What language was the Normans called? <|endoftext|>\n",
      "<|startoftext|>\n",
      "[CONTEXT]: The Normans (Norman: Nourmands; French: Normands; Latin: Normanni) were the people who in the 10th and 11th centuries gave their name to Normandy, a region in France. They were descended from Norse (\"Norman\" comes from \"Norseman\") raiders and pirates from Denmark, Iceland and Norway who, under their leader Rollo, agreed to swear fealty to King Charles III of West Francia. Through generations of assimilation and mixing with the native Frankish and Roman-Gaulish populations, their descendants would gradually merge with the Carolingian-based cultures of West Francia. The distinct cultural and ethnic identity of the Normans emerged initially in the first half of the 10th century, and it continued to evolve over the succeeding centuries.  [QUESTION]: Where did the Normans come from? <|endoftext|>\n",
      "<|startoftext|>\n",
      "[CONTEXT]: The Normans (Norman: Nourmands; French: Normands; Latin: Normanni) were the people who in the 10th and 11th centuries gave their name to Normandy, a region in France. They were descended from Norse (\"Norman\" comes from \"Norseman\") raiders and pirates from Denmark, Iceland and Norway who, under their leader Rollo, agreed to swear fealty to King Charles III of West Francia. Through generations of assimilation and mixing with the native Frankish and Roman-Gaulish populations, their descendants would gradually merge with the Carolingian-based cultures of West Francia. The distinct cultural and ethnic identity of the Normans emerged initially in the first half of the 10th century, and it continued to evolve over the succeeding centuries.  [QUESTION]: What language was the Normans called? <|endoftext|>\n",
      "<|startoftext|>\n",
      "[CONTEXT]: The Normans (Norman: Nourmands; French: Normands; Latin: Normanni) were the people who in the 10th and 11th centuries gave their name to Normandy, a region in France. They were descended from Norse (\"Norman\" comes from \"Norseman\") raiders and pirates from Denmark, Iceland and Norway who, under their leader Rollo, agreed to swear fealty to King Charles III of West Francia. Through generations of assimilation and mixing with the native Frankish and Roman-Gaulish populations, their descendants would gradually merge with the Carolingian-based cultures of West Francia. The distinct cultural and ethnic identity of the Normans emerged initially in the first half of the 10th century, and it continued to evolve over the succeeding centuries.  [QUESTION]: When did the Normans' culture of West Francia merge with the Carolingian? <|endoftext|>\n",
      "<|startoftext|>\n",
      "[CONTEXT]: The Normans (Norman: Nourmands; French: Normanni) were the people who in the 10th and 11th centuries gave their name to Normandy, a region in France. They were descended from Norse (\"Norman\" comes from \"Norseman\") raiders and pirates from Denmark, Iceland and Norway who, under their leader Rollo, agreed to swear fealty to King Charles III of West Francia. Through generations of assimilation and mixing with the native Frankish and Roman-Gaulish populations, their descendants would gradually merge with the Carolingian-based cultures of West Francia. The distinct cultural and ethnic identity of the Normans emerged initially in the first half of the 10th century, and it continued to evolve over the succeeding centuries.  [QUESTION]: Who did the Normans' descend from? <|endoftext|>\n",
      "<|startoftext|>\n",
      "[CONTEXT]: The Normans (Norman: Nourmands; French: Normanni) were the people who in the 10th\n",
      "====================\n",
      "<|startoftext|>\n",
      "[CONTEXT]: The Normans (Norman: Nourmands; French: Normands; Latin: Normanni) were the people who in the 10th and 11th centuries gave their name to Normandy, a region in France. They were descended from Norse (\"Norman\" comes from \"Norseman\") raiders and pirates from Denmark, Iceland and Norway who, under their leader Rollo, agreed to swear fealty to King Charles III of West Francia. Through generations of assimilation and mixing with the native Frankish and Roman-Gaulish populations, their descendants would gradually merge with the Carolingian-based cultures of West Francia. The distinct cultural and ethnic identity of the Normans emerged initially in the first half of the 10th century, and it continued to evolve over the succeeding centuries.  [QUESTION]: Who gave their name to Normandy? <|endoftext|>\n",
      "<|startoftext|>\n",
      "[CONTEXT]: The Normans (Norman: Nourmands; French: Normands; Latin: Normanni) were the people who in the 10th and 11th centuries gave their name to Normandy, a region in France. They were descended from Norse (\"Norman\" comes from \"Norseman\") raiders and pirates from Denmark, Iceland and Norway who, under their leader Rollo, agreed to swear fealty to King Charles III of West Francia. Through generations of assimilation and mixing with the native Frankish and Roman-Gaulish populations, their descendants would gradually merge with the Carolingian-based cultures of West Francia. The distinct cultural and ethnic identity of the Normans emerged initially in the first half of the 10th century, and it continued to evolve over the succeeding centuries.  [QUESTION]: When did Normans establish themselves in Normandy? <|endoftext|>\n",
      "<|startoftext|>\n",
      "[CONTEXT]: The Normans (Norman: Nourmands; French: Normands; Latin: Normanni) were the people who in the 10th and 11th centuries gave their name to Normandy, a region in France. They were descended from Norse (\"Norman\" comes from \"Norseman\") raiders and pirates from Denmark, Iceland and Norway who, under their leader Rollo, agreed to swear fealty to King Charles III of West Francia. Through generations of assimilation and mixing with the native Frankish and Roman-Gaulish populations, their descendants would gradually merge with the Carolingian-based cultures of West Francia. The distinct cultural and ethnic identity of the Normans emerged initially in the first half of the 10th century, and it continued to evolve over the succeeding centuries.  [QUESTION]: Where did the Normans originate? <|endoftext|>\n",
      "<|startoftext|>\n",
      "[CONTEXT]: The Normans (Norman: Nourmands; French: Normands; Latin: Normanni) were the people who in the 10th and 11th centuries gave their name to Normandy, a region in France. They were descended from Norse (\"Norman\" comes from \"Norseman\") raiders and pirates from Denmark, Iceland and Norway who, under their leader Rollo, agreed to swear fealty to King Charles III of West Francia. Through generations of assimilation and mixing with the native Frankish and Roman-Gaulish populations, their descendants would gradually merge with the Carolingian-based cultures of West Francia. The distinct cultural and ethnic identity of the Normans emerged initially in the first half of the 10th century, and it continued to evolve over the succeeding centuries.  [QUESTION]: Where did the Normans give their name to? <|endoftext|>\n",
      "<|startoftext|>\n",
      "[CONTEXT]: The Normans (Norman: Nourmands; French: Normanni) were the people who in the 10th and 11th centuries gave their name to Normandy, a region in France. They were descended from Norse (\"Norman\" comes from \"Norseman\") raiders and pirates from Denmark, Iceland and Norway who, under their leader Rollo, agreed to swear fealty to King Charles III of West Francia. Through generations of assimilation and mixing with the native Frankish and Roman-Gaulish populations, their descendants would gradually merge with the Carolingian-based cultures of West Francia. The distinct cultural and ethnic identity of the Normans emerged initially in the first half of the 10th century, and it continued to evolve over the succeeding centuries.  [QUESTION]: What happened to the Normans? <|endoftext|>\n",
      "<|startoftext|>\n",
      "[CONTEXT]: The Normans (Norman: Nourmands; French: Normanni) were the people who in the 10th and 11th centuries gave their name to Normandy, a\n",
      "====================\n",
      "<|startoftext|>\n",
      "[CONTEXT]: The Normans (Norman: Nourmands; French: Normands; Latin: Normanni) were the people who in the 10th and 11th centuries gave their name to Normandy, a region in France. They were descended from Norse (\"Norman\" comes from \"Norseman\") raiders and pirates from Denmark, Iceland and Norway who, under their leader Rollo, agreed to swear fealty to King Charles III of West Francia. Through generations of assimilation and mixing with the native Frankish and Roman-Gaulish populations, their descendants would gradually merge with the Carolingian-based cultures of West Francia. The distinct cultural and ethnic identity of the Normans emerged initially in the first half of the 10th century, and it continued to evolve over the succeeding centuries.  [QUESTION]: What was the name of the Normans? <|endoftext|>\n",
      "<|startoftext|>\n",
      "[CONTEXT]: The Normans (Norman: Nourmands; French: Normants; Latin: Normanni) were the people who in the 10th and 11th centuries gave their name to Normandy, a region in France. They were descended from Norse (\"Norman\" comes from \"Norseman\") raiders and pirates from Denmark, Iceland and Norway who, under their leader Rollo, agreed to swear fealty to King Charles III of West Francia. Through generations of assimilation and mixing with the native Frankish and Roman-Gaulish populations, their descendants would gradually merge with the Carolingian-based cultures of West Francia. The distinct cultural and ethnic identity of the Normans emerged initially in the first half of the 10th century, and it continued to evolve over the succeeding centuries.  [QUESTION]: When did the Normans give their name? <|endoftext|>\n",
      "<|startoftext|>\n",
      "[CONTEXT]: The Normans (Norman: Nourmands; French: Normants; Latin: Normanni) were the people who in the 10th and 11th centuries gave their name to Normandy, a region in France. They were descended from Norse (\"Norman\" comes from \"Norseman\") raiders and pirates from Denmark, Iceland and Norway who, under their leader Rollo, agreed to swear fealty to King Charles III of West Francia. Through generations of assimilation and mixing with the native Frankish and Roman-Gaulish populations, their descendants would gradually merge with the Carolingian-based cultures of West Francia. The distinct cultural and ethnic identity of the Normans emerged initially in the first half of the 10th century, and it continued to evolve over the succeeding centuries.  [QUESTION]: What was the Normans named? <|endoftext|>\n",
      "<|startoftext|>\n",
      "[CONTEXT]: The Normans (Norman: Nourmands; French: Normants; Latin: Normanni) were the people who in the 10th and 11th centuries gave their name to Normandy, a region in France. They were descended from Norse (\"Norman\" comes from \"Norseman\") raiders and pirates from Denmark, Iceland and Norway who, under their leader Rollo, agreed to swear fealty to King Charles III of West Francia. Through generations of assimilation and mixing with the native Frankish and Roman-Gaulish populations, their descendants would gradually merge with the Carolingian-based cultures of West Francia. The distinct cultural and ethnic identity of the Normans emerged initially in the first half of the 10th century, and it continued to evolve over the succeeding centuries.  [QUESTION]: When did the Normans merge with the Carolingians? <|endoftext|>\n",
      "<|startoftext|>\n",
      "[CONTEXT]: The Normans (Norman: Nourmands; French: Normants; Latin: Normanni) were the people who in the 10th and 11th centuries gave their name to Normandy, a region in France. They were descended from Norse (\"Norman\" comes from \"Norseman\") raiders and pirates from Denmark, Iceland and Norway who, under their leader Rollo, agreed to swear fealty to King Charles III of West Francia. Through generations of assimilation and mixing with the native Frankish and Roman-Gaulish populations, their descendants would gradually merge with the Carolingian-based cultures of West Francia. The distinct cultural and ethnic identity of the Normans emerged initially in the first half of the 10th century, and it continued to evolve over the succeeding centuries.  [QUESTION]: When the Normans took over Normandy, what language did they gain? <|endoftext|>\n",
      "<|startoftext|>\n",
      "[CONTEXT]: The Normans (Norman: Nourmands; French: Normants; Latin: Norm\n",
      "====================\n",
      "<|startoftext|>\n",
      "[CONTEXT]: The Normans (Norman: Nourmands; French: Normands; Latin: Normanni) were the people who in the 10th and 11th centuries gave their name to Normandy, a region in France. They were descended from Norse (\"Norman\" comes from \"Norseman\") raiders and pirates from Denmark, Iceland and Norway who, under their leader Rollo, agreed to swear fealty to King Charles III of West Francia. Through generations of assimilation and mixing with the native Frankish and Roman-Gaulish populations, their descendants would gradually merge with the Carolingian-based cultures of West Francia. The distinct cultural and ethnic identity of the Normans emerged initially in the first half of the 10th century, and it continued to evolve over the succeeding centuries.  [QUESTION]: What was the name of the Normans? <|endoftext|>\n",
      "<|startoftext|>\n",
      "[CONTEXT]: The Normans (Norman: Nourmands; French: Normands; Latin: Normanni) were the people who in the 10th and 11th centuries gave their name to Normandy, a region in France. They were descended from Norse (\"Norman\" comes from \"Norseman\") raiders and pirates from Denmark, Iceland and Norway who, under their leader Rollo, agreed to swear fealty to King Charles III of West Francia. Through generations of assimilation and mixing with the native Frankish and Roman-Gaulish populations, their descendants would gradually merge with the Carolingian-based cultures of West Francia. The distinct cultural and ethnic identity of the Normans emerged initially in the first half of the 10th century, and it continued to evolve over the succeeding centuries.  [QUESTION]: When did the Normans take on their named name? <|endoftext|>\n",
      "<|startoftext|>\n",
      "[CONTEXT]: The Normans (Norman: Nourmands; French: Normands; Latin: Normanni) were the people who in the 10th and 11th centuries gave their name to Normandy, a region in France. They were descended from Norse (\"Norman\" comes from \"Norseman\") raiders and pirates from Denmark, Iceland and Norway who, under their leader Rollo, agreed to swear fealty to King Charles III of West Francia. Through generations of assimilation and mixing with the native Frankish and Roman-Gaulish populations, their descendants would gradually merge with the Carolingian-based cultures of West Francia. The distinct cultural and ethnic identity of the Normans emerged initially in the first half of the 10th century, and it continued to evolve over the succeeding centuries.  [QUESTION]: What was the name of the Normans? <|endoftext|>\n",
      "<|startoftext|>\n",
      "[CONTEXT]: The Normans (Norman: Nourmands; French: Normands; Latin: Normanni) were the people who in the 10th and 11th centuries gave their name to Normandy, a region in France. They were descended from Norse (\"Norman\" comes from \"Norseman\") raiders and pirates from Denmark, Iceland and Norway who, under their leader Rollo, agreed to swear fealty to King Charles III of West Francia. Through generations of assimilation and mixing with the native Frankish and Roman-Gaulish populations, their descendants would gradually merge with the Carolingian-based cultures of West Francia. The distinct cultural and ethnic identity of the Normans emerged initially in the first half of the 10th century, and it continued to evolve over the succeeding centuries.  [QUESTION]: What did the Normans give their name to? <|endoftext|>\n",
      "<|startoftext|>\n",
      "[CONTEXT]: The Normans (Norman: Nourmands; French: Normanni) were the people who in the 10th and 11th centuries gave their name to Normandy, a region in France. They were descended from Norse (\"Norman\" comes from \"Norseman\") raiders and pirates from Denmark, Iceland and Norway who, under their leader Rollo, agreed to swear fealty to King Charles III of West Francia. Through generations of assimilation and mixing with the native Frankish and Roman-Gaulish populations, their descendants would gradually merge with the Carolingian-based cultures of West Francia. The distinct cultural and ethnic identity of the Normans emerged initially in the first half of the 10th century, and it continued to evolve over the succeeding centuries.  [QUESTION]: What was the name of the Normans that gave Normandy? <|endoftext|>\n",
      "<|startoftext|>\n",
      "[CONTEXT]: The Normans (Norman: Nourmands; French: Normanni) were the people who in the 10th\n",
      "====================\n",
      "<|startoftext|>\n",
      "[CONTEXT]: The Normans (Norman: Nourmands; French: Normands; Latin: Normanni) were the people who in the 10th and 11th centuries gave their name to Normandy, a region in France. They were descended from Norse (\"Norman\" comes from \"Norseman\") raiders and pirates from Denmark, Iceland and Norway who, under their leader Rollo, agreed to swear fealty to King Charles III of West Francia. Through generations of assimilation and mixing with the native Frankish and Roman-Gaulish populations, their descendants would gradually merge with the Carolingian-based cultures of West Francia. The distinct cultural and ethnic identity of the Normans emerged initially in the first half of the 10th century, and it continued to evolve over the succeeding centuries.  [QUESTION]: Which people were the Normans? <|endoftext|>\n",
      "<|startoftext|>\n",
      "[CONTEXT]: The Normans (Norman: Nourmands; French: Normands; Latin: Normanni) were the people who in the 10th and 11th centuries gave their name to Normandy, a region in France. They were descended from Norse (\"Norman\" comes from \"Norseman\") raiders and pirates from Denmark, Iceland and Norway who, under their leader Rollo, agreed to swear fealty to King Charles III of West Francia. Through generations of assimilation and mixing with the native Frankish and Roman-Gaulish populations, their descendants would gradually merge with the Carolingian-based cultures of West Francia. The distinct cultural and ethnic identity of the Normans emerged initially in the first half of the 10th century, and it continued to evolve over the succeeding centuries.  [QUESTION]: What was the name of the leader of the Normans? <|endoftext|>\n",
      "<|startoftext|>\n",
      "[CONTEXT]: The Normans (Norman: Nourmands; French: Normands; Latin: Normanni) were the people who in the 10th and 11th centuries gave their name to Normandy, a region in France. They were descended from Norse (\"Norman\" comes from \"Norseman\") raiders and pirates from Denmark, Iceland and Norway who, under their leader Rollo, agreed to swear fealty to King Charles III of West Francia. Through generations of assimilation and mixing with the native Frankish and Roman-Gaulish populations, their descendants would gradually merge with the Carolingian-based cultures of West Francia. The distinct cultural and ethnic identity of the Normans emerged initially in the first half of the 10th century, and it continued to evolve over the succeeding centuries.  [QUESTION]: Which culture was the Normans descended from? <|endoftext|>\n",
      "<|startoftext|>\n",
      "[CONTEXT]: The Normans (Norman: Nourmands; French: Normands; Latin: Normanni) were the people who in the 10th and 11th centuries gave their name to Normandy, a region in France. They were descended from Norse (\"Norman\" comes from \"Norseman\") raiders and pirates from Denmark, Iceland and Norway who, under their leader Rollo, agreed to swear fealty to King Charles III of West Francia. Through generations of assimilation and mixing with the native Frankish and Roman-Gaulish populations, their descendants would gradually merge with the Carolingian-based cultures of West Francia. The distinct cultural and ethnic identity of the Normans emerged initially in the first half of the 10th century, and it continued to evolve over the succeeding centuries.  [QUESTION]: When the Normans were from the Norse, what was their name? <|endoftext|>\n",
      "<|startoftext|>\n",
      "[CONTEXT]: The Normans (Norman: Nourmands; French: Normanni) were the people who in the 10th and 11th centuries gave their name to Normandy, a region in France. They were descended from Norse (\"Norseman\") raiders and pirates from Denmark, Iceland and Norway who, under their leader Rollo, agreed to swear fealty to King Charles III of West Francia. Through generations of assimilation and mixing with the native Frankish and Roman-Gaulish populations, their descendants would gradually merge with the Carolingian-based cultures of West Francia. The distinct cultural and ethnic identity of the Normans emerged initially in the first half of the 10th century, and it continued to evolve over the succeeding centuries.  [QUESTION]: What was the name of the name of the Normans? <|endoftext|>\n",
      "<|startoftext|>\n",
      "[CONTEXT]: The Normans (Norman: Nourmands; French: Normanni) were the people who in the 10th and 11th\n",
      "====================\n",
      "<|startoftext|>\n",
      "[CONTEXT]: The Normans (Norman: Nourmands; French: Normands; Latin: Normanni) were the people who in the 10th and 11th centuries gave their name to Normandy, a region in France. They were descended from Norse (\"Norman\" comes from \"Norseman\") raiders and pirates from Denmark, Iceland and Norway who, under their leader Rollo, agreed to swear fealty to King Charles III of West Francia. Through generations of assimilation and mixing with the native Frankish and Roman-Gaulish populations, their descendants would gradually merge with the Carolingian-based cultures of West Francia. The distinct cultural and ethnic identity of the Normans emerged initially in the first half of the 10th century, and it continued to evolve over the succeeding centuries.  [QUESTION]: What did the Normans give their name to? <|endoftext|>\n",
      "<|startoftext|>\n",
      "[CONTEXT]: The Normans (Norman: Nourmands; French: Normands; Latin: Normanni) were the people who in the 10th and 11th centuries gave their name to Normandy, a region in France. They were descended from Norse (\"Norman\" comes from \"Norseman\") raiders and pirates from Denmark, Iceland and Norway who, under their leader Rollo, agreed to swear fealty to King Charles III of West Francia. Through generations of assimilation and mixing with the native Frankish and Roman-Gaulish populations, their descendants would gradually merge with the Carolingian-based cultures of West Francia. The distinct cultural and ethnic identity of the Normans emerged initially in the first half of the 10th century, and it continued to evolve over the succeeding centuries.  [QUESTION]: Who was the leader of Rollo? <|endoftext|>\n",
      "<|startoftext|>\n",
      "[CONTEXT]: The Normans (Norman: Nourmands; French: Normands; Latin: Normanni) were the people who in the 10th and 11th centuries gave their name to Normandy, a region in France. They were descended from Norse (\"Norman\" comes from \"Norseman\") raiders and pirates from Denmark, Iceland and Norway who, under their leader Rollo, agreed to swear fealty to King Charles III of West Francia. Through generations of assimilation and mixing with the native Frankish and Roman-Gaulish populations, their descendants would gradually merge with the Carolingian-based cultures of West Francia. The distinct cultural and ethnic identity of the Normans emerged initially in the first half of the 10th century, and it continued to evolve over the succeeding centuries.  [QUESTION]: What was the Normans' origins? <|endoftext|>\n",
      "<|startoftext|>\n",
      "[CONTEXT]: The Normans (Norman: Nourmands; French: Normands; Latin: Normanni) were the people who in the 10th and 11th centuries gave their name to Normandy, a region in France. They were descended from Norse (\"Norman\" comes from \"Norseman\") raiders and pirates from Denmark, Iceland and Norway who, under their leader Rollo, agreed to swear fealty to King Charles III of West Francia. Through generations of assimilation and mixing with the native Frankish and Roman-Gaulish populations, their descendants would gradually merge with the Carolingian-based cultures of West Francia. The distinct cultural and ethnic identity of the Normans emerged initially in the first half of the 10th century, and it continued to evolve over the succeeding centuries.  [QUESTION]: What is the Normans' fyeinglange? <|endoftext|>\n",
      "<|startoftext|>\n",
      "[CONTEXT]: The Normans (Norman: Nourmands; French: Normanni) were the people who in the 10th and 11th centuries gave their name to Normandy, a region in France. They were descended from Norse (\"Norman\" comes from \"Norseman\") raiders and pirates from Denmark, Iceland and Norway who, under their leader Rollo, agreed to swear fealty to King Charles III of West Francia. Through generations of assimilation and mixing with the native Frankish and Roman-Gaulish populations, their descendants would gradually merge with the Carolingian-based cultures of West Francia. The distinct cultural and ethnic identity of the Normans emerged initially in the first half of the 10th century, and it continued to evolve over the succeeding centuries.  [QUESTION]: When was the Normans' fyeinglange? <|endoftext|>\n",
      "<|startoftext|>\n",
      "[CONTEXT]: The Normans (Norman: Nourmands; French: Normanni) were the people who in the 10th and\n",
      "====================\n",
      "<|startoftext|>\n",
      "[CONTEXT]: The Normans (Norman: Nourmands; French: Normands; Latin: Normanni) were the people who in the 10th and 11th centuries gave their name to Normandy, a region in France. They were descended from Norse (\"Norman\" comes from \"Norseman\") raiders and pirates from Denmark, Iceland and Norway who, under their leader Rollo, agreed to swear fealty to King Charles III of West Francia. Through generations of assimilation and mixing with the native Frankish and Roman-Gaulish populations, their descendants would gradually merge with the Carolingian-based cultures of West Francia. The distinct cultural and ethnic identity of the Normans emerged initially in the first half of the 10th century, and it continued to evolve over the succeeding centuries.  [QUESTION]: Which group of people gave their name to Normandy? <|endoftext|>\n",
      "<|startoftext|>\n",
      "[CONTEXT]: The Normans (Norman: Nourmands; French: Normands; Latin: Normanni) were the people who in the 10th and 11th centuries gave their name to Normandy, a region in France. They were descended from Norse (\"Norman\" comes from \"Norseman\") raiders and pirates from Denmark, Iceland and Norway who, under their leader Rollo, agreed to swear fealty to King Charles III of West Francia. Through generations of assimilation and mixing with the native Frankish and Roman-Gaulish populations, their descendants would gradually merge with the Carolingian-based cultures of West Francia. The distinct cultural and ethnic identity of the Normans emerged initially in the first half of the 10th century, and it continued to evolve over the succeeding centuries.  [QUESTION]: Who is the leader from Denmark? <|endoftext|>\n",
      "<|startoftext|>\n",
      "[CONTEXT]: The Normans (Norman: Nourmands; French: Normands; Latin: Normanni) were the people who in the 10th and 11th centuries gave their name to Normandy, a region in France. They were descended from Norse (\"Norman\" comes from \"Norseman\") raiders and pirates from Denmark, Iceland and Norway who, under their leader Rollo, agreed to swear fealty to King Charles III of West Francia. Through generations of assimilation and mixing with the native Frankish and Roman-Gaulish populations, their descendants would gradually merge with the Carolingian-based cultures of West Francia. The distinct cultural and ethnic identity of the Normans emerged initially in the first half of the 10th century, and it continued to evolve over the succeeding centuries.  [QUESTION]: What language is Normans' language in? <|endoftext|>\n",
      "<|startoftext|>\n",
      "[CONTEXT]: The Normans (Norman: Nourmands; French: Normants; Latin: Normanni) were the people who in the 10th and 11th centuries gave their name to Normandy, a region in France. They were descended from Norse (\"Norman\" comes from \"Norseman\") raiders and pirates from Denmark, Iceland and Norway who, under their leader Rollo, agreed to swear fealty to King Charles III of West Francia. Through generations of assimilation and mixing with the native Frankish and Roman-Gaulish populations, their descendants would gradually merge with the Carolingian-based cultures of West Francia. The distinct cultural and ethnic identity of the Normans emerged initially in the first half of the 10th century, and it continued to evolve over the succeeding centuries.  [QUESTION]: Who would the Normans' descendants be? <|endoftext|>\n",
      "<|startoftext|>\n",
      "[CONTEXT]: The Normans (Norman: Nourmands; French: Normants; Latin: Normanni) were the people who in the 10th and 11th centuries gave their name to Normandy, a region in France. They were descended from Norse (\"Norman\" comes from \"Norseman\") raiders and pirates from Denmark, Iceland and Norway who, under their leader Rollo, agreed to swear fealty to King Charles III of West Francia. Through generations of assimilation and mixing with the native Frankish and Roman-Gaulish populations, their descendants would gradually merge with the Carolingian-based cultures of West Francia. The distinct cultural and ethnic identity of the Normans emerged initially in the first half of the 10th century, and it continued to evolve over the succeeding centuries.  [QUESTION]: Which language is the Normans' language in? <|endoftext|>\n",
      "<|startoftext|>\n",
      "[CONTEXT]: The Normans (Norman: Nourmands; French: Normants; Latin: Normanni) were the people who\n",
      "====================\n",
      "<|startoftext|>\n",
      "[CONTEXT]: The Normans (Norman: Nourmands; French: Normands; Latin: Normanni) were the people who in the 10th and 11th centuries gave their name to Normandy, a region in France. They were descended from Norse (\"Norman\" comes from \"Norseman\") raiders and pirates from Denmark, Iceland and Norway who, under their leader Rollo, agreed to swear fealty to King Charles III of West Francia. Through generations of assimilation and mixing with the native Frankish and Roman-Gaulish populations, their descendants would gradually merge with the Carolingian-based cultures of West Francia. The distinct cultural and ethnic identity of the Normans emerged initially in the first half of the 10th century, and it continued to evolve over the succeeding centuries.  [QUESTION]: What did the Normans their names of Normandie and Normandie derive from? <|endoftext|>\n",
      "<|startoftext|>\n",
      "[CONTEXT]: The Normans (Norman: Nourmands; French: Normands; Latin: Normanni) were the people who in the 10th and 11th centuries gave their name to Normandy, a region in France. They were descended from Norse (\"Norman\" comes from \"Norseman\") raiders and pirates from Denmark, Iceland and Norway who, under their leader Rollo, agreed to swear fealty to King Charles III of West Francia. Through generations of assimilation and mixing with the native Frankish and Roman-Gaulish populations, their descendants would gradually merge with the Carolingian-based cultures of West Francia. The distinct cultural and ethnic identity of the Normans emerged initially in the first half of the 10th century, and it continued to evolve over the succeeding centuries.  [QUESTION]: What was the leader of the Normans? <|endoftext|>\n",
      "<|startoftext|>\n",
      "[CONTEXT]: The Normans (Norman: Nourmands; French: Normands; Latin: Normanni) were the people who in the 10th and 11th centuries gave their name to Normandy, a region in France. They were descended from Norse (\"Norman\" comes from \"Norseman\") raiders and pirates from Denmark, Iceland and Norway who, under their leader Rollo, agreed to swear fealty to King Charles III of West Francia. Through generations of assimilation and mixing with the native Frankish and Roman-Gaulish populations, their descendants would gradually merge with the Carolingian-based cultures of West Francia. The distinct cultural and ethnic identity of the Normans emerged initially in the first half of the 10th century, and it continued to evolve over the succeeding centuries.  [QUESTION]: What were the Normans called by the Normans? <|endoftext|>\n",
      "<|startoftext|>\n",
      "[CONTEXT]: The Normans (Norman: Nourmands; French: Normands; Latin: Normanni) were the people who in the 10th and 11th centuries gave their name to Normandy, a region in France. They were descended from Norse (\"Norman\" comes from \"Norseman\") raiders and pirates from Denmark, Iceland and Norway who, under their leader Rollo, agreed to swear fealty to King Charles III of West Francia. Through generations of assimilation and mixing with the native Frankish and Roman-Gaulish populations, their descendants would gradually merge with the Carolingian-based cultures of West Francia. The distinct cultural and ethnic identity of the Normans emerged initially in the first half of the 10th century, and it continued to evolve over the succeeding centuries.  [QUESTION]: Where did the Normans from Normandie come from? <|endoftext|>\n",
      "<|startoftext|>\n",
      "[CONTEXT]: The Normans (Norman: Nourmands; French: Normandies; Latin: Normanni) were the people who in the 10th and 11th centuries gave their name to Normandy, a region in France. They were descended from Norse (\"Norman\" comes from \"Norseman\") raiders and pirates from Denmark, Iceland and Norway who, under their leader Rollo, agreed to swear fealty to King Charles III of West Francia. Through generations of assimilation and mixing with the native Frankish and Roman-Gaulish populations, their descendants would gradually merge with the Carolingian-based cultures of West Francia. The distinct cultural and ethnic identity of the Normans emerged initially in the first half of the 10th century, and it continued to evolve over the succeeding centuries.  [QUESTION]: What was the name of the Normans before they came to the present state? <|endoftext|>\n",
      "<|startoftext|>\n",
      "[CONTEXT]: The Normans (\n",
      "====================\n",
      "<|startoftext|>\n",
      "[CONTEXT]: The Normans (Norman: Nourmands; French: Normands; Latin: Normanni) were the people who in the 10th and 11th centuries gave their name to Normandy, a region in France. They were descended from Norse (\"Norman\" comes from \"Norseman\") raiders and pirates from Denmark, Iceland and Norway who, under their leader Rollo, agreed to swear fealty to King Charles III of West Francia. Through generations of assimilation and mixing with the native Frankish and Roman-Gaulish populations, their descendants would gradually merge with the Carolingian-based cultures of West Francia. The distinct cultural and ethnic identity of the Normans emerged initially in the first half of the 10th century, and it continued to evolve over the succeeding centuries.  [QUESTION]: What was the name of the Normans? <|endoftext|>\n",
      "<|startoftext|>\n",
      "[CONTEXT]: The Normans (Norman: Nourmands; French: Normands; Latin: Normanni) were the people who in the 10th and 11th centuries gave their name to Normandy, a region in France. They were descended from Norse (\"Norman\" comes from \"Norseman\") raiders and pirates from Denmark, Iceland and Norway who, under their leader Rollo, agreed to swear fealty to King Charles III of West Francia. Through generations of assimilation and mixing with the native Frankish and Roman-Gaulish populations, their descendants would gradually merge with the Carolingian-based cultures of West Francia. The distinct cultural and ethnic identity of the Normans emerged initially in the first half of the 10th century, and it continued to evolve over the succeeding centuries.  [QUESTION]: Who was Rollo? <|endoftext|>\n",
      "<|startoftext|>\n",
      "[CONTEXT]: The Normans (Norman: Nourmands; French: Normands; Latin: Normanni) were the people who in the 10th and 11th centuries gave their name to Normandy, a region in France. They were descended from Norse (\"Norman\" comes from \"Norseman\") raiders and pirates from Denmark, Iceland and Norway who, under their leader Rollo, agreed to swear fealty to King Charles III of West Francia. Through generations of assimilation and mixing with the native Frankish and Roman-Gaulish populations, their descendants would gradually merge with the Carolingian-based cultures of West Francia. The distinct cultural and ethnic identity of the Normans emerged initially in the first half of the 10th century, and it continued to evolve over the succeeding centuries.  [QUESTION]: What language was the Normans from? <|endoftext|>\n",
      "<|startoftext|>\n",
      "[CONTEXT]: The Normans (Norman: Nourmands; French: Normands; Latin: Normanni) were the people who in the 10th and 11th centuries gave their name to Normandy, a region in France. They were descended from Norse (\"Norman\" comes from \"Norseman\") raiders and pirates from Denmark, Iceland and Norway who, under their leader Rollo, agreed to swear fealty to King Charles III of West Francia. Through generations of assimilation and mixing with the native Frankish and Roman-Gaulish populations, their descendants would gradually merge with the Carolingian-based cultures of West Francia. The distinct cultural and ethnic identity of the Normans emerged initially in the first half of the 10th century, and it continued to evolve over the succeeding centuries.  [QUESTION]: What language was the Normans from? <|endoftext|>\n",
      "<|startoftext|>\n",
      "[CONTEXT]: The Normans (Norman: Nourmands; French: Normanni) were the people who in the 10th and 11th centuries gave their name to Normandy, a region in France. They were descended from Norse (\"Norman\" comes from \"Norseman\") raiders and pirates from Denmark, Iceland and Norway who, under their leader Rollo, agreed to swear fealty to King Charles III of West Francia. Through generations of assimilation and mixing with the native Frankish and Roman-Gaulish populations, their descendants would gradually merge with the Carolingian-based cultures of West Francia. The distinct cultural and ethnic identity of the Normans emerged initially in the first half of the 10th century, and it continued to evolve over the succeeding centuries.  [QUESTION]: What was the Normans' culture? <|endoftext|>\n",
      "<|startoftext|>\n",
      "[CONTEXT]: The Normans (Norman: Nourmands; French: Normanni) were the people who in the 10th and 11th centuries gave their name to Normandy, a region in\n",
      "====================\n",
      "<|startoftext|>\n",
      "[CONTEXT]: The Normans (Norman: Nourmands; French: Normands; Latin: Normanni) were the people who in the 10th and 11th centuries gave their name to Normandy, a region in France. They were descended from Norse (\"Norman\" comes from \"Norseman\") raiders and pirates from Denmark, Iceland and Norway who, under their leader Rollo, agreed to swear fealty to King Charles III of West Francia. Through generations of assimilation and mixing with the native Frankish and Roman-Gaulish populations, their descendants would gradually merge with the Carolingian-based cultures of West Francia. The distinct cultural and ethnic identity of the Normans emerged initially in the first half of the 10th century, and it continued to evolve over the succeeding centuries.  [QUESTION]: What language did the Normans speak? <|endoftext|>\n",
      "<|startoftext|>\n",
      "[CONTEXT]: The Normans (Norman: Nourmands; French: Normands; Latin: Normanni) were the people who in the 10th and 11th centuries gave their name to Normandy, a region in France. They were descended from Norse (\"Norman\" comes from \"Norseman\") raiders and pirates from Denmark, Iceland and Norway who, under their leader Rollo, agreed to swear fealty to King Charles III of West Francia. Through generations of assimilation and mixing with the native Frankish and Roman-Gaulish populations, their descendants would gradually merge with the Carolingian-based cultures of West Francia. The distinct cultural and ethnic identity of the Normans emerged initially in the first half of the 10th century, and it continued to evolve over the succeeding centuries.  [QUESTION]: What language was the Normans descended from? <|endoftext|>\n",
      "<|startoftext|>\n",
      "[CONTEXT]: The Normans (Norman: Nourmands; French: Normands; Latin: Normanni) were the people who in the 10th and 11th centuries gave their name to Normandy, a region in France. They were descended from Norse (\"Norman\" comes from \"Norseman\") raiders and pirates from Denmark, Iceland and Norway who, under their leader Rollo, agreed to swear fealty to King Charles III of West Francia. Through generations of assimilation and mixing with the native Frankish and Roman-Gaulish populations, their descendants would gradually merge with the Carolingian-based cultures of West Francia. The distinct cultural and ethnic identity of the Normans emerged initially in the first half of the 10th century, and it continued to evolve over the succeeding centuries.  [QUESTION]: What was the name of Rollo's brother? <|endoftext|>\n",
      "<|startoftext|>\n",
      "[CONTEXT]: The Normans (Norman: Nourmands; French: Normands; Latin: Normanni) were the people who in the 10th and 11th centuries gave their name to Normandy, a region in France. They were descended from Norse (\"Norman\" comes from \"Norseman\") raiders and pirates from Denmark, Iceland and Norway who, under their leader Rollo, agreed to swear fealty to King Charles III of West Francia. Through generations of assimilation and mixing with the native Frankish and Roman-Gaulish populations, their descendants would gradually merge with the Carolingian-based cultures of West Francia. The distinct cultural and ethnic identity of the Normans emerged initially in the first half of the 10th century, and it continued to evolve over the succeeding centuries.  [QUESTION]: What linguistics was applied to the Normans? <|endoftext|>\n",
      "<|startoftext|>\n",
      "[CONTEXT]: The Normans (Norman: Nourmands; French: Normanni) were the people who in the 10th and 11th centuries gave their name to Normandy, a region in France. They were descended from Norse (\"Norman\" comes from \"Norseman\") raiders and pirates from Denmark, Iceland and Norway who, under their leader Rollo, agreed to swear fealty to King Charles III of West Francia. Through generations of assimilation and mixing with the native Frankish and Roman-Gaulish populations, their descendants would gradually merge with the Carolingian-based cultures of West Francia. The distinct cultural and ethnic identity of the Normans emerged initially in the first half of the 10th century, and it continued to evolve over the succeeding centuries.  [QUESTION]: Where did the Normans come from? <|endoftext|>\n",
      "<|startoftext|>\n",
      "[CONTEXT]: The Normans (Norman: Nourmands; French: Normanni) were the people who in the 10th and 11th centuries gave their\n",
      "====================\n"
     ]
    }
   ],
   "source": [
    "gpt2.generate(sess,\n",
    "              temperature=0.7,\n",
    "              prefix=pre,\n",
    "              nsamples=10,\n",
    "              batch_size=10,\n",
    "              run_name=\"QGen_SQUAD_test\",\n",
    "              )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### QuAC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"345M\"\n",
    "if not os.path.isdir(os.path.join(\"models\", model_name)):\n",
    "    print(f\"Downloading {model_name} model...\")\n",
    "    gpt2.download_gpt2(model_name=model_name)   # model is saved into current directory under /models/<size>/\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "qsess = gpt2.start_tf_sess()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/gpt_2_simple/src/sample.py:17: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "WARNING:tensorflow:From /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/gpt_2_simple/src/memory_saving_gradients.py:62: get_backward_walk_ops (from tensorflow.contrib.graph_editor.select) is deprecated and will be removed after 2019-06-06.\n",
      "Instructions for updating:\n",
      "Please use tensorflow.python.ops.op_selector.get_backward_walk_ops.\n",
      "Loading checkpoint models/345M/model.ckpt\n",
      "INFO:tensorflow:Restoring parameters from models/345M/model.ckpt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [03:56<00:00, 236.68s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset has 46016379 tokens\n",
      "Training...\n",
      "[1 | 22.46] loss=1.58 avg=1.58\n",
      "[2 | 32.63] loss=1.71 avg=1.65\n",
      "[3 | 43.03] loss=1.58 avg=1.63\n",
      "[4 | 53.19] loss=1.34 avg=1.55\n",
      "[5 | 63.32] loss=1.75 avg=1.59\n",
      "[6 | 73.38] loss=2.57 avg=1.76\n",
      "[7 | 83.41] loss=1.17 avg=1.67\n",
      "[8 | 93.47] loss=1.53 avg=1.65\n",
      "[9 | 103.46] loss=1.81 avg=1.67\n",
      "[10 | 113.49] loss=1.83 avg=1.69\n",
      "[11 | 123.59] loss=1.18 avg=1.64\n",
      "[12 | 133.72] loss=2.77 avg=1.74\n",
      "[13 | 143.79] loss=1.02 avg=1.68\n",
      "[14 | 153.88] loss=1.45 avg=1.66\n",
      "[15 | 163.96] loss=1.92 avg=1.68\n",
      "[16 | 174.15] loss=2.63 avg=1.75\n",
      "[17 | 184.27] loss=1.06 avg=1.70\n",
      "[18 | 194.36] loss=2.36 avg=1.74\n",
      "[19 | 204.43] loss=1.74 avg=1.74\n",
      "[20 | 214.54] loss=2.08 avg=1.76\n",
      "[21 | 225.03] loss=1.02 avg=1.72\n",
      "[22 | 235.13] loss=1.33 avg=1.70\n",
      "[23 | 245.28] loss=1.18 avg=1.68\n",
      "[24 | 255.32] loss=1.65 avg=1.68\n",
      "[25 | 265.42] loss=2.41 avg=1.71\n",
      "[26 | 275.45] loss=2.47 avg=1.74\n",
      "[27 | 285.43] loss=1.43 avg=1.73\n",
      "[28 | 295.48] loss=1.81 avg=1.73\n",
      "[29 | 305.54] loss=1.21 avg=1.71\n",
      "[30 | 315.59] loss=1.28 avg=1.70\n",
      "[31 | 325.64] loss=2.16 avg=1.71\n",
      "[32 | 335.65] loss=1.23 avg=1.69\n",
      "[33 | 345.58] loss=2.01 avg=1.71\n",
      "[34 | 355.63] loss=1.99 avg=1.72\n",
      "[35 | 365.71] loss=1.20 avg=1.70\n",
      "[36 | 375.75] loss=2.50 avg=1.72\n",
      "[37 | 385.80] loss=2.20 avg=1.74\n",
      "[38 | 395.83] loss=2.17 avg=1.75\n",
      "[39 | 406.19] loss=2.34 avg=1.77\n",
      "[40 | 416.20] loss=1.31 avg=1.76\n",
      "[41 | 426.22] loss=3.07 avg=1.80\n",
      "[42 | 436.31] loss=2.03 avg=1.80\n",
      "[43 | 446.36] loss=2.93 avg=1.84\n",
      "[44 | 456.36] loss=1.35 avg=1.82\n",
      "[45 | 466.34] loss=1.50 avg=1.81\n",
      "[46 | 476.39] loss=1.33 avg=1.80\n",
      "[47 | 486.44] loss=1.80 avg=1.80\n",
      "[48 | 496.48] loss=2.90 avg=1.83\n",
      "[49 | 506.52] loss=1.55 avg=1.82\n",
      "[50 | 516.58] loss=1.69 avg=1.82\n",
      "[51 | 526.63] loss=2.38 avg=1.83\n",
      "[52 | 536.63] loss=1.70 avg=1.83\n",
      "[53 | 546.58] loss=2.80 avg=1.85\n",
      "[54 | 556.58] loss=1.44 avg=1.84\n",
      "[55 | 566.70] loss=2.46 avg=1.86\n",
      "[56 | 576.88] loss=2.42 avg=1.87\n",
      "[57 | 587.09] loss=2.99 avg=1.90\n",
      "[58 | 597.06] loss=1.17 avg=1.88\n",
      "[59 | 607.08] loss=1.88 avg=1.88\n",
      "[60 | 617.02] loss=2.73 avg=1.90\n",
      "[61 | 627.01] loss=2.39 avg=1.91\n",
      "[62 | 637.03] loss=2.22 avg=1.92\n",
      "[63 | 647.03] loss=1.41 avg=1.91\n",
      "[64 | 657.02] loss=1.33 avg=1.89\n",
      "[65 | 667.09] loss=1.55 avg=1.89\n",
      "[66 | 677.13] loss=2.10 avg=1.89\n",
      "[67 | 687.12] loss=2.17 avg=1.90\n",
      "[68 | 697.13] loss=1.39 avg=1.89\n",
      "[69 | 707.14] loss=2.28 avg=1.89\n",
      "[70 | 717.13] loss=2.39 avg=1.90\n",
      "[71 | 727.31] loss=1.84 avg=1.90\n",
      "[72 | 737.33] loss=1.41 avg=1.89\n",
      "[73 | 747.38] loss=2.52 avg=1.90\n",
      "[74 | 757.57] loss=0.98 avg=1.89\n",
      "[75 | 767.76] loss=2.23 avg=1.89\n",
      "[76 | 777.76] loss=1.32 avg=1.88\n",
      "[77 | 787.86] loss=1.28 avg=1.87\n",
      "[78 | 797.88] loss=1.61 avg=1.87\n",
      "[79 | 807.90] loss=3.06 avg=1.89\n",
      "[80 | 817.92] loss=1.59 avg=1.88\n",
      "[81 | 828.01] loss=2.51 avg=1.89\n",
      "[82 | 838.08] loss=1.37 avg=1.89\n",
      "[83 | 848.09] loss=2.06 avg=1.89\n",
      "[84 | 858.26] loss=1.62 avg=1.88\n",
      "[85 | 868.33] loss=1.38 avg=1.88\n",
      "[86 | 878.47] loss=1.48 avg=1.87\n",
      "[87 | 888.54] loss=1.31 avg=1.86\n",
      "[88 | 898.58] loss=1.66 avg=1.86\n",
      "[89 | 908.65] loss=1.41 avg=1.85\n",
      "[90 | 918.73] loss=1.20 avg=1.84\n",
      "[91 | 928.73] loss=1.68 avg=1.83\n",
      "[92 | 939.09] loss=1.52 avg=1.83\n",
      "[93 | 949.22] loss=1.92 avg=1.83\n",
      "[94 | 959.27] loss=1.44 avg=1.82\n",
      "[95 | 969.33] loss=2.11 avg=1.83\n",
      "[96 | 979.37] loss=1.07 avg=1.82\n",
      "[97 | 989.41] loss=0.84 avg=1.80\n",
      "[98 | 999.39] loss=2.08 avg=1.81\n",
      "[99 | 1009.38] loss=2.69 avg=1.82\n",
      "[100 | 1019.42] loss=2.36 avg=1.83\n",
      "Saving checkpoint/QGen_QUAC_test/model-100\n",
      "[101 | 1033.04] loss=2.04 avg=1.83\n",
      "[102 | 1042.99] loss=1.87 avg=1.83\n",
      "[103 | 1052.98] loss=1.67 avg=1.83\n",
      "[104 | 1063.02] loss=2.18 avg=1.83\n",
      "[105 | 1073.08] loss=2.22 avg=1.84\n",
      "[106 | 1083.08] loss=1.87 avg=1.84\n",
      "[107 | 1093.02] loss=1.19 avg=1.83\n",
      "[108 | 1103.01] loss=2.71 avg=1.84\n",
      "[109 | 1113.04] loss=3.28 avg=1.87\n",
      "[110 | 1123.31] loss=2.76 avg=1.88\n",
      "[111 | 1133.36] loss=1.28 avg=1.87\n",
      "[112 | 1143.38] loss=1.96 avg=1.87\n",
      "[113 | 1153.39] loss=1.27 avg=1.86\n",
      "[114 | 1163.43] loss=2.13 avg=1.87\n",
      "[115 | 1173.44] loss=2.08 avg=1.87\n",
      "[116 | 1183.44] loss=1.79 avg=1.87\n",
      "[117 | 1193.41] loss=1.53 avg=1.86\n",
      "[118 | 1203.43] loss=1.30 avg=1.86\n",
      "[119 | 1213.37] loss=1.19 avg=1.85\n",
      "[120 | 1223.41] loss=2.48 avg=1.86\n",
      "[121 | 1233.44] loss=1.68 avg=1.85\n",
      "[122 | 1243.44] loss=1.49 avg=1.85\n",
      "[123 | 1253.45] loss=1.41 avg=1.84\n",
      "[124 | 1263.42] loss=1.53 avg=1.84\n",
      "[125 | 1273.45] loss=2.55 avg=1.85\n",
      "[126 | 1283.45] loss=1.60 avg=1.84\n",
      "[127 | 1293.47] loss=0.95 avg=1.83\n",
      "[128 | 1303.83] loss=1.05 avg=1.82\n",
      "[129 | 1313.89] loss=2.78 avg=1.83\n",
      "[130 | 1323.97] loss=1.55 avg=1.83\n",
      "[131 | 1334.01] loss=1.31 avg=1.82\n",
      "[132 | 1344.08] loss=2.92 avg=1.84\n",
      "[133 | 1354.14] loss=2.18 avg=1.84\n",
      "[134 | 1364.13] loss=3.16 avg=1.86\n",
      "[135 | 1374.07] loss=2.08 avg=1.86\n",
      "[136 | 1384.05] loss=1.91 avg=1.86\n",
      "[137 | 1394.04] loss=1.85 avg=1.86\n",
      "[138 | 1404.06] loss=3.42 avg=1.88\n",
      "[139 | 1414.03] loss=1.70 avg=1.88\n",
      "[140 | 1424.05] loss=1.52 avg=1.88\n",
      "[141 | 1434.05] loss=1.27 avg=1.87\n",
      "[142 | 1444.06] loss=1.27 avg=1.86\n",
      "[143 | 1454.02] loss=1.35 avg=1.85\n",
      "[144 | 1463.99] loss=1.15 avg=1.85\n",
      "[145 | 1474.02] loss=2.52 avg=1.85\n",
      "[146 | 1484.31] loss=1.45 avg=1.85\n",
      "[147 | 1494.31] loss=3.05 avg=1.86\n",
      "[148 | 1504.33] loss=1.58 avg=1.86\n",
      "[149 | 1514.29] loss=2.20 avg=1.87\n",
      "[150 | 1524.28] loss=2.53 avg=1.87\n",
      "[151 | 1534.28] loss=1.76 avg=1.87\n",
      "[152 | 1544.24] loss=1.77 avg=1.87\n",
      "[153 | 1554.22] loss=0.82 avg=1.86\n",
      "[154 | 1564.19] loss=1.86 avg=1.86\n",
      "[155 | 1574.20] loss=1.68 avg=1.86\n",
      "[156 | 1584.18] loss=1.42 avg=1.85\n",
      "[157 | 1594.21] loss=1.85 avg=1.85\n",
      "[158 | 1604.22] loss=2.31 avg=1.86\n",
      "[159 | 1614.26] loss=2.88 avg=1.87\n",
      "[160 | 1624.20] loss=1.90 avg=1.87\n",
      "[161 | 1634.17] loss=1.89 avg=1.87\n",
      "[162 | 1644.14] loss=2.13 avg=1.87\n",
      "[163 | 1654.11] loss=3.07 avg=1.89\n",
      "[164 | 1664.40] loss=2.26 avg=1.89\n",
      "[165 | 1674.37] loss=2.36 avg=1.90\n",
      "[166 | 1684.33] loss=1.38 avg=1.89\n",
      "[167 | 1694.33] loss=1.80 avg=1.89\n",
      "[168 | 1704.33] loss=1.29 avg=1.88\n",
      "[169 | 1714.36] loss=1.34 avg=1.88\n",
      "[170 | 1724.32] loss=1.09 avg=1.87\n",
      "[171 | 1734.29] loss=1.54 avg=1.86\n",
      "[172 | 1744.34] loss=3.21 avg=1.88\n",
      "[173 | 1754.35] loss=1.55 avg=1.87\n",
      "[174 | 1764.38] loss=1.70 avg=1.87\n",
      "[175 | 1774.39] loss=2.78 avg=1.88\n",
      "[176 | 1784.37] loss=2.43 avg=1.89\n",
      "[177 | 1794.38] loss=2.69 avg=1.90\n",
      "[178 | 1804.38] loss=1.23 avg=1.89\n",
      "[179 | 1814.35] loss=1.55 avg=1.89\n",
      "[180 | 1824.37] loss=1.15 avg=1.88\n",
      "[181 | 1834.39] loss=1.36 avg=1.87\n",
      "[182 | 1844.69] loss=1.11 avg=1.86\n",
      "[183 | 1854.77] loss=1.30 avg=1.86\n",
      "[184 | 1864.76] loss=2.29 avg=1.86\n",
      "[185 | 1874.71] loss=1.77 avg=1.86\n",
      "[186 | 1884.70] loss=2.56 avg=1.87\n",
      "[187 | 1894.71] loss=1.49 avg=1.86\n",
      "[188 | 1904.69] loss=1.75 avg=1.86\n",
      "[189 | 1914.68] loss=1.09 avg=1.85\n",
      "[190 | 1924.69] loss=3.37 avg=1.87\n",
      "[191 | 1934.72] loss=1.44 avg=1.87\n",
      "[192 | 1944.67] loss=1.36 avg=1.86\n",
      "[193 | 1954.67] loss=1.25 avg=1.85\n",
      "[194 | 1964.67] loss=3.09 avg=1.87\n",
      "[195 | 1974.65] loss=1.41 avg=1.86\n",
      "[196 | 1984.62] loss=3.16 avg=1.88\n",
      "[197 | 1994.67] loss=1.26 avg=1.87\n",
      "[198 | 2004.64] loss=2.28 avg=1.88\n",
      "[199 | 2014.63] loss=1.70 avg=1.87\n",
      "[200 | 2024.96] loss=1.71 avg=1.87\n",
      "Saving checkpoint/QGen_QUAC_test/model-200\n",
      "WARNING:tensorflow:From /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/tensorflow_core/python/training/saver.py:963: remove_checkpoint (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use standard file APIs to delete files with this prefix.\n",
      "[201 | 2037.99] loss=1.41 avg=1.87\n",
      "[202 | 2048.02] loss=2.14 avg=1.87\n",
      "[203 | 2058.13] loss=2.75 avg=1.88\n",
      "[204 | 2068.31] loss=2.13 avg=1.88\n",
      "[205 | 2078.34] loss=2.54 avg=1.89\n",
      "[206 | 2088.39] loss=1.67 avg=1.89\n",
      "[207 | 2098.47] loss=1.51 avg=1.88\n",
      "[208 | 2108.52] loss=2.16 avg=1.89\n",
      "[209 | 2118.58] loss=1.74 avg=1.88\n",
      "[210 | 2128.69] loss=1.58 avg=1.88\n",
      "[211 | 2138.71] loss=1.53 avg=1.88\n",
      "[212 | 2148.75] loss=2.08 avg=1.88\n",
      "[213 | 2158.80] loss=1.43 avg=1.87\n",
      "[214 | 2168.90] loss=2.74 avg=1.88\n",
      "[215 | 2178.97] loss=1.50 avg=1.88\n",
      "[216 | 2189.04] loss=1.48 avg=1.88\n",
      "[217 | 2199.34] loss=1.34 avg=1.87\n",
      "[218 | 2209.44] loss=2.04 avg=1.87\n",
      "[219 | 2219.46] loss=1.44 avg=1.87\n",
      "[220 | 2229.56] loss=1.77 avg=1.87\n",
      "[221 | 2239.58] loss=2.56 avg=1.87\n",
      "[222 | 2249.60] loss=1.35 avg=1.87\n",
      "[223 | 2259.63] loss=1.13 avg=1.86\n",
      "[224 | 2269.66] loss=1.79 avg=1.86\n",
      "[225 | 2279.68] loss=2.25 avg=1.86\n",
      "[226 | 2289.75] loss=1.47 avg=1.86\n",
      "[227 | 2299.81] loss=1.32 avg=1.85\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[228 | 2309.80] loss=3.15 avg=1.87\n",
      "[229 | 2319.82] loss=1.92 avg=1.87\n",
      "[230 | 2329.83] loss=1.37 avg=1.86\n",
      "[231 | 2339.92] loss=2.16 avg=1.87\n",
      "[232 | 2349.95] loss=1.19 avg=1.86\n",
      "[233 | 2359.99] loss=2.59 avg=1.87\n",
      "[234 | 2370.00] loss=2.04 avg=1.87\n",
      "[235 | 2380.27] loss=1.57 avg=1.86\n",
      "[236 | 2390.33] loss=1.09 avg=1.86\n",
      "[237 | 2400.40] loss=2.76 avg=1.87\n",
      "[238 | 2410.45] loss=2.10 avg=1.87\n",
      "[239 | 2420.46] loss=2.65 avg=1.88\n",
      "[240 | 2430.51] loss=1.67 avg=1.87\n",
      "[241 | 2440.50] loss=1.34 avg=1.87\n",
      "[242 | 2450.48] loss=1.39 avg=1.86\n",
      "[243 | 2460.45] loss=2.18 avg=1.87\n",
      "[244 | 2470.45] loss=1.09 avg=1.86\n",
      "[245 | 2480.44] loss=2.38 avg=1.86\n",
      "[246 | 2490.51] loss=1.45 avg=1.86\n",
      "[247 | 2500.51] loss=2.31 avg=1.86\n",
      "[248 | 2510.55] loss=1.93 avg=1.87\n",
      "[249 | 2520.57] loss=3.11 avg=1.88\n",
      "[250 | 2530.59] loss=2.80 avg=1.89\n",
      "[251 | 2540.66] loss=2.41 avg=1.89\n",
      "[252 | 2550.72] loss=1.31 avg=1.89\n",
      "[253 | 2561.02] loss=1.77 avg=1.89\n",
      "[254 | 2571.09] loss=2.43 avg=1.89\n",
      "[255 | 2581.14] loss=1.11 avg=1.88\n",
      "[256 | 2591.20] loss=1.62 avg=1.88\n",
      "[257 | 2601.22] loss=1.76 avg=1.88\n",
      "[258 | 2611.33] loss=1.99 avg=1.88\n",
      "[259 | 2621.39] loss=1.81 avg=1.88\n",
      "[260 | 2631.42] loss=1.92 avg=1.88\n",
      "[261 | 2641.45] loss=1.27 avg=1.87\n",
      "[262 | 2651.44] loss=1.88 avg=1.87\n",
      "[263 | 2661.49] loss=2.27 avg=1.88\n",
      "[264 | 2671.59] loss=1.61 avg=1.88\n",
      "[265 | 2681.60] loss=2.59 avg=1.88\n",
      "[266 | 2691.58] loss=2.32 avg=1.89\n",
      "[267 | 2701.59] loss=1.50 avg=1.88\n",
      "[268 | 2711.59] loss=1.40 avg=1.88\n",
      "[269 | 2721.62] loss=1.21 avg=1.87\n",
      "[270 | 2731.65] loss=1.80 avg=1.87\n",
      "[271 | 2741.93] loss=0.97 avg=1.86\n",
      "[272 | 2751.93] loss=3.31 avg=1.88\n",
      "[273 | 2761.97] loss=2.53 avg=1.88\n",
      "[274 | 2772.00] loss=2.30 avg=1.89\n",
      "[275 | 2782.06] loss=1.09 avg=1.88\n",
      "[276 | 2792.12] loss=1.74 avg=1.88\n",
      "[277 | 2802.19] loss=1.21 avg=1.87\n",
      "[278 | 2812.23] loss=2.06 avg=1.87\n",
      "[279 | 2822.26] loss=3.29 avg=1.89\n",
      "[280 | 2832.25] loss=1.46 avg=1.88\n",
      "[281 | 2842.25] loss=2.82 avg=1.89\n",
      "[282 | 2852.30] loss=1.88 avg=1.89\n",
      "[283 | 2862.36] loss=1.70 avg=1.89\n",
      "[284 | 2872.39] loss=1.41 avg=1.89\n",
      "[285 | 2882.42] loss=1.68 avg=1.88\n",
      "[286 | 2892.43] loss=1.23 avg=1.88\n",
      "[287 | 2902.47] loss=1.68 avg=1.87\n",
      "[288 | 2912.47] loss=1.73 avg=1.87\n",
      "[289 | 2922.79] loss=1.14 avg=1.87\n",
      "[290 | 2932.79] loss=2.59 avg=1.87\n",
      "[291 | 2942.85] loss=1.82 avg=1.87\n",
      "[292 | 2952.86] loss=1.33 avg=1.87\n",
      "[293 | 2962.87] loss=2.61 avg=1.87\n",
      "[294 | 2972.89] loss=1.93 avg=1.88\n",
      "[295 | 2982.94] loss=1.24 avg=1.87\n",
      "[296 | 2992.96] loss=1.34 avg=1.86\n",
      "[297 | 3003.04] loss=1.91 avg=1.86\n",
      "[298 | 3013.10] loss=1.08 avg=1.86\n",
      "[299 | 3023.18] loss=1.27 avg=1.85\n",
      "[300 | 3033.20] loss=1.24 avg=1.84\n",
      "Saving checkpoint/QGen_QUAC_test/model-300\n",
      "[301 | 3046.18] loss=1.91 avg=1.84\n",
      "[302 | 3056.13] loss=1.13 avg=1.84\n",
      "[303 | 3066.10] loss=2.18 avg=1.84\n",
      "[304 | 3076.14] loss=2.27 avg=1.84\n",
      "[305 | 3086.18] loss=1.99 avg=1.85\n",
      "[306 | 3096.22] loss=1.58 avg=1.84\n",
      "[307 | 3106.44] loss=0.89 avg=1.83\n",
      "[308 | 3116.43] loss=1.92 avg=1.83\n",
      "[309 | 3126.38] loss=2.12 avg=1.84\n",
      "[310 | 3136.37] loss=2.01 avg=1.84\n",
      "[311 | 3146.30] loss=1.47 avg=1.83\n",
      "[312 | 3156.29] loss=1.52 avg=1.83\n",
      "[313 | 3166.28] loss=1.44 avg=1.83\n",
      "[314 | 3176.28] loss=2.02 avg=1.83\n",
      "[315 | 3186.31] loss=1.87 avg=1.83\n",
      "[316 | 3196.33] loss=0.82 avg=1.82\n",
      "[317 | 3206.32] loss=1.31 avg=1.81\n",
      "[318 | 3216.32] loss=1.57 avg=1.81\n",
      "[319 | 3226.29] loss=2.58 avg=1.82\n",
      "[320 | 3236.23] loss=3.00 avg=1.83\n",
      "[321 | 3246.21] loss=2.08 avg=1.83\n",
      "[322 | 3256.18] loss=2.75 avg=1.84\n",
      "[323 | 3266.18] loss=1.42 avg=1.84\n",
      "[324 | 3276.22] loss=3.14 avg=1.85\n",
      "[325 | 3286.49] loss=1.32 avg=1.85\n",
      "[326 | 3296.49] loss=0.96 avg=1.84\n",
      "[327 | 3306.44] loss=3.24 avg=1.85\n",
      "[328 | 3316.41] loss=2.66 avg=1.86\n",
      "[329 | 3326.39] loss=1.11 avg=1.85\n",
      "[330 | 3336.40] loss=1.70 avg=1.85\n",
      "[331 | 3346.46] loss=1.09 avg=1.84\n",
      "[332 | 3356.56] loss=2.38 avg=1.85\n",
      "[334 | 3376.57] loss=1.43 avg=1.84\n",
      "[335 | 3386.57] loss=2.82 avg=1.85\n",
      "[336 | 3396.53] loss=1.13 avg=1.84\n",
      "[337 | 3406.54] loss=1.81 avg=1.84\n",
      "[338 | 3416.51] loss=1.53 avg=1.84\n",
      "[339 | 3426.50] loss=0.93 avg=1.83\n",
      "[340 | 3436.50] loss=1.15 avg=1.82\n",
      "[341 | 3446.49] loss=1.98 avg=1.82\n",
      "[342 | 3456.60] loss=1.93 avg=1.82\n",
      "[343 | 3466.77] loss=2.48 avg=1.83\n",
      "[344 | 3476.75] loss=1.74 avg=1.83\n",
      "[345 | 3486.75] loss=1.16 avg=1.82\n",
      "[346 | 3496.69] loss=1.16 avg=1.82\n",
      "[347 | 3506.68] loss=2.46 avg=1.82\n",
      "[348 | 3516.63] loss=1.68 avg=1.82\n",
      "[349 | 3526.59] loss=1.37 avg=1.82\n",
      "[350 | 3536.57] loss=2.29 avg=1.82\n",
      "[351 | 3546.55] loss=1.50 avg=1.82\n",
      "[352 | 3556.53] loss=1.46 avg=1.81\n",
      "[353 | 3566.50] loss=1.52 avg=1.81\n",
      "[354 | 3576.48] loss=1.06 avg=1.80\n",
      "[355 | 3586.49] loss=0.98 avg=1.80\n",
      "[356 | 3596.46] loss=1.33 avg=1.79\n",
      "[357 | 3606.43] loss=1.67 avg=1.79\n",
      "[358 | 3616.41] loss=2.08 avg=1.79\n",
      "[359 | 3626.42] loss=1.81 avg=1.79\n",
      "[360 | 3636.50] loss=1.89 avg=1.79\n",
      "[361 | 3646.67] loss=3.23 avg=1.81\n",
      "[362 | 3656.66] loss=1.18 avg=1.80\n",
      "[363 | 3666.65] loss=1.81 avg=1.80\n",
      "[364 | 3676.62] loss=1.26 avg=1.80\n",
      "[365 | 3686.63] loss=1.09 avg=1.79\n",
      "[366 | 3696.61] loss=1.42 avg=1.78\n",
      "[367 | 3706.63] loss=1.53 avg=1.78\n",
      "[368 | 3716.60] loss=1.35 avg=1.78\n",
      "[369 | 3726.60] loss=2.01 avg=1.78\n",
      "[370 | 3736.58] loss=0.96 avg=1.77\n",
      "[371 | 3746.64] loss=1.38 avg=1.77\n",
      "[372 | 3756.58] loss=1.58 avg=1.77\n",
      "[373 | 3766.58] loss=1.18 avg=1.76\n",
      "[374 | 3776.58] loss=2.64 avg=1.77\n",
      "[375 | 3786.57] loss=1.27 avg=1.76\n",
      "[376 | 3796.56] loss=2.68 avg=1.77\n",
      "[377 | 3806.62] loss=1.83 avg=1.77\n",
      "[378 | 3816.76] loss=1.15 avg=1.77\n",
      "[379 | 3826.99] loss=2.21 avg=1.77\n",
      "[380 | 3836.93] loss=1.90 avg=1.77\n",
      "[381 | 3846.90] loss=2.27 avg=1.78\n",
      "[382 | 3856.90] loss=2.12 avg=1.78\n",
      "[383 | 3866.92] loss=1.42 avg=1.78\n",
      "[384 | 3876.94] loss=1.39 avg=1.77\n",
      "[385 | 3886.89] loss=1.86 avg=1.78\n",
      "[386 | 3896.89] loss=2.36 avg=1.78\n",
      "[387 | 3906.93] loss=2.10 avg=1.78\n",
      "[388 | 3916.91] loss=1.94 avg=1.79\n",
      "[389 | 3926.90] loss=1.22 avg=1.78\n",
      "[390 | 3936.87] loss=2.24 avg=1.79\n",
      "[391 | 3946.85] loss=2.35 avg=1.79\n",
      "[392 | 3956.86] loss=1.08 avg=1.78\n",
      "[393 | 3966.85] loss=1.80 avg=1.78\n",
      "[394 | 3976.81] loss=1.85 avg=1.78\n",
      "[395 | 3986.78] loss=1.78 avg=1.78\n",
      "[396 | 3996.92] loss=1.43 avg=1.78\n",
      "[397 | 4007.09] loss=3.52 avg=1.80\n",
      "[398 | 4017.05] loss=1.61 avg=1.80\n",
      "[399 | 4027.03] loss=1.96 avg=1.80\n",
      "[400 | 4037.03] loss=1.69 avg=1.80\n",
      "Saving checkpoint/QGen_QUAC_test/model-400\n",
      "[401 | 4050.02] loss=2.59 avg=1.81\n",
      "[402 | 4060.05] loss=1.89 avg=1.81\n",
      "[403 | 4070.11] loss=1.16 avg=1.80\n",
      "[404 | 4080.24] loss=1.80 avg=1.80\n",
      "[405 | 4090.23] loss=2.27 avg=1.80\n",
      "[406 | 4100.28] loss=1.11 avg=1.80\n",
      "[407 | 4110.28] loss=1.37 avg=1.79\n",
      "[408 | 4120.32] loss=1.94 avg=1.79\n",
      "[409 | 4130.29] loss=1.11 avg=1.79\n",
      "[410 | 4140.28] loss=1.15 avg=1.78\n",
      "[411 | 4150.31] loss=2.18 avg=1.78\n",
      "[412 | 4160.29] loss=1.17 avg=1.78\n",
      "[413 | 4170.30] loss=1.01 avg=1.77\n",
      "[414 | 4180.51] loss=1.60 avg=1.77\n",
      "[415 | 4190.50] loss=1.53 avg=1.77\n",
      "[416 | 4200.52] loss=1.15 avg=1.76\n",
      "[417 | 4210.57] loss=1.24 avg=1.76\n",
      "[418 | 4220.65] loss=2.34 avg=1.76\n",
      "[419 | 4230.69] loss=2.15 avg=1.77\n",
      "[420 | 4240.72] loss=2.21 avg=1.77\n",
      "[421 | 4250.72] loss=3.06 avg=1.78\n",
      "[422 | 4260.73] loss=1.81 avg=1.78\n",
      "[423 | 4270.73] loss=2.59 avg=1.79\n",
      "[424 | 4280.71] loss=1.63 avg=1.79\n",
      "[425 | 4290.73] loss=2.16 avg=1.79\n",
      "[426 | 4300.79] loss=1.67 avg=1.79\n",
      "[427 | 4310.82] loss=1.39 avg=1.79\n",
      "[428 | 4320.91] loss=2.19 avg=1.79\n",
      "[429 | 4330.93] loss=1.82 avg=1.79\n",
      "[430 | 4340.98] loss=3.02 avg=1.80\n",
      "[431 | 4350.99] loss=1.23 avg=1.80\n",
      "[432 | 4361.27] loss=1.34 avg=1.79\n",
      "[433 | 4371.31] loss=1.58 avg=1.79\n",
      "[434 | 4381.37] loss=1.22 avg=1.79\n",
      "[435 | 4391.38] loss=1.83 avg=1.79\n",
      "[436 | 4401.46] loss=1.63 avg=1.79\n",
      "[437 | 4411.50] loss=1.35 avg=1.78\n",
      "[438 | 4421.54] loss=2.07 avg=1.78\n",
      "[439 | 4431.54] loss=2.27 avg=1.79\n",
      "[440 | 4441.56] loss=2.65 avg=1.80\n",
      "[441 | 4451.58] loss=2.00 avg=1.80\n",
      "[442 | 4461.58] loss=1.54 avg=1.80\n",
      "[443 | 4471.65] loss=2.22 avg=1.80\n",
      "[444 | 4481.63] loss=1.56 avg=1.80\n",
      "[445 | 4491.64] loss=1.53 avg=1.80\n",
      "[446 | 4501.67] loss=1.31 avg=1.79\n",
      "[447 | 4511.72] loss=2.35 avg=1.80\n",
      "[448 | 4521.69] loss=2.77 avg=1.81\n",
      "[449 | 4531.73] loss=1.85 avg=1.81\n",
      "[450 | 4541.93] loss=1.35 avg=1.80\n",
      "[451 | 4552.00] loss=2.73 avg=1.81\n",
      "[452 | 4562.02] loss=2.65 avg=1.82\n",
      "[453 | 4572.14] loss=1.34 avg=1.82\n",
      "[454 | 4582.15] loss=2.10 avg=1.82\n",
      "[455 | 4592.22] loss=1.27 avg=1.81\n",
      "[456 | 4602.32] loss=1.70 avg=1.81\n",
      "[457 | 4612.31] loss=1.48 avg=1.81\n",
      "[458 | 4622.34] loss=1.67 avg=1.81\n",
      "[459 | 4632.40] loss=1.95 avg=1.81\n",
      "[460 | 4642.39] loss=1.59 avg=1.81\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[461 | 4652.42] loss=2.46 avg=1.81\n",
      "[462 | 4662.46] loss=0.78 avg=1.80\n",
      "[463 | 4672.47] loss=1.42 avg=1.80\n",
      "[464 | 4682.45] loss=2.90 avg=1.81\n",
      "[465 | 4692.49] loss=1.75 avg=1.81\n",
      "[466 | 4702.47] loss=1.66 avg=1.81\n",
      "[467 | 4712.50] loss=1.27 avg=1.80\n",
      "[468 | 4722.80] loss=1.90 avg=1.80\n",
      "[469 | 4732.86] loss=1.90 avg=1.80\n",
      "[470 | 4742.90] loss=2.77 avg=1.81\n",
      "[471 | 4752.87] loss=1.62 avg=1.81\n",
      "[472 | 4762.88] loss=1.14 avg=1.80\n",
      "[473 | 4772.84] loss=1.24 avg=1.80\n",
      "[474 | 4782.89] loss=3.33 avg=1.81\n",
      "[475 | 4792.88] loss=1.58 avg=1.81\n",
      "[476 | 4802.91] loss=2.42 avg=1.82\n",
      "[477 | 4812.96] loss=1.82 avg=1.82\n",
      "[1485 | 14968.24] loss=1.72 avg=1.69\n",
      "[1486 | 14978.48] loss=1.41 avg=1.69\n",
      "[1487 | 14988.54] loss=2.00 avg=1.69\n",
      "[1488 | 14998.55] loss=2.14 avg=1.70\n",
      "[1489 | 15008.54] loss=1.15 avg=1.69\n",
      "[1490 | 15018.56] loss=1.86 avg=1.69\n",
      "[1491 | 15028.62] loss=1.46 avg=1.69\n",
      "[1492 | 15038.59] loss=1.10 avg=1.68\n",
      "[1493 | 15048.60] loss=2.89 avg=1.70\n",
      "[1494 | 15058.63] loss=1.41 avg=1.69\n",
      "[1495 | 15068.58] loss=2.11 avg=1.70\n",
      "[1496 | 15078.59] loss=2.80 avg=1.71\n",
      "[1497 | 15088.64] loss=1.44 avg=1.71\n",
      "[1498 | 15098.61] loss=2.26 avg=1.71\n",
      "[1499 | 15108.65] loss=2.41 avg=1.72\n",
      "[1500 | 15118.67] loss=2.32 avg=1.72\n",
      "Saving checkpoint/QGen_QUAC_test/model-1500\n",
      "[1501 | 15131.53] loss=1.45 avg=1.72\n",
      "[1502 | 15141.53] loss=2.55 avg=1.73\n",
      "[1503 | 15151.54] loss=2.34 avg=1.74\n",
      "[1504 | 15161.78] loss=2.56 avg=1.74\n",
      "[1505 | 15171.82] loss=1.25 avg=1.74\n",
      "[1506 | 15181.81] loss=2.24 avg=1.74\n",
      "[1507 | 15191.81] loss=1.35 avg=1.74\n",
      "[1508 | 15201.82] loss=1.96 avg=1.74\n",
      "[1509 | 15211.75] loss=0.94 avg=1.73\n",
      "[1510 | 15221.76] loss=1.65 avg=1.73\n",
      "[1511 | 15231.75] loss=1.81 avg=1.73\n",
      "[1512 | 15241.79] loss=1.84 avg=1.74\n",
      "[1513 | 15251.73] loss=1.93 avg=1.74\n",
      "[1514 | 15261.66] loss=2.07 avg=1.74\n",
      "[1515 | 15271.64] loss=1.86 avg=1.74\n",
      "[1516 | 15281.62] loss=1.50 avg=1.74\n",
      "[1517 | 15291.58] loss=1.17 avg=1.73\n",
      "[1518 | 15301.55] loss=1.09 avg=1.73\n",
      "[1519 | 15311.52] loss=3.26 avg=1.74\n",
      "[1520 | 15321.49] loss=1.11 avg=1.74\n",
      "[1521 | 15331.52] loss=1.58 avg=1.74\n",
      "[1522 | 15341.72] loss=1.58 avg=1.73\n",
      "[1523 | 15351.75] loss=1.56 avg=1.73\n",
      "[1524 | 15361.72] loss=1.90 avg=1.73\n",
      "[1525 | 15371.70] loss=1.01 avg=1.73\n",
      "[1526 | 15381.67] loss=1.00 avg=1.72\n",
      "[1527 | 15391.64] loss=3.07 avg=1.73\n",
      "[1528 | 15401.61] loss=1.06 avg=1.73\n",
      "[1529 | 15411.56] loss=2.66 avg=1.74\n",
      "[1530 | 15421.61] loss=2.42 avg=1.74\n",
      "[1531 | 15431.61] loss=1.97 avg=1.74\n",
      "[1532 | 15441.64] loss=2.72 avg=1.75\n",
      "[1533 | 15451.63] loss=1.75 avg=1.75\n",
      "[1534 | 15461.59] loss=1.46 avg=1.75\n",
      "[1535 | 15471.60] loss=2.69 avg=1.76\n",
      "[1536 | 15481.63] loss=1.65 avg=1.76\n",
      "[1537 | 15491.65] loss=1.06 avg=1.75\n",
      "[1538 | 15501.64] loss=1.64 avg=1.75\n",
      "[1539 | 15511.60] loss=1.51 avg=1.75\n",
      "[1540 | 15521.87] loss=3.34 avg=1.76\n",
      "[1541 | 15531.86] loss=1.11 avg=1.76\n",
      "[1542 | 15541.92] loss=1.70 avg=1.76\n",
      "[1543 | 15551.97] loss=1.22 avg=1.75\n",
      "[1544 | 15561.92] loss=0.89 avg=1.74\n",
      "[1545 | 15571.96] loss=1.14 avg=1.74\n",
      "[1546 | 15581.94] loss=2.08 avg=1.74\n",
      "[1547 | 15591.91] loss=1.76 avg=1.74\n",
      "[1548 | 15602.00] loss=2.11 avg=1.74\n",
      "[1549 | 15612.01] loss=2.51 avg=1.75\n",
      "[1550 | 15621.95] loss=2.35 avg=1.76\n",
      "[1551 | 15631.93] loss=0.88 avg=1.75\n",
      "[1552 | 15641.94] loss=1.36 avg=1.75\n",
      "[1553 | 15651.98] loss=1.83 avg=1.75\n",
      "[1554 | 15661.99] loss=2.59 avg=1.75\n",
      "[1555 | 15671.99] loss=1.53 avg=1.75\n",
      "[1556 | 15681.97] loss=1.96 avg=1.75\n",
      "[1557 | 15691.98] loss=0.72 avg=1.74\n",
      "[1558 | 15702.23] loss=1.42 avg=1.74\n",
      "[1559 | 15712.22] loss=1.10 avg=1.73\n",
      "[1560 | 15722.29] loss=1.12 avg=1.73\n",
      "[1561 | 15732.26] loss=1.33 avg=1.72\n",
      "[1562 | 15742.25] loss=1.54 avg=1.72\n",
      "[1563 | 15752.27] loss=1.60 avg=1.72\n",
      "[1564 | 15762.36] loss=1.37 avg=1.72\n",
      "[1565 | 15772.31] loss=1.76 avg=1.72\n",
      "[1566 | 15782.30] loss=1.84 avg=1.72\n",
      "[1567 | 15792.29] loss=1.33 avg=1.72\n",
      "[1568 | 15802.31] loss=1.48 avg=1.71\n",
      "[1569 | 15812.28] loss=1.94 avg=1.72\n",
      "[1570 | 15822.28] loss=1.93 avg=1.72\n",
      "[1571 | 15832.22] loss=2.08 avg=1.72\n",
      "[1572 | 15842.19] loss=1.47 avg=1.72\n",
      "[1573 | 15852.19] loss=1.50 avg=1.72\n",
      "[1574 | 15862.14] loss=1.60 avg=1.72\n",
      "[1575 | 15872.14] loss=1.46 avg=1.71\n",
      "[1576 | 15882.35] loss=1.33 avg=1.71\n",
      "[1577 | 15892.34] loss=2.02 avg=1.71\n",
      "[1578 | 15902.32] loss=1.71 avg=1.71\n",
      "[1579 | 15912.30] loss=1.23 avg=1.71\n",
      "[1580 | 15922.28] loss=1.16 avg=1.70\n",
      "[1581 | 15932.27] loss=1.88 avg=1.70\n",
      "[1582 | 15942.24] loss=1.63 avg=1.70\n",
      "[1583 | 15952.22] loss=2.16 avg=1.71\n",
      "[1584 | 15962.26] loss=2.40 avg=1.71\n",
      "[1585 | 15972.25] loss=2.82 avg=1.73\n",
      "[1586 | 15982.25] loss=1.68 avg=1.72\n",
      "[1587 | 15992.22] loss=2.29 avg=1.73\n",
      "[1588 | 16002.14] loss=1.80 avg=1.73\n",
      "[1589 | 16012.09] loss=0.99 avg=1.72\n",
      "[1590 | 16022.03] loss=1.22 avg=1.72\n",
      "[1591 | 16032.05] loss=1.59 avg=1.72\n",
      "[1592 | 16042.02] loss=1.34 avg=1.71\n",
      "[1593 | 16052.04] loss=1.88 avg=1.72\n",
      "[1594 | 16062.27] loss=3.43 avg=1.73\n",
      "[1595 | 16072.28] loss=1.86 avg=1.73\n",
      "[1596 | 16082.30] loss=1.89 avg=1.74\n",
      "[1597 | 16092.29] loss=1.59 avg=1.73\n",
      "[1598 | 16102.30] loss=2.68 avg=1.74\n",
      "[1599 | 16112.31] loss=2.24 avg=1.75\n",
      "[1600 | 16122.28] loss=1.02 avg=1.74\n",
      "Saving checkpoint/QGen_QUAC_test/model-1600\n",
      "[1601 | 16135.23] loss=0.87 avg=1.73\n",
      "[1602 | 16145.31] loss=2.07 avg=1.74\n",
      "[1603 | 16155.33] loss=1.16 avg=1.73\n",
      "[1604 | 16165.36] loss=1.97 avg=1.73\n",
      "[1605 | 16175.35] loss=1.74 avg=1.73\n",
      "[1606 | 16185.33] loss=1.88 avg=1.73\n",
      "[1607 | 16195.35] loss=1.57 avg=1.73\n",
      "[1608 | 16205.38] loss=1.66 avg=1.73\n",
      "[1609 | 16215.39] loss=1.60 avg=1.73\n",
      "[1610 | 16225.40] loss=1.37 avg=1.73\n",
      "[1611 | 16235.40] loss=2.70 avg=1.74\n",
      "[1612 | 16245.72] loss=1.26 avg=1.73\n",
      "[1613 | 16255.77] loss=2.50 avg=1.74\n",
      "[1614 | 16265.78] loss=0.91 avg=1.73\n",
      "[1615 | 16275.78] loss=1.07 avg=1.72\n",
      "[1616 | 16285.75] loss=1.28 avg=1.72\n",
      "[1617 | 16295.76] loss=1.99 avg=1.72\n",
      "[1618 | 16305.80] loss=1.57 avg=1.72\n",
      "[1619 | 16315.85] loss=2.31 avg=1.73\n",
      "[1620 | 16325.85] loss=2.62 avg=1.74\n",
      "[1621 | 16335.89] loss=2.13 avg=1.74\n",
      "[1622 | 16345.88] loss=3.23 avg=1.75\n",
      "[1623 | 16355.86] loss=2.03 avg=1.76\n",
      "[1624 | 16365.87] loss=1.95 avg=1.76\n",
      "[1625 | 16375.81] loss=1.67 avg=1.76\n",
      "[1626 | 16385.89] loss=1.99 avg=1.76\n",
      "[1627 | 16395.95] loss=1.38 avg=1.76\n",
      "[1628 | 16405.95] loss=1.41 avg=1.75\n",
      "[1629 | 16415.99] loss=1.92 avg=1.76\n",
      "[1630 | 16426.23] loss=2.31 avg=1.76\n",
      "[1631 | 16436.30] loss=1.94 avg=1.76\n",
      "[1632 | 16446.34] loss=1.93 avg=1.76\n",
      "[1633 | 16456.35] loss=1.24 avg=1.76\n",
      "[1634 | 16466.30] loss=1.43 avg=1.76\n",
      "[1635 | 16476.36] loss=1.95 avg=1.76\n",
      "[1636 | 16486.36] loss=1.58 avg=1.76\n",
      "[1637 | 16496.41] loss=1.24 avg=1.75\n",
      "[1638 | 16506.45] loss=2.13 avg=1.75\n",
      "[1639 | 16516.52] loss=1.57 avg=1.75\n",
      "[1640 | 16526.54] loss=1.48 avg=1.75\n",
      "[1641 | 16536.57] loss=1.61 avg=1.75\n",
      "[1642 | 16546.59] loss=1.27 avg=1.74\n",
      "[1643 | 16556.56] loss=1.52 avg=1.74\n",
      "[1644 | 16566.60] loss=1.86 avg=1.74\n",
      "[1645 | 16576.56] loss=2.55 avg=1.75\n",
      "[1646 | 16586.58] loss=1.66 avg=1.75\n",
      "[1647 | 16596.66] loss=1.33 avg=1.75\n",
      "[1648 | 16606.92] loss=1.63 avg=1.74\n",
      "[1649 | 16616.94] loss=1.78 avg=1.74\n",
      "[1650 | 16626.96] loss=1.23 avg=1.74\n",
      "[1651 | 16636.92] loss=1.92 avg=1.74\n",
      "[1652 | 16646.92] loss=1.66 avg=1.74\n",
      "[1653 | 16656.95] loss=2.09 avg=1.74\n",
      "[1654 | 16666.92] loss=0.91 avg=1.74\n",
      "[1655 | 16676.99] loss=1.80 avg=1.74\n",
      "[1656 | 16686.94] loss=1.88 avg=1.74\n",
      "[1657 | 16696.90] loss=1.58 avg=1.74\n",
      "[1658 | 16706.92] loss=3.26 avg=1.75\n",
      "[1659 | 16716.94] loss=1.34 avg=1.75\n",
      "[1660 | 16726.91] loss=2.31 avg=1.75\n",
      "[1661 | 16737.00] loss=2.44 avg=1.76\n",
      "[1662 | 16747.02] loss=1.59 avg=1.76\n",
      "[1663 | 16756.99] loss=1.56 avg=1.76\n",
      "[1664 | 16766.95] loss=1.21 avg=1.75\n",
      "[1665 | 16776.99] loss=0.96 avg=1.74\n",
      "[1666 | 16787.22] loss=2.08 avg=1.75\n",
      "[1667 | 16797.19] loss=2.20 avg=1.75\n",
      "[1668 | 16807.20] loss=1.71 avg=1.75\n",
      "[1669 | 16817.21] loss=1.31 avg=1.75\n",
      "[1670 | 16827.22] loss=1.24 avg=1.74\n",
      "[1671 | 16837.17] loss=1.21 avg=1.74\n",
      "[1672 | 16847.19] loss=1.45 avg=1.73\n",
      "[1673 | 16857.17] loss=1.09 avg=1.73\n",
      "[1674 | 16867.15] loss=1.07 avg=1.72\n",
      "[1675 | 16877.19] loss=1.71 avg=1.72\n",
      "[1676 | 16887.22] loss=1.41 avg=1.72\n",
      "[1677 | 16897.23] loss=1.98 avg=1.72\n",
      "[1678 | 16907.15] loss=1.39 avg=1.72\n",
      "[1679 | 16917.18] loss=1.29 avg=1.71\n",
      "[1680 | 16927.22] loss=2.88 avg=1.72\n",
      "[1681 | 16937.18] loss=1.18 avg=1.72\n",
      "[1682 | 16947.15] loss=1.98 avg=1.72\n",
      "[1683 | 16957.33] loss=1.44 avg=1.72\n",
      "[1684 | 16967.47] loss=1.49 avg=1.72\n",
      "[1685 | 16977.45] loss=2.20 avg=1.72\n",
      "[1686 | 16987.50] loss=2.16 avg=1.72\n",
      "[1687 | 16997.50] loss=1.72 avg=1.72\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1688 | 17007.50] loss=1.11 avg=1.72\n",
      "[1689 | 17017.46] loss=1.12 avg=1.71\n",
      "[1690 | 17027.51] loss=2.35 avg=1.72\n",
      "[1691 | 17037.61] loss=1.00 avg=1.71\n",
      "[1692 | 17047.65] loss=1.29 avg=1.71\n",
      "[1693 | 17057.62] loss=1.28 avg=1.70\n",
      "[1694 | 17067.65] loss=1.76 avg=1.70\n",
      "[1695 | 17077.66] loss=2.54 avg=1.71\n",
      "[1696 | 17087.65] loss=2.46 avg=1.72\n",
      "[1697 | 17097.71] loss=0.98 avg=1.71\n",
      "[1698 | 17107.72] loss=2.76 avg=1.72\n",
      "[1699 | 17117.74] loss=1.19 avg=1.72\n",
      "[1700 | 17127.81] loss=1.27 avg=1.71\n",
      "Saving checkpoint/QGen_QUAC_test/model-1700\n",
      "[1701 | 17141.01] loss=2.10 avg=1.72\n",
      "[1702 | 17150.96] loss=1.92 avg=1.72\n",
      "[1703 | 17160.95] loss=1.70 avg=1.72\n",
      "[1704 | 17171.04] loss=1.62 avg=1.72\n",
      "[1705 | 17181.03] loss=1.37 avg=1.71\n",
      "[1706 | 17191.04] loss=1.84 avg=1.72\n",
      "[1707 | 17200.99] loss=1.58 avg=1.71\n",
      "[1708 | 17210.99] loss=2.10 avg=1.72\n",
      "[1709 | 17220.95] loss=1.75 avg=1.72\n",
      "[1710 | 17230.97] loss=3.26 avg=1.73\n",
      "[1711 | 17240.92] loss=2.25 avg=1.74\n",
      "[1712 | 17250.89] loss=2.44 avg=1.75\n",
      "[1713 | 17260.85] loss=1.63 avg=1.74\n",
      "[1714 | 17270.82] loss=1.21 avg=1.74\n",
      "[1715 | 17280.83] loss=3.37 avg=1.76\n",
      "[1716 | 17290.83] loss=1.23 avg=1.75\n",
      "[1717 | 17300.82] loss=1.46 avg=1.75\n",
      "[1718 | 17310.85] loss=1.84 avg=1.75\n",
      "[1719 | 17321.04] loss=1.77 avg=1.75\n",
      "[1720 | 17331.06] loss=2.96 avg=1.76\n",
      "[1721 | 17341.06] loss=1.64 avg=1.76\n",
      "[1722 | 17351.07] loss=2.42 avg=1.77\n",
      "[1723 | 17361.10] loss=1.40 avg=1.76\n",
      "[1724 | 17371.18] loss=2.52 avg=1.77\n",
      "[1725 | 17381.17] loss=2.60 avg=1.78\n",
      "[1726 | 17391.15] loss=2.63 avg=1.79\n",
      "[1727 | 17401.16] loss=1.50 avg=1.78\n",
      "[1728 | 17411.12] loss=1.31 avg=1.78\n",
      "[1729 | 17421.08] loss=1.94 avg=1.78\n",
      "[1730 | 17431.07] loss=1.19 avg=1.78\n",
      "[1731 | 17441.06] loss=1.78 avg=1.78\n",
      "[1732 | 17451.03] loss=1.34 avg=1.77\n",
      "[1733 | 17461.01] loss=2.64 avg=1.78\n",
      "[1734 | 17470.98] loss=2.04 avg=1.78\n",
      "[1735 | 17480.99] loss=0.98 avg=1.77\n",
      "[1736 | 17490.98] loss=2.02 avg=1.78\n",
      "[1737 | 17501.21] loss=1.49 avg=1.77\n",
      "[1738 | 17511.18] loss=1.08 avg=1.77\n",
      "[1739 | 17521.16] loss=1.62 avg=1.77\n",
      "[1740 | 17531.14] loss=3.16 avg=1.78\n",
      "[1741 | 17541.18] loss=1.39 avg=1.78\n",
      "[1742 | 17551.13] loss=1.43 avg=1.77\n",
      "[1743 | 17561.12] loss=1.90 avg=1.77\n",
      "[1744 | 17571.09] loss=1.48 avg=1.77\n",
      "[1745 | 17581.16] loss=1.61 avg=1.77\n",
      "[1746 | 17591.22] loss=1.10 avg=1.76\n",
      "[1747 | 17601.27] loss=2.79 avg=1.77\n",
      "[1748 | 17611.36] loss=1.21 avg=1.77\n",
      "[1749 | 17621.40] loss=1.10 avg=1.76\n",
      "[1750 | 17631.49] loss=1.39 avg=1.76\n",
      "[1751 | 17641.53] loss=0.84 avg=1.75\n",
      "[1752 | 17651.53] loss=1.26 avg=1.74\n",
      "[1753 | 17661.54] loss=2.36 avg=1.75\n",
      "[1754 | 17671.62] loss=1.64 avg=1.75\n",
      "[1755 | 17681.87] loss=1.98 avg=1.75\n",
      "[1756 | 17691.96] loss=1.68 avg=1.75\n",
      "[1757 | 17701.96] loss=1.22 avg=1.74\n",
      "[1758 | 17711.99] loss=2.13 avg=1.75\n",
      "[1759 | 17722.00] loss=1.52 avg=1.75\n",
      "[1760 | 17732.00] loss=3.10 avg=1.76\n",
      "[1761 | 17741.98] loss=1.12 avg=1.75\n",
      "[1762 | 17751.98] loss=1.95 avg=1.75\n",
      "[1763 | 17761.97] loss=1.66 avg=1.75\n",
      "[1764 | 17772.00] loss=1.67 avg=1.75\n",
      "[1765 | 17781.97] loss=2.17 avg=1.76\n",
      "[1766 | 17792.02] loss=1.07 avg=1.75\n",
      "[1767 | 17802.06] loss=1.83 avg=1.75\n",
      "[1768 | 17812.02] loss=2.54 avg=1.76\n",
      "[1769 | 17822.07] loss=1.27 avg=1.75\n",
      "[1770 | 17832.07] loss=2.00 avg=1.76\n",
      "[1771 | 17842.09] loss=1.58 avg=1.75\n",
      "[1772 | 17852.08] loss=1.39 avg=1.75\n",
      "[1773 | 17862.28] loss=1.65 avg=1.75\n",
      "[1774 | 17872.30] loss=1.29 avg=1.75\n",
      "[1775 | 17882.33] loss=2.51 avg=1.75\n",
      "[1776 | 17892.40] loss=1.13 avg=1.75\n",
      "[1777 | 17902.39] loss=1.30 avg=1.74\n",
      "[1778 | 17912.39] loss=1.92 avg=1.74\n",
      "[1779 | 17922.38] loss=1.04 avg=1.74\n",
      "[1780 | 17932.33] loss=0.86 avg=1.73\n",
      "[1781 | 17942.38] loss=2.10 avg=1.73\n",
      "[1782 | 17952.33] loss=2.03 avg=1.73\n",
      "[1783 | 17962.29] loss=1.31 avg=1.73\n",
      "[1784 | 17972.24] loss=2.40 avg=1.74\n",
      "[1785 | 17982.27] loss=2.12 avg=1.74\n",
      "[1786 | 17992.23] loss=2.11 avg=1.74\n",
      "[1787 | 18002.20] loss=2.07 avg=1.75\n",
      "[1788 | 18012.12] loss=1.77 avg=1.75\n",
      "[1789 | 18022.15] loss=1.55 avg=1.75\n",
      "[1790 | 18032.11] loss=1.59 avg=1.74\n",
      "[1791 | 18042.27] loss=0.84 avg=1.74\n",
      "[1792 | 18052.22] loss=1.90 avg=1.74\n",
      "[1793 | 18062.24] loss=1.72 avg=1.74\n",
      "[1794 | 18072.26] loss=2.36 avg=1.74\n",
      "[1795 | 18082.25] loss=1.82 avg=1.74\n",
      "[1796 | 18092.25] loss=2.55 avg=1.75\n",
      "[1797 | 18102.25] loss=2.20 avg=1.76\n",
      "[1798 | 18112.28] loss=1.36 avg=1.75\n",
      "[1799 | 18122.24] loss=1.38 avg=1.75\n",
      "[1800 | 18132.24] loss=1.07 avg=1.74\n",
      "Saving checkpoint/QGen_QUAC_test/model-1800\n",
      "[1801 | 18145.18] loss=1.49 avg=1.74\n",
      "[1802 | 18155.25] loss=0.97 avg=1.73\n",
      "[1803 | 18165.31] loss=2.99 avg=1.74\n",
      "[1804 | 18175.32] loss=1.25 avg=1.74\n",
      "[1805 | 18185.37] loss=2.60 avg=1.75\n",
      "[1806 | 18195.36] loss=1.35 avg=1.74\n",
      "[1807 | 18205.39] loss=2.14 avg=1.75\n",
      "[1808 | 18215.49] loss=1.51 avg=1.75\n",
      "[1809 | 18225.77] loss=1.60 avg=1.74\n",
      "[1810 | 18235.77] loss=2.47 avg=1.75\n",
      "[1811 | 18245.78] loss=1.04 avg=1.74\n",
      "[1812 | 18255.80] loss=1.44 avg=1.74\n",
      "[1813 | 18265.80] loss=1.75 avg=1.74\n",
      "[1814 | 18275.84] loss=1.79 avg=1.74\n",
      "[1815 | 18285.85] loss=2.70 avg=1.75\n",
      "[1816 | 18295.86] loss=1.60 avg=1.75\n",
      "[1817 | 18305.89] loss=1.52 avg=1.75\n",
      "[1818 | 18315.90] loss=1.16 avg=1.74\n",
      "[1819 | 18325.93] loss=0.73 avg=1.73\n",
      "[1820 | 18335.94] loss=1.71 avg=1.73\n",
      "[1821 | 18346.02] loss=3.41 avg=1.75\n",
      "[1822 | 18356.14] loss=2.94 avg=1.76\n",
      "[1823 | 18366.32] loss=2.42 avg=1.77\n",
      "[1824 | 18376.40] loss=1.78 avg=1.77\n",
      "[1825 | 18386.49] loss=1.98 avg=1.77\n",
      "[1826 | 18396.67] loss=1.73 avg=1.77\n",
      "[1827 | 18406.95] loss=1.18 avg=1.76\n",
      "[1828 | 18417.02] loss=2.63 avg=1.77\n",
      "[1829 | 18427.14] loss=1.46 avg=1.77\n",
      "[1830 | 18437.19] loss=1.56 avg=1.77\n",
      "[1831 | 18447.31] loss=2.22 avg=1.77\n",
      "[1832 | 18457.41] loss=2.95 avg=1.78\n",
      "[1833 | 18467.51] loss=1.55 avg=1.78\n",
      "[1834 | 18477.62] loss=2.21 avg=1.78\n",
      "[1835 | 18487.78] loss=2.61 avg=1.79\n",
      "[1836 | 18497.88] loss=1.52 avg=1.79\n",
      "[1837 | 18508.08] loss=1.69 avg=1.79\n",
      "[1838 | 18518.22] loss=1.62 avg=1.79\n",
      "[1839 | 18528.33] loss=2.94 avg=1.80\n",
      "[1840 | 18538.43] loss=2.15 avg=1.80\n",
      "[1841 | 18548.49] loss=0.68 avg=1.79\n",
      "[1842 | 18558.59] loss=1.15 avg=1.78\n",
      "[1843 | 18568.68] loss=1.60 avg=1.78\n",
      "[1844 | 18578.96] loss=1.43 avg=1.78\n",
      "[1845 | 18589.08] loss=1.11 avg=1.77\n",
      "[1846 | 18599.20] loss=1.67 avg=1.77\n",
      "[1847 | 18609.34] loss=1.26 avg=1.77\n",
      "[1848 | 18619.45] loss=0.79 avg=1.76\n",
      "[1849 | 18629.57] loss=1.56 avg=1.75\n",
      "[1850 | 18639.63] loss=1.26 avg=1.75\n",
      "[1851 | 18649.66] loss=1.02 avg=1.74\n",
      "[1852 | 18659.71] loss=1.78 avg=1.74\n",
      "[1853 | 18669.79] loss=1.21 avg=1.74\n",
      "[1854 | 18679.84] loss=1.94 avg=1.74\n",
      "[1855 | 18689.90] loss=1.47 avg=1.74\n",
      "[1856 | 18700.00] loss=2.26 avg=1.74\n",
      "[1857 | 18710.05] loss=2.59 avg=1.75\n",
      "[1858 | 18720.17] loss=1.21 avg=1.75\n",
      "[1859 | 18730.26] loss=2.20 avg=1.75\n",
      "[1860 | 18740.34] loss=1.49 avg=1.75\n",
      "[1861 | 18750.39] loss=1.75 avg=1.75\n",
      "[1862 | 18760.66] loss=1.20 avg=1.74\n",
      "[1863 | 18770.77] loss=2.16 avg=1.75\n",
      "[1864 | 18780.88] loss=2.73 avg=1.76\n",
      "[1865 | 18790.96] loss=1.09 avg=1.75\n",
      "[1866 | 18801.01] loss=1.19 avg=1.74\n",
      "[1867 | 18811.10] loss=1.20 avg=1.74\n",
      "[1868 | 18821.15] loss=1.61 avg=1.74\n",
      "[1869 | 18831.20] loss=1.24 avg=1.73\n",
      "[1870 | 18841.31] loss=2.35 avg=1.74\n",
      "[1871 | 18851.37] loss=0.99 avg=1.73\n",
      "[1872 | 18861.43] loss=0.83 avg=1.72\n",
      "[1873 | 18871.48] loss=1.47 avg=1.72\n",
      "[1874 | 18881.61] loss=3.16 avg=1.73\n",
      "[1875 | 18891.70] loss=1.53 avg=1.73\n",
      "[1876 | 18901.77] loss=1.64 avg=1.73\n",
      "[1877 | 18911.87] loss=1.84 avg=1.73\n",
      "[1878 | 18921.95] loss=1.28 avg=1.73\n",
      "[1879 | 18932.03] loss=1.76 avg=1.73\n",
      "[1880 | 18942.33] loss=2.80 avg=1.74\n",
      "[1881 | 18952.41] loss=1.70 avg=1.74\n",
      "[1882 | 18962.58] loss=1.37 avg=1.73\n",
      "[1883 | 18972.64] loss=1.42 avg=1.73\n",
      "[1884 | 18982.73] loss=2.30 avg=1.74\n",
      "[1885 | 18992.78] loss=2.10 avg=1.74\n",
      "[1886 | 19002.84] loss=1.12 avg=1.73\n",
      "[1887 | 19012.90] loss=1.17 avg=1.73\n",
      "[1888 | 19022.99] loss=1.65 avg=1.73\n",
      "[1889 | 19033.03] loss=2.07 avg=1.73\n",
      "[1890 | 19043.12] loss=1.39 avg=1.73\n",
      "[1891 | 19053.16] loss=1.85 avg=1.73\n",
      "[1892 | 19063.24] loss=2.91 avg=1.74\n",
      "[1893 | 19073.35] loss=1.17 avg=1.74\n",
      "[1894 | 19083.43] loss=1.76 avg=1.74\n",
      "[1895 | 19093.49] loss=1.48 avg=1.73\n",
      "[1896 | 19103.67] loss=2.08 avg=1.74\n",
      "[1897 | 19113.81] loss=1.04 avg=1.73\n",
      "[1898 | 19124.22] loss=1.28 avg=1.72\n",
      "[1899 | 19134.32] loss=2.25 avg=1.73\n",
      "[1900 | 19144.46] loss=1.61 avg=1.73\n",
      "Saving checkpoint/QGen_QUAC_test/model-1900\n",
      "[1901 | 19157.51] loss=1.30 avg=1.72\n",
      "[1902 | 19167.55] loss=1.57 avg=1.72\n",
      "[1903 | 19177.65] loss=1.73 avg=1.72\n",
      "[1904 | 19187.75] loss=1.52 avg=1.72\n",
      "[1905 | 19197.78] loss=1.25 avg=1.72\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1906 | 19207.88] loss=1.74 avg=1.72\n",
      "[1907 | 19217.95] loss=1.00 avg=1.71\n",
      "[1908 | 19228.06] loss=2.20 avg=1.71\n",
      "[1909 | 19238.13] loss=1.99 avg=1.72\n",
      "[1910 | 19248.13] loss=2.00 avg=1.72\n",
      "[1911 | 19258.19] loss=2.07 avg=1.72\n",
      "[1912 | 19268.30] loss=1.71 avg=1.72\n",
      "[1913 | 19278.37] loss=1.81 avg=1.72\n",
      "[1914 | 19288.40] loss=3.08 avg=1.74\n",
      "[1915 | 19298.67] loss=2.46 avg=1.74\n",
      "[1916 | 19308.71] loss=1.18 avg=1.74\n",
      "[1917 | 19318.79] loss=1.51 avg=1.74\n",
      "[1918 | 19328.87] loss=1.42 avg=1.73\n",
      "[1919 | 19338.83] loss=1.19 avg=1.73\n",
      "[1920 | 19348.90] loss=2.29 avg=1.73\n",
      "[1921 | 19358.96] loss=0.89 avg=1.73\n",
      "[1922 | 19368.99] loss=1.77 avg=1.73\n",
      "[1923 | 19379.08] loss=2.73 avg=1.74\n",
      "[1924 | 19389.13] loss=2.09 avg=1.74\n",
      "[1925 | 19399.19] loss=1.82 avg=1.74\n",
      "[1926 | 19409.24] loss=2.34 avg=1.75\n",
      "[1927 | 19419.29] loss=2.31 avg=1.75\n",
      "[1928 | 19429.34] loss=1.93 avg=1.75\n",
      "[1929 | 19439.38] loss=1.43 avg=1.75\n",
      "[1930 | 19449.51] loss=1.57 avg=1.75\n",
      "[1931 | 19459.58] loss=1.43 avg=1.75\n",
      "[1932 | 19469.69] loss=1.76 avg=1.75\n",
      "[1933 | 19479.98] loss=1.48 avg=1.74\n",
      "[1934 | 19490.05] loss=1.13 avg=1.74\n",
      "[1935 | 19500.22] loss=1.27 avg=1.73\n",
      "[1936 | 19510.31] loss=1.23 avg=1.73\n",
      "[1937 | 19520.30] loss=1.47 avg=1.72\n",
      "[1938 | 19530.31] loss=1.43 avg=1.72\n",
      "[1939 | 19540.40] loss=1.40 avg=1.72\n",
      "[1940 | 19550.42] loss=1.26 avg=1.71\n",
      "[1941 | 19560.47] loss=0.65 avg=1.70\n",
      "[1942 | 19570.60] loss=0.93 avg=1.70\n",
      "[1943 | 19580.64] loss=1.82 avg=1.70\n",
      "[1944 | 19590.75] loss=1.32 avg=1.69\n",
      "[1945 | 19600.82] loss=1.59 avg=1.69\n",
      "[1946 | 19610.83] loss=1.26 avg=1.69\n",
      "[1947 | 19620.92] loss=1.35 avg=1.68\n",
      "[1948 | 19630.94] loss=1.73 avg=1.68\n",
      "[1949 | 19641.08] loss=2.26 avg=1.69\n",
      "[1950 | 19651.13] loss=1.22 avg=1.69\n",
      "[1951 | 19661.38] loss=2.13 avg=1.69\n",
      "[1952 | 19671.39] loss=1.33 avg=1.69\n",
      "[1953 | 19681.35] loss=2.09 avg=1.69\n",
      "[1954 | 19691.34] loss=1.42 avg=1.69\n",
      "[1955 | 19701.30] loss=1.21 avg=1.68\n",
      "[1956 | 19711.32] loss=1.36 avg=1.68\n",
      "[1957 | 19721.42] loss=1.53 avg=1.68\n",
      "[1958 | 19731.48] loss=1.37 avg=1.68\n",
      "[1959 | 19741.50] loss=1.29 avg=1.67\n",
      "[1960 | 19751.49] loss=1.72 avg=1.67\n",
      "[1961 | 19761.46] loss=1.26 avg=1.67\n",
      "[1962 | 19771.49] loss=1.78 avg=1.67\n",
      "[1963 | 19781.49] loss=1.19 avg=1.66\n",
      "[1964 | 19791.49] loss=0.87 avg=1.66\n",
      "[1965 | 19801.54] loss=2.08 avg=1.66\n",
      "[1966 | 19811.55] loss=1.34 avg=1.66\n",
      "[1967 | 19821.51] loss=1.82 avg=1.66\n",
      "[1968 | 19831.58] loss=1.73 avg=1.66\n",
      "[1969 | 19841.82] loss=1.87 avg=1.66\n",
      "[1970 | 19851.81] loss=1.92 avg=1.66\n",
      "[1971 | 19861.81] loss=1.26 avg=1.66\n",
      "[1972 | 19871.81] loss=1.45 avg=1.66\n",
      "[1973 | 19881.81] loss=1.86 avg=1.66\n",
      "[1974 | 19891.76] loss=1.69 avg=1.66\n",
      "[1975 | 19901.78] loss=1.83 avg=1.66\n",
      "[1976 | 19911.80] loss=1.86 avg=1.66\n",
      "[1977 | 19921.77] loss=1.21 avg=1.66\n",
      "[1978 | 19931.75] loss=1.83 avg=1.66\n",
      "[1979 | 19941.74] loss=1.20 avg=1.66\n",
      "[1980 | 19951.71] loss=1.66 avg=1.66\n",
      "[1981 | 19961.69] loss=1.55 avg=1.66\n",
      "[1982 | 19971.66] loss=1.25 avg=1.65\n",
      "[1983 | 19981.73] loss=1.68 avg=1.65\n",
      "[1984 | 19991.75] loss=1.20 avg=1.65\n",
      "[1985 | 20001.71] loss=2.26 avg=1.65\n",
      "[1986 | 20011.67] loss=1.41 avg=1.65\n",
      "[1987 | 20021.87] loss=2.54 avg=1.66\n",
      "[1988 | 20031.93] loss=1.42 avg=1.66\n",
      "[1989 | 20041.94] loss=2.09 avg=1.66\n",
      "[1990 | 20051.97] loss=2.71 avg=1.67\n",
      "[1991 | 20061.93] loss=1.17 avg=1.67\n",
      "[1992 | 20071.93] loss=1.42 avg=1.66\n",
      "[1993 | 20081.90] loss=2.08 avg=1.67\n",
      "[1994 | 20091.88] loss=1.03 avg=1.66\n",
      "[1995 | 20101.88] loss=2.01 avg=1.67\n",
      "[1996 | 20111.92] loss=2.69 avg=1.68\n",
      "[1997 | 20121.91] loss=2.61 avg=1.69\n",
      "[1998 | 20131.89] loss=1.50 avg=1.68\n",
      "[1999 | 20141.92] loss=2.02 avg=1.69\n",
      "[2000 | 20151.89] loss=1.47 avg=1.69\n",
      "Saving checkpoint/QGen_QUAC_test/model-2000\n",
      "======== SAMPLE 1 ========\n",
      " his own family.  After taking the reins, he led a successful and sometimes controversial course in public schools, earning a B.A. with a concentration in philosophy, in which his students included a group that included writers like Sigmund Freud and George Peirse. In 1952, he attended Yale Law School, where his classmates included scholars such as John Jowett and Norman Cohn. After graduating, he continued his academic work, becoming a law professor at the University of Chicago and subsequently the United States District Court for the Eastern District of New York. The following year, he taught at the University of Chicago until his retirement from law.  In 1962, his wife, Janet Ann, had a second child, and by late 1963 she had moved back into marriage.  He had been appointed a senior professor of mathematics at the United States Naval Postgraduate School.  He had been active in the civil rights struggle when he was a student at Columbia University.  After leaving Princeton for Yale in 1964, Gould received a teaching position at the Department of Economics of the University of Chicago's Stern School of Business. CANNOTANSWER [QUESTION]: What was his relationship with Janet? <|endoftext|>\n",
      "<|startoftext|>\n",
      "[CONTEXT]: From his 1960s onwards, Gould remained a popular academic, appearing in radio and television documentaries, and contributing to several books, including Gould's 1986 film In Search of Meaning: A New American History from the First Revolution to the Second Civil Rights Movement (with his son, Steven Gould), and The End of American Radicalism: The Rise of the Radical Right (in which his son was the featured character), and The Long Peace, with whom he is closely associated. It was in this period Gould's greatest intellectual work. Gould also worked to expand his readership of philosophy but this was done by building increasingly large, intricate studies which combined philosophical, sociological, biographical and linguistic elements.  At this point Gould left the Department of Economics, and during his career taught several courses at Princeton University. Gould did not withdraw from teaching, and was active politically in the 1960s. He wrote a book, The Unfinished Revolution: The Rise of the National Revolution in 1961-62. Although not the title of his own documentary, The Rise of the National Revolution, the book was a work of \"pure intellectual curiosity.\" In it, Gould argues that despite the election of Lyndon B. Johnson as President of the United States in 1964 and the subsequent civil rights movement, the conservative president was likely to continue the conservative drift, with little prospect for change.  Gould's third documentary, The Last Resort: The Final Days of the Conservative Movement (1993), was the final segment of his career. The film showed him, in a final confrontation with a conservative professor at Harvard, explaining why he had been so \"satisfied\" with conservatism. The film was largely filmed in the basement of his home, and it has been described as his most personal work. In addition, Gould claimed to have written the book for the film. It was released in 1995, when it was reported to be the most expensive documentary ever made - at a total value of US$10 million. With over $2 million and four studios, it proved impossible for Gould to make it look professional or convincing, and several critics rated it less than professional, but Gould said \"I felt if you've got the right stuff, you know it.\"  Gould's final documentary was the 1994 film The Great Divided: A Year on the Campaign Trail (about the 1996 election that saw the Republican candidate, Bill Kristol, accused of racial politics and inciting violence at the University of Virginia) but without his son, Steven. CANNOTANSWER [QUESTION]: What else do we know about Gould's career? <|endoftext|>\n",
      "<|startoftext|>\n",
      "[CONTEXT]: After leaving Princeton for Yale in 1964, Gould received a teaching position at the Department of Economics of the University of Chicago's Stern School of Business. CANNOTANSWER [QUESTION]: What was Gould's position in the college? <|endoftext|>\n",
      "<|startoftext|>\n",
      "[CONTEXT]: From his 1960s onwards, Gould remained a popular academic, appearing in radio and television documentaries, and contributing to several books, including Gould's 1986 film In Search of Meaning: A New American History from the First Revolution to the Second Civil Rights Movement (with his son, Steven Gould), and The End of American Radicalism: The Rise of the Radical Right (in which his son was the featured character), and The Long Peace, with whom he is closely associated. It was in this period Gould's greatest intellectual work. Gould also worked to expand his readership of philosophy but this was done by building increasingly large, intricate studies which combined philosophical, sociological, biographical and linguistic elements.  At this point Gould left the Department of Economics, and during his career taught several courses at Princeton University. Gould did not withdraw\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2001 | 20318.73] loss=1.85 avg=1.69\n",
      "[2002 | 20328.12] loss=1.39 avg=1.68\n",
      "[2003 | 20337.70] loss=2.55 avg=1.69\n",
      "[2004 | 20347.38] loss=1.64 avg=1.69\n",
      "[2005 | 20356.76] loss=1.96 avg=1.69\n",
      "[2006 | 20366.15] loss=1.18 avg=1.69\n",
      "[2007 | 20375.55] loss=1.49 avg=1.69\n",
      "[2008 | 20385.19] loss=1.28 avg=1.68\n",
      "[2009 | 20394.59] loss=2.45 avg=1.69\n",
      "[2010 | 20403.97] loss=1.62 avg=1.69\n",
      "[2011 | 20413.36] loss=1.33 avg=1.69\n",
      "[2012 | 20422.79] loss=1.79 avg=1.69\n",
      "[2013 | 20432.19] loss=1.30 avg=1.68\n",
      "[2014 | 20441.59] loss=2.32 avg=1.69\n",
      "[2015 | 20450.96] loss=1.75 avg=1.69\n",
      "[2016 | 20460.37] loss=0.96 avg=1.68\n",
      "[2017 | 20469.76] loss=2.40 avg=1.69\n",
      "[2018 | 20479.14] loss=1.74 avg=1.69\n",
      "[2019 | 20488.55] loss=1.70 avg=1.69\n",
      "[2020 | 20497.95] loss=1.75 avg=1.69\n",
      "[2021 | 20507.36] loss=2.12 avg=1.70\n",
      "[2022 | 20516.78] loss=1.81 avg=1.70\n",
      "[2023 | 20526.37] loss=1.45 avg=1.70\n",
      "[2024 | 20536.34] loss=1.25 avg=1.69\n",
      "[2025 | 20546.30] loss=1.81 avg=1.69\n",
      "[2026 | 20556.28] loss=1.99 avg=1.69\n",
      "[2027 | 20566.51] loss=2.71 avg=1.70\n",
      "[2028 | 20576.57] loss=1.57 avg=1.70\n",
      "[2029 | 20586.89] loss=1.47 avg=1.70\n",
      "[2030 | 20597.26] loss=1.72 avg=1.70\n",
      "[2031 | 20607.63] loss=1.88 avg=1.70\n",
      "[2032 | 20617.89] loss=1.54 avg=1.70\n",
      "[2033 | 20628.21] loss=0.87 avg=1.69\n",
      "[2034 | 20638.30] loss=1.41 avg=1.69\n",
      "[2035 | 20648.26] loss=1.50 avg=1.69\n",
      "[2036 | 20658.22] loss=0.89 avg=1.68\n",
      "[2037 | 20668.18] loss=0.95 avg=1.67\n",
      "[2038 | 20678.38] loss=3.04 avg=1.69\n",
      "[2039 | 20688.77] loss=2.87 avg=1.70\n",
      "[2040 | 20699.10] loss=1.99 avg=1.70\n",
      "[2041 | 20709.49] loss=1.16 avg=1.70\n",
      "[2042 | 20719.85] loss=2.50 avg=1.70\n",
      "[2043 | 20730.20] loss=1.21 avg=1.70\n",
      "[2044 | 20740.44] loss=2.68 avg=1.71\n",
      "[2045 | 20750.39] loss=1.90 avg=1.71\n",
      "[2046 | 20760.37] loss=1.68 avg=1.71\n",
      "[2047 | 20770.32] loss=0.72 avg=1.70\n",
      "[2048 | 20780.31] loss=1.54 avg=1.70\n",
      "[2049 | 20790.34] loss=2.94 avg=1.71\n",
      "[2050 | 20800.34] loss=2.09 avg=1.72\n",
      "[2051 | 20810.36] loss=1.07 avg=1.71\n",
      "[2052 | 20820.36] loss=2.29 avg=1.71\n",
      "[2053 | 20830.33] loss=1.36 avg=1.71\n",
      "[2054 | 20840.28] loss=1.38 avg=1.71\n",
      "[2055 | 20850.26] loss=2.32 avg=1.71\n",
      "[2056 | 20860.27] loss=1.16 avg=1.71\n",
      "[2057 | 20870.22] loss=2.39 avg=1.72\n",
      "[2058 | 20880.21] loss=1.36 avg=1.71\n",
      "[2059 | 20890.17] loss=1.67 avg=1.71\n",
      "[2060 | 20900.14] loss=1.39 avg=1.71\n",
      "[2061 | 20910.12] loss=1.06 avg=1.70\n",
      "[2062 | 20920.36] loss=1.06 avg=1.70\n",
      "[2063 | 20930.36] loss=1.40 avg=1.69\n",
      "[2064 | 20940.34] loss=1.27 avg=1.69\n",
      "[2065 | 20950.30] loss=1.12 avg=1.68\n",
      "[2066 | 20960.26] loss=2.55 avg=1.69\n",
      "[2067 | 20970.23] loss=2.41 avg=1.70\n",
      "[2068 | 20980.20] loss=2.29 avg=1.70\n",
      "[2069 | 20990.17] loss=1.45 avg=1.70\n",
      "[2070 | 21000.13] loss=3.16 avg=1.72\n",
      "[2071 | 21010.08] loss=1.38 avg=1.71\n",
      "[2072 | 21020.06] loss=1.05 avg=1.71\n",
      "[2073 | 21030.11] loss=1.54 avg=1.70\n",
      "[2074 | 21040.55] loss=1.04 avg=1.70\n",
      "[2075 | 21050.98] loss=1.54 avg=1.70\n",
      "[2076 | 21061.40] loss=1.33 avg=1.69\n",
      "[2077 | 21071.81] loss=2.35 avg=1.70\n",
      "[2078 | 21082.23] loss=1.20 avg=1.69\n",
      "[2079 | 21092.66] loss=1.88 avg=1.70\n",
      "[2080 | 21103.32] loss=1.71 avg=1.70\n",
      "[2081 | 21113.71] loss=1.55 avg=1.69\n",
      "[2082 | 21124.15] loss=0.75 avg=1.69\n",
      "[2083 | 21134.48] loss=0.76 avg=1.68\n",
      "[2084 | 21144.91] loss=0.98 avg=1.67\n",
      "[2085 | 21155.30] loss=2.90 avg=1.68\n",
      "[2086 | 21165.72] loss=1.45 avg=1.68\n",
      "[2087 | 21176.14] loss=1.19 avg=1.67\n",
      "[2088 | 21186.58] loss=1.33 avg=1.67\n",
      "[2089 | 21196.97] loss=1.93 avg=1.67\n",
      "[2090 | 21207.41] loss=2.05 avg=1.68\n",
      "[2091 | 21217.83] loss=1.45 avg=1.67\n",
      "[2092 | 21228.23] loss=1.33 avg=1.67\n",
      "[2093 | 21238.62] loss=1.94 avg=1.67\n",
      "[2094 | 21249.00] loss=1.95 avg=1.68\n",
      "[2095 | 21259.41] loss=1.31 avg=1.67\n",
      "[2096 | 21269.82] loss=1.03 avg=1.67\n",
      "[2097 | 21280.47] loss=2.22 avg=1.67\n",
      "[2098 | 21290.88] loss=1.83 avg=1.67\n",
      "[2099 | 21301.28] loss=2.43 avg=1.68\n",
      "[2100 | 21311.68] loss=0.95 avg=1.67\n",
      "Saving checkpoint/QGen_QUAC_test/model-2100\n",
      "[2101 | 21325.20] loss=1.73 avg=1.67\n",
      "[2102 | 21335.56] loss=2.07 avg=1.68\n",
      "[2103 | 21345.93] loss=1.57 avg=1.68\n",
      "[2104 | 21356.32] loss=1.80 avg=1.68\n",
      "[2105 | 21366.59] loss=1.92 avg=1.68\n",
      "[2106 | 21376.88] loss=1.64 avg=1.68\n",
      "[2107 | 21387.24] loss=2.76 avg=1.69\n",
      "[2108 | 21397.58] loss=2.28 avg=1.70\n",
      "[2109 | 21407.90] loss=1.57 avg=1.70\n",
      "[2110 | 21418.27] loss=1.65 avg=1.70\n",
      "[2111 | 21428.56] loss=1.32 avg=1.69\n",
      "[2112 | 21438.89] loss=1.52 avg=1.69\n",
      "[2113 | 21449.26] loss=2.31 avg=1.70\n",
      "[2114 | 21459.84] loss=2.97 avg=1.71\n",
      "[2115 | 21470.20] loss=1.62 avg=1.71\n",
      "[2116 | 21480.55] loss=1.10 avg=1.70\n",
      "[2117 | 21490.85] loss=1.91 avg=1.70\n",
      "[2118 | 21501.07] loss=3.13 avg=1.72\n",
      "[2119 | 21511.43] loss=2.24 avg=1.72\n",
      "[2120 | 21521.80] loss=1.13 avg=1.72\n",
      "[2121 | 21532.17] loss=2.26 avg=1.72\n",
      "[2122 | 21542.48] loss=2.02 avg=1.73\n",
      "[2123 | 21552.82] loss=2.41 avg=1.73\n",
      "[2124 | 21563.18] loss=1.30 avg=1.73\n",
      "[2125 | 21573.55] loss=0.92 avg=1.72\n",
      "[2126 | 21583.91] loss=1.80 avg=1.72\n",
      "[2127 | 21594.26] loss=3.08 avg=1.73\n",
      "[2128 | 21604.57] loss=1.59 avg=1.73\n",
      "[2129 | 21614.88] loss=1.10 avg=1.73\n",
      "[2130 | 21625.27] loss=1.86 avg=1.73\n",
      "[2131 | 21635.67] loss=1.45 avg=1.73\n",
      "[2132 | 21646.38] loss=0.94 avg=1.72\n",
      "[2133 | 21656.73] loss=1.72 avg=1.72\n",
      "[2134 | 21667.08] loss=2.53 avg=1.73\n",
      "[2135 | 21677.37] loss=1.80 avg=1.73\n",
      "[2136 | 21687.72] loss=1.86 avg=1.73\n",
      "[2137 | 21698.06] loss=2.80 avg=1.74\n",
      "[2138 | 21708.38] loss=1.64 avg=1.74\n",
      "[2139 | 21718.73] loss=2.05 avg=1.74\n",
      "[2140 | 21729.01] loss=1.86 avg=1.74\n",
      "[2141 | 21739.40] loss=2.28 avg=1.75\n",
      "[2142 | 21749.81] loss=1.43 avg=1.74\n",
      "[2143 | 21760.20] loss=2.96 avg=1.76\n",
      "[2144 | 21770.59] loss=1.68 avg=1.76\n",
      "[2145 | 21780.95] loss=1.88 avg=1.76\n",
      "[2146 | 21791.25] loss=1.83 avg=1.76\n",
      "[2147 | 21801.60] loss=2.56 avg=1.77\n",
      "[2148 | 21811.99] loss=1.59 avg=1.76\n",
      "[2149 | 21822.54] loss=1.69 avg=1.76\n",
      "[2150 | 21832.50] loss=1.42 avg=1.76\n",
      "[2151 | 21842.48] loss=1.44 avg=1.76\n",
      "[2152 | 21852.43] loss=0.82 avg=1.75\n",
      "[2153 | 21862.38] loss=1.06 avg=1.74\n",
      "[2154 | 21872.33] loss=1.83 avg=1.74\n",
      "[2155 | 21882.30] loss=2.27 avg=1.75\n",
      "[2156 | 21892.25] loss=1.83 avg=1.75\n",
      "[2157 | 21902.19] loss=2.11 avg=1.75\n",
      "[2158 | 21912.11] loss=2.96 avg=1.76\n",
      "[2159 | 21922.06] loss=1.48 avg=1.76\n",
      "[2160 | 21932.02] loss=0.92 avg=1.75\n",
      "[2161 | 21941.97] loss=1.28 avg=1.75\n",
      "[2162 | 21951.89] loss=1.30 avg=1.74\n",
      "[2163 | 21961.86] loss=2.22 avg=1.75\n",
      "[2164 | 21971.80] loss=1.84 avg=1.75\n",
      "[2165 | 21981.72] loss=1.22 avg=1.74\n",
      "[2166 | 21991.68] loss=1.79 avg=1.74\n",
      "[2167 | 22001.90] loss=2.30 avg=1.75\n",
      "[2168 | 22011.83] loss=1.06 avg=1.74\n",
      "[2169 | 22021.79] loss=0.99 avg=1.73\n",
      "[2170 | 22031.75] loss=1.94 avg=1.74\n",
      "[2171 | 22041.70] loss=0.79 avg=1.73\n",
      "[2172 | 22051.63] loss=2.18 avg=1.73\n",
      "[2173 | 22061.59] loss=1.46 avg=1.73\n",
      "[2174 | 22071.54] loss=1.55 avg=1.73\n",
      "[2175 | 22081.51] loss=1.84 avg=1.73\n",
      "[2176 | 22091.46] loss=1.31 avg=1.72\n",
      "[2177 | 22101.43] loss=1.63 avg=1.72\n",
      "[2178 | 22111.39] loss=1.22 avg=1.72\n",
      "[2179 | 22121.32] loss=1.22 avg=1.71\n",
      "[2180 | 22131.49] loss=1.49 avg=1.71\n",
      "[2181 | 22141.44] loss=1.33 avg=1.71\n",
      "[2182 | 22151.40] loss=1.10 avg=1.70\n",
      "[2183 | 22161.35] loss=2.07 avg=1.70\n",
      "[2184 | 22171.34] loss=1.86 avg=1.71\n",
      "[2185 | 22181.52] loss=1.25 avg=1.70\n",
      "[2186 | 22191.45] loss=2.08 avg=1.71\n",
      "[2187 | 22201.40] loss=1.69 avg=1.71\n",
      "[2188 | 22211.33] loss=1.14 avg=1.70\n",
      "[2189 | 22221.27] loss=1.36 avg=1.70\n",
      "[2190 | 22231.21] loss=0.83 avg=1.69\n",
      "[2191 | 22241.17] loss=1.73 avg=1.69\n",
      "[2192 | 22251.10] loss=2.05 avg=1.69\n",
      "[2193 | 22261.06] loss=0.97 avg=1.68\n",
      "[2194 | 22271.01] loss=1.37 avg=1.68\n",
      "[2195 | 22280.94] loss=2.00 avg=1.68\n",
      "[2196 | 22290.89] loss=2.75 avg=1.69\n",
      "[2197 | 22300.82] loss=2.02 avg=1.70\n",
      "[2198 | 22310.78] loss=2.26 avg=1.70\n",
      "[2199 | 22320.73] loss=1.31 avg=1.70\n",
      "[2200 | 22330.66] loss=2.35 avg=1.71\n",
      "Saving checkpoint/QGen_QUAC_test/model-2200\n",
      "[2201 | 22343.57] loss=2.19 avg=1.71\n",
      "[2202 | 22353.54] loss=1.77 avg=1.71\n",
      "[2203 | 22363.77] loss=1.19 avg=1.71\n",
      "[2204 | 22373.79] loss=3.14 avg=1.72\n",
      "[2205 | 22383.77] loss=1.86 avg=1.72\n",
      "[2206 | 22393.73] loss=1.22 avg=1.72\n",
      "[2207 | 22403.70] loss=1.90 avg=1.72\n",
      "[2208 | 22413.67] loss=0.89 avg=1.71\n",
      "[2209 | 22423.64] loss=1.74 avg=1.71\n",
      "[2210 | 22433.82] loss=2.22 avg=1.72\n",
      "[2211 | 22444.15] loss=2.00 avg=1.72\n",
      "[2212 | 22454.40] loss=2.19 avg=1.72\n",
      "[2213 | 22464.37] loss=1.61 avg=1.72\n",
      "[2214 | 22474.36] loss=1.28 avg=1.72\n",
      "[2215 | 22484.34] loss=2.46 avg=1.73\n",
      "[2216 | 22494.32] loss=1.47 avg=1.72\n",
      "[2217 | 22504.33] loss=0.98 avg=1.72\n",
      "[2218 | 22514.31] loss=1.91 avg=1.72\n",
      "[2219 | 22524.28] loss=0.99 avg=1.71\n",
      "[2220 | 22534.25] loss=2.24 avg=1.72\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2221 | 22544.52] loss=1.37 avg=1.71\n",
      "[2222 | 22554.46] loss=1.76 avg=1.71\n",
      "[2223 | 22564.43] loss=1.67 avg=1.71\n",
      "[2224 | 22574.38] loss=1.46 avg=1.71\n",
      "[2225 | 22584.36] loss=1.41 avg=1.71\n",
      "[2226 | 22594.33] loss=2.10 avg=1.71\n",
      "[2227 | 22604.32] loss=1.47 avg=1.71\n",
      "[2228 | 22614.27] loss=2.42 avg=1.72\n",
      "[2229 | 22624.26] loss=0.77 avg=1.71\n",
      "[2230 | 22634.21] loss=1.78 avg=1.71\n",
      "[2231 | 22644.19] loss=1.70 avg=1.71\n",
      "[2232 | 22654.16] loss=2.45 avg=1.71\n",
      "[2233 | 22664.12] loss=1.40 avg=1.71\n",
      "[2234 | 22674.10] loss=1.50 avg=1.71\n",
      "[2235 | 22684.08] loss=1.58 avg=1.71\n",
      "[2236 | 22694.05] loss=0.82 avg=1.70\n",
      "[2237 | 22704.02] loss=1.17 avg=1.69\n",
      "[2238 | 22714.01] loss=2.61 avg=1.70\n",
      "[2239 | 22724.25] loss=2.68 avg=1.71\n",
      "[2240 | 22734.21] loss=2.45 avg=1.72\n",
      "[2241 | 22744.22] loss=2.94 avg=1.73\n",
      "[2242 | 22754.26] loss=1.46 avg=1.73\n",
      "[2243 | 22764.65] loss=0.98 avg=1.72\n",
      "[2244 | 22775.04] loss=1.06 avg=1.72\n",
      "[2245 | 22785.31] loss=0.89 avg=1.71\n",
      "[2246 | 22795.29] loss=1.12 avg=1.70\n",
      "[2247 | 22805.28] loss=1.70 avg=1.70\n",
      "[2248 | 22815.27] loss=2.14 avg=1.71\n",
      "[2249 | 22825.22] loss=2.22 avg=1.71\n",
      "[2250 | 22835.16] loss=1.86 avg=1.71\n",
      "[2251 | 22845.10] loss=1.80 avg=1.71\n",
      "[2252 | 22855.05] loss=1.92 avg=1.72\n",
      "[2253 | 22864.99] loss=3.55 avg=1.73\n",
      "[2254 | 22874.94] loss=1.53 avg=1.73\n",
      "[2255 | 22884.89] loss=2.02 avg=1.73\n",
      "[2256 | 22894.85] loss=1.61 avg=1.73\n",
      "[2257 | 22905.12] loss=2.26 avg=1.74\n",
      "[2258 | 22915.08] loss=1.27 avg=1.73\n",
      "[2259 | 22925.03] loss=1.56 avg=1.73\n",
      "[2260 | 22934.99] loss=1.74 avg=1.73\n",
      "[2261 | 22944.93] loss=2.20 avg=1.74\n",
      "[2262 | 22954.88] loss=1.44 avg=1.73\n",
      "[2263 | 22964.83] loss=2.06 avg=1.74\n",
      "[2264 | 22974.78] loss=0.99 avg=1.73\n",
      "[2265 | 22984.73] loss=1.06 avg=1.72\n",
      "[2266 | 22994.68] loss=0.88 avg=1.71\n",
      "[2267 | 23004.61] loss=0.91 avg=1.71\n",
      "[2268 | 23014.56] loss=1.39 avg=1.70\n",
      "[2269 | 23024.50] loss=1.47 avg=1.70\n",
      "[2270 | 23034.46] loss=0.73 avg=1.69\n",
      "[2271 | 23044.40] loss=1.41 avg=1.69\n",
      "[2272 | 23054.33] loss=2.05 avg=1.69\n",
      "[2273 | 23064.27] loss=1.08 avg=1.69\n",
      "[2274 | 23074.21] loss=1.36 avg=1.68\n",
      "[2275 | 23084.44] loss=1.31 avg=1.68\n",
      "[2276 | 23094.39] loss=2.27 avg=1.68\n",
      "[2277 | 23104.36] loss=1.22 avg=1.68\n",
      "[2278 | 23114.31] loss=1.21 avg=1.68\n",
      "[2279 | 23124.26] loss=1.77 avg=1.68\n",
      "[2280 | 23133.88] loss=1.32 avg=1.67\n",
      "[2281 | 23143.73] loss=1.17 avg=1.67\n",
      "[2282 | 23153.14] loss=1.18 avg=1.66\n",
      "[2283 | 23162.53] loss=1.76 avg=1.66\n",
      "[2284 | 23171.93] loss=1.19 avg=1.66\n",
      "[2285 | 23181.31] loss=2.27 avg=1.67\n",
      "[2286 | 23190.71] loss=1.68 avg=1.67\n",
      "[2287 | 23200.09] loss=2.01 avg=1.67\n",
      "[2288 | 23209.49] loss=2.48 avg=1.68\n",
      "[2289 | 23218.89] loss=1.13 avg=1.67\n",
      "[2290 | 23228.26] loss=2.39 avg=1.68\n",
      "[2291 | 23237.64] loss=1.55 avg=1.68\n",
      "[2292 | 23247.03] loss=1.26 avg=1.67\n",
      "[2293 | 23256.48] loss=1.03 avg=1.67\n",
      "[2294 | 23266.08] loss=1.06 avg=1.66\n",
      "[2295 | 23275.46] loss=1.41 avg=1.66\n",
      "[2296 | 23284.88] loss=0.87 avg=1.65\n",
      "[2297 | 23294.27] loss=2.25 avg=1.66\n",
      "[2298 | 23303.64] loss=1.05 avg=1.65\n",
      "[2299 | 23313.46] loss=1.96 avg=1.65\n",
      "[2300 | 23323.40] loss=1.88 avg=1.66\n",
      "Saving checkpoint/QGen_QUAC_test/model-2300\n",
      "[2301 | 23336.27] loss=1.41 avg=1.65\n",
      "[2302 | 23346.20] loss=1.28 avg=1.65\n",
      "[2303 | 23356.15] loss=1.24 avg=1.64\n",
      "[2304 | 23366.07] loss=1.43 avg=1.64\n",
      "[2305 | 23376.00] loss=1.68 avg=1.64\n",
      "[2306 | 23385.92] loss=1.11 avg=1.64\n",
      "[2307 | 23395.85] loss=1.25 avg=1.63\n",
      "[2308 | 23405.77] loss=1.03 avg=1.63\n",
      "[2309 | 23415.70] loss=1.64 avg=1.63\n",
      "[2310 | 23425.61] loss=1.32 avg=1.62\n",
      "[2311 | 23435.55] loss=1.71 avg=1.63\n",
      "[2312 | 23445.78] loss=2.22 avg=1.63\n",
      "[2313 | 23455.69] loss=1.59 avg=1.63\n",
      "[2314 | 23465.60] loss=1.76 avg=1.63\n",
      "[2315 | 23475.50] loss=3.18 avg=1.65\n",
      "[2316 | 23485.44] loss=1.27 avg=1.64\n",
      "[2317 | 23495.38] loss=1.40 avg=1.64\n",
      "[2318 | 23505.34] loss=1.44 avg=1.64\n",
      "[2319 | 23515.30] loss=2.17 avg=1.65\n",
      "[2320 | 23525.29] loss=3.20 avg=1.66\n",
      "[2321 | 23535.25] loss=2.77 avg=1.67\n",
      "[2322 | 23545.20] loss=1.45 avg=1.67\n",
      "[2323 | 23555.12] loss=1.95 avg=1.67\n",
      "[2324 | 23565.44] loss=1.61 avg=1.67\n",
      "[2325 | 23575.73] loss=1.06 avg=1.67\n",
      "[2326 | 23585.95] loss=2.56 avg=1.67\n",
      "[2327 | 23596.26] loss=1.88 avg=1.68\n",
      "[2328 | 23606.56] loss=1.29 avg=1.67\n",
      "[2329 | 23616.91] loss=1.34 avg=1.67\n",
      "[2330 | 23627.43] loss=2.18 avg=1.67\n",
      "[2331 | 23637.66] loss=1.40 avg=1.67\n",
      "[2332 | 23647.90] loss=2.90 avg=1.68\n",
      "[2333 | 23658.09] loss=1.20 avg=1.68\n",
      "[2334 | 23668.38] loss=1.19 avg=1.67\n",
      "[2335 | 23678.60] loss=1.10 avg=1.67\n",
      "[2336 | 23688.88] loss=2.92 avg=1.68\n",
      "[2337 | 23699.07] loss=1.40 avg=1.68\n",
      "[2338 | 23709.33] loss=1.51 avg=1.68\n",
      "[2339 | 23719.61] loss=1.08 avg=1.67\n",
      "[2340 | 23729.91] loss=1.94 avg=1.67\n",
      "[2341 | 23740.13] loss=2.09 avg=1.68\n",
      "[2342 | 23750.33] loss=1.52 avg=1.68\n",
      "[2343 | 23760.61] loss=0.96 avg=1.67\n",
      "[2345 | 23781.12] loss=1.36 avg=1.67\n",
      "[2346 | 23791.37] loss=2.42 avg=1.68\n",
      "[2348 | 23812.18] loss=1.94 avg=1.68\n",
      "[2349 | 23822.67] loss=2.33 avg=1.68\n",
      "[2350 | 23833.06] loss=0.87 avg=1.68\n",
      "[2351 | 23843.44] loss=1.12 avg=1.67\n",
      "[2352 | 23853.60] loss=2.32 avg=1.68\n",
      "[2353 | 23863.87] loss=1.92 avg=1.68\n",
      "[2354 | 23873.95] loss=2.33 avg=1.69\n",
      "[2355 | 23884.11] loss=1.76 avg=1.69\n",
      "[2356 | 23894.33] loss=2.40 avg=1.69\n",
      "[2357 | 23904.53] loss=3.16 avg=1.71\n",
      "[2448 | 24838.78] loss=2.33 avg=1.73\n",
      "[2449 | 24848.91] loss=2.23 avg=1.73\n",
      "[2450 | 24859.22] loss=1.64 avg=1.73\n",
      "[2451 | 24869.47] loss=2.12 avg=1.74\n",
      "[2452 | 24880.20] loss=0.66 avg=1.73\n",
      "[2453 | 24890.40] loss=1.08 avg=1.72\n",
      "[2454 | 24900.58] loss=1.35 avg=1.72\n",
      "[2455 | 24910.73] loss=3.28 avg=1.73\n",
      "[2456 | 24920.93] loss=1.85 avg=1.73\n",
      "[2457 | 24931.15] loss=1.46 avg=1.73\n",
      "[2458 | 24941.36] loss=1.65 avg=1.73\n",
      "[2684 | 27255.30] loss=2.36 avg=1.65\n",
      "[2685 | 27265.47] loss=2.51 avg=1.66\n",
      "[2686 | 27275.63] loss=1.35 avg=1.66\n",
      "[2952 | 29967.36] loss=1.82 avg=1.65\n",
      "[2953 | 29977.26] loss=1.05 avg=1.65\n",
      "[2954 | 29987.16] loss=1.85 avg=1.65\n",
      "[2955 | 29997.09] loss=1.37 avg=1.65\n"
     ]
    }
   ],
   "source": [
    "gpt2.finetune(qsess,\n",
    "              dataset= \"data/quac_train.txt\", \n",
    "              model_name='345M', \n",
    "              steps=4000, \n",
    "              restore_from='fresh', \n",
    "              run_name='QGen_QUAC_test', \n",
    "              print_every=1, \n",
    "              sample_every=2000, \n",
    "              save_every=100  \n",
    "             )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Pre-trained Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = open('data/quac_test.txt')\n",
    "testset = file.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_lst = testset.split('<|endoftext|>\\n')\n",
    "\n",
    "test_str = test_lst[0].split('[QUESTION]:')[0]\n",
    "\n",
    "ctx = test_str\n",
    "pre = ctx + \" [QUESTION]:\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading checkpoint checkpoint/QGen_QUAC_test/model-4000\n",
      "INFO:tensorflow:Restoring parameters from checkpoint/QGen_QUAC_test/model-4000\n"
     ]
    }
   ],
   "source": [
    "qsess = gpt2.start_tf_sess()\n",
    "gpt2.load_gpt2(qsess, run_name='QGen_QUAC_test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|startoftext|>\n",
      "[CONTEXT]: In May 1983, she married Nikos Karvelas, a composer, with whom she collaborated in 1975 and in November she gave birth to her daughter Sofia. After their marriage, she started a close collaboration with Karvelas. Since 1975, all her releases have become gold or platinum and have included songs by Karvelas. In 1986, she participated at the Cypriot National Final for Eurovision Song Contest with the song Thelo Na Gino Star (\"I Want To Be A Star\"), taking second place. This song is still unreleased up to date. In 1984, Vissi left her record company EMI Greece and signed with CBS Records Greece, which later became Sony Music Greece, a collaboration that lasted until 2013. In March 1984, she released Na 'Hes Kardia (\"If You Had a Heart\"). The album was certified gold. The following year her seventh album Kati Simveni (\"Something Is Happening\") was released which included one of her most famous songs, titled \"Dodeka\" [\"Twelve (O'Clock)\"] and reached gold status selling 80.000 units. In 1986 I Epomeni Kinisi (\"The Next Move\") was released. The album included the hit Pragmata (\"Things\") and went platinum, becoming the best selling record of the year. In February 1988 she released her ninth album Tora (\"Now\") and in December the album Empnefsi! (\"Inspiration!\") which went gold. In 1988, she made her debut as a radio producer on ANT1 Radio. Her radio program was titled after one of her songs Ta Koritsia Einai Atakta (\"Girls Are Naughty\") and was aired every weekend. In the same year, she participated with the song Klaio (\"I'm Crying\") at the Greek National Final for Eurovision Song Contest, finishing third. In 1989, she released the highly successful studio album Fotia (Fire), being one of the first albums to feature western sounds. The lead single Pseftika (\"Fake\") became a big hit and the album reached platinum status, selling 180.000 copies and becoming the second best selling record of 1990. She performed at \"Diogenis Palace\" in that same year, Athens's biggest nightclub/music hall at the time. CANNOTANSWER  [QUESTION]: Did she participate in any other events? <|endoftext|>\n",
      "<|startoftext|>\n",
      "[CONTEXT]: In May 1983, she married Nikos Karvelas, a composer, with whom she collaborated in 1975 and in November she gave birth to her daughter Sofia. After their marriage, she started a close collaboration with Karvelas. Since 1975, all her releases have become gold or platinum and have included songs by Karvelas. In 1986, she participated at the Cypriot National Final for Eurovision Song Contest with the song Thelo Na Gino Star (\"I Want To Be A Star\"), taking second place. This song is still unreleased up to date. In 1984, Vissi left her record company EMI Greece and signed with CBS Records Greece, which later became Sony Music Greece, a collaboration that lasted until 2013. In March 1984, she released Na 'Hes Kardia (\"If You Had a Heart\"). The album was certified gold. The following year her seventh album Kati Simveni (\"The Next Move\") was released which included one of her most famous songs, titled \"Dodeka\" [\"Twelve (O'Clock)\"] and reached gold status selling 80.000 units. In 1986 I Epomeni Kinisi (\"The Next Move\") was released which included the hit Pragmata (\"Things\") and went platinum, becoming the best selling record of the year. In February 1988 she released her ninth album Tora (\"Now\") and in December the album Empnefsi! (\"Inspiration!\") which went gold. In 1988, she made her debut as a radio producer on ANT1 Radio. Her radio program was titled after one of her songs Ta Koritsia Einai Atakta (\"Girls Are Naughty\") and was aired every weekend. In the same year, she participated with the song Klaio (\"I'm Crying\") at the Greek National Final for Eurovision Song Contest, finishing third. In 1989, she released the highly successful studio album Fotia (Fire), being one of the first albums to feature western sounds. The lead single Pseftika (\"Fake\") became a big hit and the album reached platinum status, selling 180.000 copies and becoming the second best selling record of 1990. She performed at \"Diogenis Palace\" in that same year, Athens's biggest nightclub/music hall at the time. CANNOTANSWER [QUESTION]: What did he do after that? <|endoftext|>\n",
      "<|startoftext|>\n",
      "[CONTEXT]: In May\n",
      "====================\n",
      "<|startoftext|>\n",
      "[CONTEXT]: In May 1983, she married Nikos Karvelas, a composer, with whom she collaborated in 1975 and in November she gave birth to her daughter Sofia. After their marriage, she started a close collaboration with Karvelas. Since 1975, all her releases have become gold or platinum and have included songs by Karvelas. In 1986, she participated at the Cypriot National Final for Eurovision Song Contest with the song Thelo Na Gino Star (\"I Want To Be A Star\"), taking second place. This song is still unreleased up to date. In 1984, Vissi left her record company EMI Greece and signed with CBS Records Greece, which later became Sony Music Greece, a collaboration that lasted until 2013. In March 1984, she released Na 'Hes Kardia (\"If You Had a Heart\"). The album was certified gold. The following year her seventh album Kati Simveni (\"Something Is Happening\") was released which included one of her most famous songs, titled \"Dodeka\" [\"Twelve (O'Clock)\"] and reached gold status selling 80.000 units. In 1986 I Epomeni Kinisi (\"The Next Move\") was released. The album included the hit Pragmata (\"Things\") and went platinum, becoming the best selling record of the year. In February 1988 she released her ninth album Tora (\"Now\") and in December the album Empnefsi! (\"Inspiration!\") which went gold. In 1988, she made her debut as a radio producer on ANT1 Radio. Her radio program was titled after one of her songs Ta Koritsia Einai Atakta (\"Girls Are Naughty\") and was aired every weekend. In the same year, she participated with the song Klaio (\"I'm Crying\") at the Greek National Final for Eurovision Song Contest, finishing third. In 1989, she released the highly successful studio album Fotia (Fire), being one of the first albums to feature western sounds. The lead single Pseftika (\"Fake\") became a big hit and the album reached platinum status, selling 180.000 copies and becoming the second best selling record of 1990. She performed at \"Diogenis Palace\" in that same year, Athens's biggest nightclub/music hall at the time. CANNOTANSWER  [QUESTION]: Did she continue recording when she left EMI? <|endoftext|>\n",
      "<|startoftext|>\n",
      "[CONTEXT]: In May 1983, she married Nikos Karvelas, a composer, with whom she collaborated in 1975 and in November she gave birth to her daughter Sofia. After their marriage, she started a close collaboration with Karvelas. Since 1975, all her releases have become gold or platinum and have included songs by Karvelas. In 1986, she participated at the Cypriot National Final for Eurovision Song Contest with the song Thelo Na Gino Star (\"I Want To Be A Star\"), taking second place. This song is still unreleased up to date. In 1984, Vissi left her record company EMI Greece and signed with CBS Records Greece, which later became Sony Music Greece, a collaboration that lasted until 2013. In March 1984, she released Na 'Hes Kardia (\"If You Had a Heart\"). The album was certified gold. The following year her seventh album Kati Simveni (\"The Next Move\") was released which included one of her most famous songs, titled \"Dodeka\" [\"Twelve (O'Clock)\"] and reached gold status selling 80.000 units. In 1986 I Epomeni Kinisi (\"The Next Move\") was released which included the hit Pragmata (\"Things\") and went platinum, becoming the best selling record of the year. In February 1988 she released her ninth album Tora (\"Now\") and in December the album Empnefsi! (\"Inspiration!\") which went gold. In 1988, she made her debut as a radio producer on ANT1 Radio. Her radio program was titled after one of her songs Ta Koritsia Einai Atakta (\"Girls Are Naughty\") and was aired every weekend. In the same year, she participated with the song Klaio (\"I'm Crying\") at the Greek National Final for Eurovision Song Contest, finishing third. In 1989, she released the highly successful studio album Fotia (Fire), being one of the first albums to feature western sounds. The lead single Pseftika (\"Fake\") became a big hit and the album reached platinum status, selling 180.000 copies and becoming the second best selling record of 1990. She performed at \"Diogenis Palace\" in that same year, Athens's biggest nightclub/music hall at the time. CANNOTANSWER [QUESTION]: What was her next album? <|endoftext|>\n",
      "<|startoftext|>\n",
      "[CONTEXT]: In\n",
      "====================\n",
      "<|startoftext|>\n",
      "[CONTEXT]: In May 1983, she married Nikos Karvelas, a composer, with whom she collaborated in 1975 and in November she gave birth to her daughter Sofia. After their marriage, she started a close collaboration with Karvelas. Since 1975, all her releases have become gold or platinum and have included songs by Karvelas. In 1986, she participated at the Cypriot National Final for Eurovision Song Contest with the song Thelo Na Gino Star (\"I Want To Be A Star\"), taking second place. This song is still unreleased up to date. In 1984, Vissi left her record company EMI Greece and signed with CBS Records Greece, which later became Sony Music Greece, a collaboration that lasted until 2013. In March 1984, she released Na 'Hes Kardia (\"If You Had a Heart\"). The album was certified gold. The following year her seventh album Kati Simveni (\"Something Is Happening\") was released which included one of her most famous songs, titled \"Dodeka\" [\"Twelve (O'Clock)\"] and reached gold status selling 80.000 units. In 1986 I Epomeni Kinisi (\"The Next Move\") was released. The album included the hit Pragmata (\"Things\") and went platinum, becoming the best selling record of the year. In February 1988 she released her ninth album Tora (\"Now\") and in December the album Empnefsi! (\"Inspiration!\") which went gold. In 1988, she made her debut as a radio producer on ANT1 Radio. Her radio program was titled after one of her songs Ta Koritsia Einai Atakta (\"Girls Are Naughty\") and was aired every weekend. In the same year, she participated with the song Klaio (\"I'm Crying\") at the Greek National Final for Eurovision Song Contest, finishing third. In 1989, she released the highly successful studio album Fotia (Fire), being one of the first albums to feature western sounds. The lead single Pseftika (\"Fake\") became a big hit and the album reached platinum status, selling 180.000 copies and becoming the second best selling record of 1990. She performed at \"Diogenis Palace\" in that same year, Athens's biggest nightclub/music hall at the time. CANNOTANSWER  [QUESTION]: what was the most important aspect of her career? <|endoftext|>\n",
      "<|startoftext|>\n",
      "[CONTEXT]: In May 1983, she married Nikos Karvelas, a composer, with whom she collaborated in 1975 and in November she gave birth to her daughter Sofia. After their marriage, she started a close collaboration with Karvelas. Since 1975, all her releases have become gold or platinum and have included songs by Karvelas. In 1986, she participated at the Cypriot National Final for Eurovision Song Contest with the song Thelo Na Gino Star (\"I Want To Be A Star\"), taking second place. This song is still unreleased up to date. In 1984, Vissi left her record company EMI Greece and signed with CBS Records, which later became Sony Music Greece, a collaboration that lasted until 2013. In March 1984, she released Na 'Hes Kardia (\"If You Had a Heart\"). The album was certified gold. The following year her seventh album Kati Simveni (\"The Next Move\") was released which included one of her most famous songs, titled \"Dodeka\" [\"Twelve (O'Clock)\"] and reached gold status selling 80.000 units. In 1986 I Epomeni Kinisi (\"The Next Move\") was released which included the hit Pragmata (\"Things\") and went platinum, becoming the best selling record of the year. In February 1988 she released her ninth album Tora (\"Now\") and in December the album Empnefsi! (\"Inspiration!\") which went gold. In 1988, she made her debut as a radio producer on ANT1 Radio. Her radio program was titled after one of her songs Ta Koritsia Einai Atakta (\"Girls Are Naughty\") and was aired every weekend. In the same year, she participated with the song Klaio (\"I'm Crying\") at the Greek National Final for Eurovision Song Contest, finishing third. In 1989, she released the highly successful studio album Fotia (Fire), being one of the first albums to feature western sounds. The lead single Pseftika (\"Fake\") became a big hit and the album reached platinum status, selling 180.000 copies and becoming the second best selling record of 1990. She performed at \"Diogenis Palace\" in that same year, Athens's biggest nightclub/music hall at the time. CANNOTANSWER [QUESTION]: did she produce any other songs? <|endoftext|>\n",
      "<|startoftext|>\n",
      "[CONTEXT]: In\n",
      "====================\n",
      "<|startoftext|>\n",
      "[CONTEXT]: In May 1983, she married Nikos Karvelas, a composer, with whom she collaborated in 1975 and in November she gave birth to her daughter Sofia. After their marriage, she started a close collaboration with Karvelas. Since 1975, all her releases have become gold or platinum and have included songs by Karvelas. In 1986, she participated at the Cypriot National Final for Eurovision Song Contest with the song Thelo Na Gino Star (\"I Want To Be A Star\"), taking second place. This song is still unreleased up to date. In 1984, Vissi left her record company EMI Greece and signed with CBS Records Greece, which later became Sony Music Greece, a collaboration that lasted until 2013. In March 1984, she released Na 'Hes Kardia (\"If You Had a Heart\"). The album was certified gold. The following year her seventh album Kati Simveni (\"Something Is Happening\") was released which included one of her most famous songs, titled \"Dodeka\" [\"Twelve (O'Clock)\"] and reached gold status selling 80.000 units. In 1986 I Epomeni Kinisi (\"The Next Move\") was released. The album included the hit Pragmata (\"Things\") and went platinum, becoming the best selling record of the year. In February 1988 she released her ninth album Tora (\"Now\") and in December the album Empnefsi! (\"Inspiration!\") which went gold. In 1988, she made her debut as a radio producer on ANT1 Radio. Her radio program was titled after one of her songs Ta Koritsia Einai Atakta (\"Girls Are Naughty\") and was aired every weekend. In the same year, she participated with the song Klaio (\"I'm Crying\") at the Greek National Final for Eurovision Song Contest, finishing third. In 1989, she released the highly successful studio album Fotia (Fire), being one of the first albums to feature western sounds. The lead single Pseftika (\"Fake\") became a big hit and the album reached platinum status, selling 180.000 copies and becoming the second best selling record of 1990. She performed at \"Diogenis Palace\" in that same year, Athens's biggest nightclub/music hall at the time. CANNOTANSWER  [QUESTION]: What did she do in 1982? <|endoftext|>\n",
      "<|startoftext|>\n",
      "[CONTEXT]: In May 1983, she married Nikos Karvelas, a composer, with whom she collaborated in 1975 and in November she gave birth to her daughter Sofia. After their marriage, she started a close collaboration with Karvelas. Since 1975, all her releases have become gold or platinum and have included songs by Karvelas. In 1986, she participated at the Cypriot National Final for Eurovision Song Contest with the song Thelo Na Gino Star (\"I Want To Be A Star\"), taking second place. This song is still unreleased up to date. In 1984, Vissi left her record company EMI Greece and signed with CBS Records Greece, which later became Sony Music Greece, a collaboration that lasted until 2013. In March 1984, she released Na 'Hes Kardia (\"If You Had a Heart\"). The album was certified gold. The following year her seventh album Kati Simveni (\"The Next Move\") was released which included one of her most famous songs, titled \"Dodeka\" [\"Twelve (O'Clock)\"] and reached gold status selling 80.000 units. In 1986 I Epomeni Kinisi (\"The Next Move\") was released which included the hit Pragmata (\"Things\") and went platinum, becoming the best selling record of the year. In February 1988 she released her ninth album Tora (\"Now\") and in December the album Empnefsi! (\"Inspiration!\") which went gold. In 1988, she made her debut as a radio producer on ANT1 Radio. Her radio program was titled after one of her songs Ta Koritsia Einai Atakta (\"Girls Are Naughty\") and was aired every weekend. In the same year, she participated with the song Klaio (\"I'm Crying\") at the Greek National Final for Eurovision Song Contest, finishing third. In 1989, she released the highly successful studio album Fotia (Fire), being one of the first albums to feature western sounds. The lead single Pseftika (\"Fake\") became a big hit and the album reached platinum status, selling 180.000 copies and becoming the second best selling record of 1990. She performed at \"Diogenis Palace\" in that same year, Athens's biggest nightclub/music hall at the time. CANNOTANSWER [QUESTION]: What is the name of the album which she made? <|endoftext|>\n",
      "<|startoftext|>\n",
      "[CONTEXT\n",
      "====================\n",
      "<|startoftext|>\n",
      "[CONTEXT]: In May 1983, she married Nikos Karvelas, a composer, with whom she collaborated in 1975 and in November she gave birth to her daughter Sofia. After their marriage, she started a close collaboration with Karvelas. Since 1975, all her releases have become gold or platinum and have included songs by Karvelas. In 1986, she participated at the Cypriot National Final for Eurovision Song Contest with the song Thelo Na Gino Star (\"I Want To Be A Star\"), taking second place. This song is still unreleased up to date. In 1984, Vissi left her record company EMI Greece and signed with CBS Records Greece, which later became Sony Music Greece, a collaboration that lasted until 2013. In March 1984, she released Na 'Hes Kardia (\"If You Had a Heart\"). The album was certified gold. The following year her seventh album Kati Simveni (\"Something Is Happening\") was released which included one of her most famous songs, titled \"Dodeka\" [\"Twelve (O'Clock)\"] and reached gold status selling 80.000 units. In 1986 I Epomeni Kinisi (\"The Next Move\") was released. The album included the hit Pragmata (\"Things\") and went platinum, becoming the best selling record of the year. In February 1988 she released her ninth album Tora (\"Now\") and in December the album Empnefsi! (\"Inspiration!\") which went gold. In 1988, she made her debut as a radio producer on ANT1 Radio. Her radio program was titled after one of her songs Ta Koritsia Einai Atakta (\"Girls Are Naughty\") and was aired every weekend. In the same year, she participated with the song Klaio (\"I'm Crying\") at the Greek National Final for Eurovision Song Contest, finishing third. In 1989, she released the highly successful studio album Fotia (Fire), being one of the first albums to feature western sounds. The lead single Pseftika (\"Fake\") became a big hit and the album reached platinum status, selling 180.000 copies and becoming the second best selling record of 1990. She performed at \"Diogenis Palace\" in that same year, Athens's biggest nightclub/music hall at the time. CANNOTANSWER  [QUESTION]: Did she win any awards? <|endoftext|>\n",
      "<|startoftext|>\n",
      "[CONTEXT]: In May 1983, she married Nikos Karvelas, a composer, with whom she collaborated in 1975 and in November she gave birth to her daughter Sofia. After their marriage, she started a close collaboration with Karvelas. Since 1975, all her releases have become gold or platinum and have included songs by Karvelas. In 1986, she participated at the Cypriot National Final for Eurovision Song Contest with the song Thelo Na Gino Star (\"I Want To Be A Star\"), taking second place. This song is still unreleased up to date. In 1984, Vissi left her record company EMI Greece and signed with CBS Records, which later became Sony Music Greece, a collaboration that lasted until 2013. In March 1984, she released Na 'Hes Kardia (\"If You Had a Heart\"). The album was certified gold. The following year her seventh album Kati Simveni (\"The Next Move\") was released which included one of her most famous songs, titled \"Dodeka\" [\"Twelve (O'Clock)\"] and reached gold status selling 80.000 units. In 1986 I Epomeni Kinisi (\"The Next Move\") was released which included the hit Pragmata (\"Things\") and went platinum, becoming the best selling record of the year. In February 1988 she released her ninth album Tora (\"Now\") and in December the album Empnefsi! (\"Inspiration!\") which went gold. In 1988, she made her debut as a radio producer on ANT1 Radio. Her radio program was titled after one of her songs Ta Koritsia Einai Atakta (\"Girls Are Naughty\") and was aired every weekend. In the same year, she participated with the song Klaio (\"I'm Crying\") at the Greek National Final for Eurovision Song Contest, finishing third. In 1989, she released the highly successful studio album Fotia (Fire), being one of the first albums to feature western sounds. The lead single Pseftika (\"Fake\") became a big hit and the album reached platinum status, selling 180.000 copies and becoming the second best selling record of 1990. She performed at \"Diogenis Palace\" in that same year, Athens's biggest nightclub/music hall at the time. CANNOTANSWER [QUESTION]: What music album was she working with? <|endoftext|>\n",
      "<|startoftext|>\n",
      "[CONTEXT]: In May 1983,\n",
      "====================\n",
      "<|startoftext|>\n",
      "[CONTEXT]: In May 1983, she married Nikos Karvelas, a composer, with whom she collaborated in 1975 and in November she gave birth to her daughter Sofia. After their marriage, she started a close collaboration with Karvelas. Since 1975, all her releases have become gold or platinum and have included songs by Karvelas. In 1986, she participated at the Cypriot National Final for Eurovision Song Contest with the song Thelo Na Gino Star (\"I Want To Be A Star\"), taking second place. This song is still unreleased up to date. In 1984, Vissi left her record company EMI Greece and signed with CBS Records Greece, which later became Sony Music Greece, a collaboration that lasted until 2013. In March 1984, she released Na 'Hes Kardia (\"If You Had a Heart\"). The album was certified gold. The following year her seventh album Kati Simveni (\"Something Is Happening\") was released which included one of her most famous songs, titled \"Dodeka\" [\"Twelve (O'Clock)\"] and reached gold status selling 80.000 units. In 1986 I Epomeni Kinisi (\"The Next Move\") was released. The album included the hit Pragmata (\"Things\") and went platinum, becoming the best selling record of the year. In February 1988 she released her ninth album Tora (\"Now\") and in December the album Empnefsi! (\"Inspiration!\") which went gold. In 1988, she made her debut as a radio producer on ANT1 Radio. Her radio program was titled after one of her songs Ta Koritsia Einai Atakta (\"Girls Are Naughty\") and was aired every weekend. In the same year, she participated with the song Klaio (\"I'm Crying\") at the Greek National Final for Eurovision Song Contest, finishing third. In 1989, she released the highly successful studio album Fotia (Fire), being one of the first albums to feature western sounds. The lead single Pseftika (\"Fake\") became a big hit and the album reached platinum status, selling 180.000 copies and becoming the second best selling record of 1990. She performed at \"Diogenis Palace\" in that same year, Athens's biggest nightclub/music hall at the time. CANNOTANSWER  [QUESTION]: What was her biggest hit to date? <|endoftext|>\n",
      "<|startoftext|>\n",
      "[CONTEXT]: In May 1983, she married Nikos Karvelas, a composer, with whom she collaborated in 1975 and in November she gave birth to her daughter Sofia. After their marriage, she started a close collaboration with Karvelas. Since 1975, all her releases have become gold or platinum and have included songs by Karvelas. In 1986, she participated at the Cypriot National Final for Eurovision Song Contest with the song Thelo Na Gino Star (\"I Want To Be A Star\"), taking second place. This song is still unreleased up to date. In 1984, Vissi left her record company EMI Greece and signed with CBS Records, which later became Sony Music Greece, a collaboration that lasted until 2013. In March 1984, she released Na 'Hes Kardia (\"If You Had a Heart\"). The album was certified gold. The following year her seventh album Kati Simveni (\"The Next Move\") was released which included one of her most famous songs, titled \"Dodeka\" [\"Twelve (O'Clock)\"] and reached gold status selling 80.000 units. In 1986 I Epomeni Kinisi (\"The Next Move\") was released which included the hit Pragmata (\"Things\") and went platinum, becoming the best selling record of the year. In February 1988 she released her ninth album Tora (\"Now\") and in December the album Empnefsi! (\"Inspiration!\") which went gold. In 1988, she made her debut as a radio producer on ANT1 Radio. Her radio program was titled after one of her songs Ta Koritsia Einai Atakta (\"Girls Are Naughty\") and was aired every weekend. In the same year, she participated with the song Klaio (\"I'm Crying\") at the Greek National Final for Eurovision Song Contest, finishing third. In 1989, she released the highly successful studio album Fotia (Fire), being one of the first albums to feature western sounds. The lead single Pseftika (\"Fake\") became a big hit and the album reached platinum status, selling 180.000 copies and becoming the second best selling record of 1990. She performed at \"Diogenis Palace\" in that same year, Athens's biggest nightclub/music hall at the time. CANNOTANSWER [QUESTION]: What became of his musical career after leaving EMI? <|endoftext|>\n",
      "<|startoftext|>\n",
      "[CONTEXT\n",
      "====================\n",
      "<|startoftext|>\n",
      "[CONTEXT]: In May 1983, she married Nikos Karvelas, a composer, with whom she collaborated in 1975 and in November she gave birth to her daughter Sofia. After their marriage, she started a close collaboration with Karvelas. Since 1975, all her releases have become gold or platinum and have included songs by Karvelas. In 1986, she participated at the Cypriot National Final for Eurovision Song Contest with the song Thelo Na Gino Star (\"I Want To Be A Star\"), taking second place. This song is still unreleased up to date. In 1984, Vissi left her record company EMI Greece and signed with CBS Records Greece, which later became Sony Music Greece, a collaboration that lasted until 2013. In March 1984, she released Na 'Hes Kardia (\"If You Had a Heart\"). The album was certified gold. The following year her seventh album Kati Simveni (\"Something Is Happening\") was released which included one of her most famous songs, titled \"Dodeka\" [\"Twelve (O'Clock)\"] and reached gold status selling 80.000 units. In 1986 I Epomeni Kinisi (\"The Next Move\") was released. The album included the hit Pragmata (\"Things\") and went platinum, becoming the best selling record of the year. In February 1988 she released her ninth album Tora (\"Now\") and in December the album Empnefsi! (\"Inspiration!\") which went gold. In 1988, she made her debut as a radio producer on ANT1 Radio. Her radio program was titled after one of her songs Ta Koritsia Einai Atakta (\"Girls Are Naughty\") and was aired every weekend. In the same year, she participated with the song Klaio (\"I'm Crying\") at the Greek National Final for Eurovision Song Contest, finishing third. In 1989, she released the highly successful studio album Fotia (Fire), being one of the first albums to feature western sounds. The lead single Pseftika (\"Fake\") became a big hit and the album reached platinum status, selling 180.000 copies and becoming the second best selling record of 1990. She performed at \"Diogenis Palace\" in that same year, Athens's biggest nightclub/music hall at the time. CANNOTANSWER  [QUESTION]: What was the situation for in 1986? <|endoftext|>\n",
      "<|startoftext|>\n",
      "[CONTEXT]: In May 1983, she married Nikos Karvelas, a composer, with whom she collaborated in 1975 and in November she gave birth to her daughter Sofia. After their marriage, she started a close collaboration with Karvelas. Since 1975, all her releases have become gold or platinum and have included songs by Karvelas. In 1986, she participated at the Cypriot National Final for Eurovision Song Contest with the song Thelo Na Gino Star (\"I Want To Be A Star\"), taking second place. This song is still unreleased up to date. In 1984, Vissi left her record company EMI Greece and signed with CBS Records, which later became Sony Music Greece, a collaboration that lasted until 2013. In March 1984, she released Na 'Hes Kardia (\"If You Had a Heart\"). The album was certified gold. The following year her seventh album Kati Simveni (\"The Next Move\") was released which included one of her most famous songs, titled \"Dodeka\" [\"Twelve (O'Clock)\"] and reached gold status selling 80.000 units. In 1986 I Epomeni Kinisi (\"The Next Move\") was released which included the hit Pragmata (\"Things\") and went platinum, becoming the best selling record of the year. In February 1988 she released her ninth album Tora (\"Now\") and in December the album Empnefsi! (\"Inspiration!\") which went gold. In 1988, she made her debut as a radio producer on ANT1 Radio. Her radio program was titled after one of her songs Ta Koritsia Einai Atakta (\"Girls Are Naughty\") and was aired every weekend. In the same year, she participated with the song Klaio (\"I'm Crying\") at the Greek National Final for Eurovision Song Contest, finishing third. In 1989, she released the highly successful studio album Fotia (Fire), being one of the first albums to feature western sounds. The lead single Pseftika (\"Fake\") became a big hit and the album reached platinum status, selling 180.000 copies and becoming the second best selling record of 1990. She performed at \"Diogenis Palace\" in that same year, Athens's biggest nightclub/music hall at the time. CANNOTANSWER [QUESTION]: Did she have a third album in 1986? <|endoftext|>\n",
      "<|startoftext|>\n",
      "[CONTEXT]: In\n",
      "====================\n",
      "<|startoftext|>\n",
      "[CONTEXT]: In May 1983, she married Nikos Karvelas, a composer, with whom she collaborated in 1975 and in November she gave birth to her daughter Sofia. After their marriage, she started a close collaboration with Karvelas. Since 1975, all her releases have become gold or platinum and have included songs by Karvelas. In 1986, she participated at the Cypriot National Final for Eurovision Song Contest with the song Thelo Na Gino Star (\"I Want To Be A Star\"), taking second place. This song is still unreleased up to date. In 1984, Vissi left her record company EMI Greece and signed with CBS Records Greece, which later became Sony Music Greece, a collaboration that lasted until 2013. In March 1984, she released Na 'Hes Kardia (\"If You Had a Heart\"). The album was certified gold. The following year her seventh album Kati Simveni (\"Something Is Happening\") was released which included one of her most famous songs, titled \"Dodeka\" [\"Twelve (O'Clock)\"] and reached gold status selling 80.000 units. In 1986 I Epomeni Kinisi (\"The Next Move\") was released. The album included the hit Pragmata (\"Things\") and went platinum, becoming the best selling record of the year. In February 1988 she released her ninth album Tora (\"Now\") and in December the album Empnefsi! (\"Inspiration!\") which went gold. In 1988, she made her debut as a radio producer on ANT1 Radio. Her radio program was titled after one of her songs Ta Koritsia Einai Atakta (\"Girls Are Naughty\") and was aired every weekend. In the same year, she participated with the song Klaio (\"I'm Crying\") at the Greek National Final for Eurovision Song Contest, finishing third. In 1989, she released the highly successful studio album Fotia (Fire), being one of the first albums to feature western sounds. The lead single Pseftika (\"Fake\") became a big hit and the album reached platinum status, selling 180.000 copies and becoming the second best selling record of 1990. She performed at \"Diogenis Palace\" in that same year, Athens's biggest nightclub/music hall at the time. CANNOTANSWER  [QUESTION]: Was she a member of the other girl groups or was she solo? <|endoftext|>\n",
      "<|startoftext|>\n",
      "[CONTEXT]: In May 1983, she married Nikos Karvelas, a composer, with whom she collaborated in 1975 and in November she gave birth to her daughter Sofia. After their marriage, she started a close collaboration with Karvelas. Since 1975, all her releases have become gold or platinum and have included songs by Karvelas. In 1986, she participated at the Cypriot National Final for Eurovision Song Contest with the song Thelo Na Gino Star (\"I Want To Be A Star\"), taking second place. This song is still unreleased up to date. In 1984, Vissi left her record company EMI Greece and signed with CBS Records, which later became Sony Music Greece, a collaboration that lasted until 2013. In March 1984, she released Na 'Hes Kardia (\"If You Had a Heart\"). The album was certified gold. The following year her seventh album Kati Simveni (\"The Next Move\") was released which included one of her most famous songs, titled \"Dodeka\" [\"Twelve (O'Clock)\"] and reached gold status selling 80.000 units. In 1986 I Epomeni Kinisi (\"The Next Move\") was released which included the hit Pragmata (\"Things\") and went platinum, becoming the best selling record of the year. In February 1988 she released her ninth album Tora (\"Now\") and in December the album Empnefsi! (\"Inspiration!\") which went gold. In 1988, she made her debut as a radio producer on ANT1 Radio. Her radio program was titled after one of her songs Ta Koritsia Einai Atakta (\"Girls Are Naughty\") and was aired every weekend. In the same year, she participated with the song Klaio (\"I'm Crying\") at the Greek National Final for Eurovision Song Contest, finishing third. In 1989, she released the highly successful studio album Fotia (Fire), being one of the first albums to feature western sounds. The lead single Pseftika (\"Fake\") became a big hit and the album reached platinum status, selling 180.000 copies and becoming the second best selling record of 1990. She performed at \"Diogenis Palace\" in that same year, Athens's biggest nightclub/music hall at the time. CANNOTANSWER [QUESTION]: Did she win any awards? <|endoftext|>\n",
      "<|startoftext|>\n",
      "[CON\n",
      "====================\n",
      "<|startoftext|>\n",
      "[CONTEXT]: In May 1983, she married Nikos Karvelas, a composer, with whom she collaborated in 1975 and in November she gave birth to her daughter Sofia. After their marriage, she started a close collaboration with Karvelas. Since 1975, all her releases have become gold or platinum and have included songs by Karvelas. In 1986, she participated at the Cypriot National Final for Eurovision Song Contest with the song Thelo Na Gino Star (\"I Want To Be A Star\"), taking second place. This song is still unreleased up to date. In 1984, Vissi left her record company EMI Greece and signed with CBS Records Greece, which later became Sony Music Greece, a collaboration that lasted until 2013. In March 1984, she released Na 'Hes Kardia (\"If You Had a Heart\"). The album was certified gold. The following year her seventh album Kati Simveni (\"Something Is Happening\") was released which included one of her most famous songs, titled \"Dodeka\" [\"Twelve (O'Clock)\"] and reached gold status selling 80.000 units. In 1986 I Epomeni Kinisi (\"The Next Move\") was released. The album included the hit Pragmata (\"Things\") and went platinum, becoming the best selling record of the year. In February 1988 she released her ninth album Tora (\"Now\") and in December the album Empnefsi! (\"Inspiration!\") which went gold. In 1988, she made her debut as a radio producer on ANT1 Radio. Her radio program was titled after one of her songs Ta Koritsia Einai Atakta (\"Girls Are Naughty\") and was aired every weekend. In the same year, she participated with the song Klaio (\"I'm Crying\") at the Greek National Final for Eurovision Song Contest, finishing third. In 1989, she released the highly successful studio album Fotia (Fire), being one of the first albums to feature western sounds. The lead single Pseftika (\"Fake\") became a big hit and the album reached platinum status, selling 180.000 copies and becoming the second best selling record of 1990. She performed at \"Diogenis Palace\" in that same year, Athens's biggest nightclub/music hall at the time. CANNOTANSWER  [QUESTION]: how did she do on the charts? <|endoftext|>\n",
      "<|startoftext|>\n",
      "[CONTEXT]: In May 1983, she married Nikos Karvelas, a composer, with whom she collaborated in 1975 and in November she gave birth to her daughter Sofia. After their marriage, she started a close collaboration with Karvelas. Since 1975, all her releases have become gold or platinum and have included songs by Karvelas. In 1986, she participated at the Cypriot National Final for Eurovision Song Contest with the song Thelo Na Gino Star (\"I Want To Be A Star\"), taking second place. This song is still unreleased up to date. In 1984, Vissi left her record company EMI Greece and signed with CBS Records, which later became Sony Music Greece, a collaboration that lasted until 2013. In March 1984, she released Na 'Hes Kardia (\"If You Had a Heart\"). The album was certified gold. The following year her seventh album Kati Simveni (\"The Next Move\") was released which included one of her most famous songs, titled \"Dodeka\" [\"Twelve (O'Clock)\"] and reached gold status selling 80.000 units. In 1986 I Epomeni Kinisi (\"The Next Move\") was released which included the hit Pragmata (\"Things\") and went platinum, becoming the best selling record of the year. In February 1988 she released her ninth album Tora (\"Now\") and in December the album Empnefsi! (\"Inspiration!\") which went gold. In 1988, she made her debut as a radio producer on ANT1 Radio. Her radio program was titled after one of her songs Ta Koritsia Einai Atakta (\"Girls Are Naughty\") and was aired every weekend. In the same year, she participated with the song Klaio (\"I'm Crying\") at the Greek National Final for Eurovision Song Contest, finishing third. In 1989, she released the highly successful studio album Fotia (Fire), being one of the first albums to feature western sounds. The lead single Pseftika (\"Fake\") became a big hit and the album reached platinum status, selling 180.000 copies and becoming the second best selling record of 1990. She performed at \"Diogenis Palace\" in that same year, Athens's biggest nightclub/music hall at the time. CANNOTANSWER [QUESTION]: Did she have any hits? <|endoftext|>\n",
      "<|startoftext|>\n",
      "[CONTEXT]: In May 1983,\n",
      "====================\n",
      "<|startoftext|>\n",
      "[CONTEXT]: In May 1983, she married Nikos Karvelas, a composer, with whom she collaborated in 1975 and in November she gave birth to her daughter Sofia. After their marriage, she started a close collaboration with Karvelas. Since 1975, all her releases have become gold or platinum and have included songs by Karvelas. In 1986, she participated at the Cypriot National Final for Eurovision Song Contest with the song Thelo Na Gino Star (\"I Want To Be A Star\"), taking second place. This song is still unreleased up to date. In 1984, Vissi left her record company EMI Greece and signed with CBS Records Greece, which later became Sony Music Greece, a collaboration that lasted until 2013. In March 1984, she released Na 'Hes Kardia (\"If You Had a Heart\"). The album was certified gold. The following year her seventh album Kati Simveni (\"Something Is Happening\") was released which included one of her most famous songs, titled \"Dodeka\" [\"Twelve (O'Clock)\"] and reached gold status selling 80.000 units. In 1986 I Epomeni Kinisi (\"The Next Move\") was released. The album included the hit Pragmata (\"Things\") and went platinum, becoming the best selling record of the year. In February 1988 she released her ninth album Tora (\"Now\") and in December the album Empnefsi! (\"Inspiration!\") which went gold. In 1988, she made her debut as a radio producer on ANT1 Radio. Her radio program was titled after one of her songs Ta Koritsia Einai Atakta (\"Girls Are Naughty\") and was aired every weekend. In the same year, she participated with the song Klaio (\"I'm Crying\") at the Greek National Final for Eurovision Song Contest, finishing third. In 1989, she released the highly successful studio album Fotia (Fire), being one of the first albums to feature western sounds. The lead single Pseftika (\"Fake\") became a big hit and the album reached platinum status, selling 180.000 copies and becoming the second best selling record of 1990. She performed at \"Diogenis Palace\" in that same year, Athens's biggest nightclub/music hall at the time. CANNOTANSWER  [QUESTION]: What was the first single released from her debut album? <|endoftext|>\n",
      "<|startoftext|>\n",
      "[CONTEXT]: In May 1983, she married Nikos Karvelas, a composer, with whom she collaborated in 1975 and in November she gave birth to her daughter Sofia. After their marriage, she started a close collaboration with Karvelas. Since 1975, all her releases have become gold or platinum and have included songs by Karvelas. In 1986, she participated at the Cypriot National Final for Eurovision Song Contest with the song Thelo Na Gino Star (\"I Want To Be A Star\"), taking second place. This song is still unreleased up to date. In 1984, Vissi left her record company EMI Greece and signed with CBS Records Greece, which later became Sony Music Greece, a collaboration that lasted until 2013. In March 1984, she released Na 'Hes Kardia (\"If You Had a Heart\"). The album was certified gold. The following year her seventh album Kati Simveni (\"The Next Move\") was released which included one of her most famous songs, titled \"Dodeka\" [\"Twelve (O'Clock)\"] and reached gold status selling 80.000 units. In 1986 I Epomeni Kinisi (\"The Next Move\") was released which included the hit Pragmata (\"Things\") and went platinum, becoming the best selling record of the year. In February 1988 she released her ninth album Tora (\"Now\") and in December the album Empnefsi! (\"Inspiration!\") which went gold. In 1988, she made her debut as a radio producer on ANT1 Radio. Her radio program was titled after one of her songs Ta Koritsia Einai Atakta (\"Girls Are Naughty\") and was aired every weekend. In the same year, she participated with the song Klaio (\"I'm Crying\") at the Greek National Final for Eurovision Song Contest, finishing third. In 1989, she released the highly successful studio album Fotia (Fire), being one of the first albums to feature western sounds. The lead single Pseftika (\"Fake\") became a big hit and the album reached platinum status, selling 180.000 copies and becoming the second best selling record of 1990. She performed at \"Diogenis Palace\" in that same year, Athens's biggest nightclub/music hall at the time. CANNOTANSWER [QUESTION]: Was her debut album released with the help of her husband? <|endoftext|>\n",
      "<|startoftext|\n",
      "====================\n"
     ]
    }
   ],
   "source": [
    "gpt2.generate(qsess,\n",
    "              temperature=0.7,\n",
    "              prefix=pre,\n",
    "              nsamples=10,\n",
    "              batch_size=10,\n",
    "              run_name=\"QGen_QUAC_test\",\n",
    "              )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hypertuning GPT-2 Squad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# future development - understand benefit in running time, considering the loss function result and/or learning rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"124M\"\n",
    "if not os.path.isdir(os.path.join(\"models\", model_name)):\n",
    "    print(f\"Downloading {model_name} model...\")\n",
    "    gpt2.download_gpt2(model_name=model_name)   # model is saved into current directory under /models/<size>/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import ParameterGrid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default batch size is 8 therefore search around it \n",
    "# default lr is 1e-5 therefore search around that \n",
    "\n",
    "param_grid = {\"batch_size\": [6, 8],\n",
    "              \"learning_rate\": [1e-4, 1e-5, 1e-6],\n",
    "              \"optimizer\": ['adam'],\n",
    "              \"steps\": [2000,4000]\n",
    "             }\n",
    "\n",
    "grid = ParameterGrid(param_grid)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading checkpoint models/124M/model.ckpt\n",
      "INFO:tensorflow:Restoring parameters from models/124M/model.ckpt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [01:23<00:00, 83.88s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset has 16793911 tokens\n",
      "Training...\n",
      "[10 | 17.74] loss=1.29 avg=1.29\n",
      "[20 | 25.15] loss=1.30 avg=1.29\n",
      "[30 | 32.54] loss=1.03 avg=1.20\n",
      "[40 | 39.94] loss=1.14 avg=1.19\n",
      "[50 | 47.39] loss=1.12 avg=1.17\n",
      "[60 | 54.80] loss=1.14 avg=1.17\n",
      "[70 | 62.22] loss=1.20 avg=1.17\n",
      "[80 | 69.65] loss=1.16 avg=1.17\n",
      "[90 | 77.09] loss=1.00 avg=1.15\n",
      "[100 | 84.53] loss=1.06 avg=1.14\n",
      "[110 | 91.97] loss=1.10 avg=1.14\n",
      "[120 | 99.42] loss=1.20 avg=1.14\n",
      "[130 | 106.86] loss=1.04 avg=1.13\n",
      "[140 | 114.30] loss=1.37 avg=1.15\n",
      "[150 | 121.75] loss=1.22 avg=1.16\n",
      "[160 | 129.20] loss=1.31 avg=1.17\n",
      "[170 | 136.66] loss=1.39 avg=1.18\n",
      "[180 | 144.13] loss=1.35 avg=1.19\n",
      "[190 | 151.60] loss=1.38 avg=1.20\n",
      "[200 | 159.07] loss=1.01 avg=1.19\n",
      "[210 | 166.54] loss=1.15 avg=1.19\n",
      "[220 | 174.00] loss=1.07 avg=1.18\n",
      "[230 | 181.48] loss=0.93 avg=1.17\n",
      "[240 | 188.95] loss=1.27 avg=1.18\n",
      "[250 | 196.43] loss=1.21 avg=1.18\n",
      "[260 | 203.90] loss=0.75 avg=1.16\n",
      "[270 | 211.38] loss=1.10 avg=1.16\n",
      "[280 | 218.85] loss=1.26 avg=1.16\n",
      "[290 | 226.37] loss=1.24 avg=1.16\n",
      "[300 | 233.84] loss=1.13 avg=1.16\n",
      "[310 | 241.33] loss=1.19 avg=1.16\n",
      "[320 | 248.82] loss=1.13 avg=1.16\n",
      "[330 | 256.31] loss=1.02 avg=1.16\n",
      "[340 | 263.79] loss=1.14 avg=1.16\n",
      "[350 | 271.26] loss=1.08 avg=1.15\n",
      "[360 | 278.74] loss=0.86 avg=1.15\n",
      "[370 | 286.27] loss=1.11 avg=1.14\n",
      "[380 | 293.74] loss=1.24 avg=1.15\n",
      "[390 | 301.24] loss=1.14 avg=1.15\n",
      "[400 | 308.70] loss=0.79 avg=1.14\n",
      "[410 | 316.18] loss=1.24 avg=1.14\n",
      "[420 | 323.64] loss=1.35 avg=1.15\n",
      "[430 | 331.11] loss=1.22 avg=1.15\n",
      "[440 | 338.58] loss=1.15 avg=1.15\n",
      "[450 | 346.06] loss=1.15 avg=1.15\n",
      "[460 | 353.55] loss=1.10 avg=1.15\n",
      "[470 | 361.04] loss=1.03 avg=1.14\n",
      "[480 | 368.53] loss=1.02 avg=1.14\n",
      "[490 | 376.03] loss=1.18 avg=1.14\n",
      "[500 | 383.53] loss=1.02 avg=1.14\n",
      "Saving checkpoint/squad_ht_0/model-500\n",
      "[510 | 393.00] loss=1.17 avg=1.14\n",
      "[520 | 400.48] loss=1.05 avg=1.14\n",
      "[530 | 407.99] loss=0.90 avg=1.13\n",
      "[540 | 415.47] loss=1.22 avg=1.13\n",
      "[550 | 422.95] loss=1.03 avg=1.13\n",
      "[560 | 430.43] loss=0.97 avg=1.13\n",
      "[570 | 437.92] loss=0.93 avg=1.12\n",
      "[580 | 445.39] loss=0.96 avg=1.12\n",
      "[590 | 452.86] loss=1.16 avg=1.12\n",
      "[600 | 460.33] loss=1.27 avg=1.12\n",
      "[610 | 467.81] loss=1.40 avg=1.13\n",
      "[620 | 475.31] loss=1.32 avg=1.13\n",
      "[630 | 482.80] loss=1.08 avg=1.13\n",
      "[640 | 490.29] loss=1.38 avg=1.14\n",
      "[650 | 497.79] loss=1.09 avg=1.14\n",
      "[660 | 505.27] loss=1.04 avg=1.13\n",
      "[670 | 512.76] loss=1.39 avg=1.14\n",
      "[680 | 520.24] loss=1.32 avg=1.14\n",
      "[690 | 527.74] loss=1.09 avg=1.14\n",
      "[700 | 535.23] loss=1.02 avg=1.14\n",
      "[710 | 542.72] loss=1.17 avg=1.14\n",
      "[720 | 550.22] loss=1.17 avg=1.14\n",
      "[730 | 557.71] loss=1.07 avg=1.14\n",
      "[740 | 565.21] loss=1.20 avg=1.14\n",
      "[750 | 572.71] loss=1.04 avg=1.14\n",
      "[760 | 580.19] loss=0.98 avg=1.14\n",
      "[770 | 587.70] loss=1.19 avg=1.14\n",
      "[780 | 595.19] loss=0.89 avg=1.13\n",
      "[790 | 602.68] loss=1.26 avg=1.13\n",
      "[800 | 610.17] loss=1.22 avg=1.14\n",
      "[810 | 617.66] loss=1.14 avg=1.14\n",
      "[820 | 625.14] loss=1.04 avg=1.13\n",
      "[830 | 632.64] loss=1.17 avg=1.13\n",
      "[840 | 640.13] loss=0.98 avg=1.13\n",
      "[850 | 647.63] loss=1.14 avg=1.13\n",
      "[860 | 655.12] loss=1.01 avg=1.13\n",
      "[870 | 662.61] loss=1.17 avg=1.13\n",
      "[880 | 670.10] loss=1.06 avg=1.13\n",
      "[890 | 677.60] loss=1.17 avg=1.13\n",
      "[900 | 685.09] loss=1.12 avg=1.13\n",
      "[910 | 692.59] loss=1.21 avg=1.13\n",
      "[920 | 700.08] loss=0.95 avg=1.13\n",
      "[930 | 707.58] loss=1.25 avg=1.13\n",
      "[940 | 715.08] loss=1.13 avg=1.13\n",
      "[950 | 722.57] loss=1.06 avg=1.13\n",
      "[960 | 730.06] loss=1.11 avg=1.13\n",
      "[970 | 737.57] loss=1.13 avg=1.13\n",
      "[980 | 745.07] loss=1.20 avg=1.13\n",
      "[990 | 752.57] loss=1.23 avg=1.13\n",
      "[1000 | 760.08] loss=1.00 avg=1.13\n",
      "Saving checkpoint/squad_ht_0/model-1000\n",
      "WARNING:tensorflow:From /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/tensorflow_core/python/training/saver.py:963: remove_checkpoint (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use standard file APIs to delete files with this prefix.\n",
      "[1010 | 769.11] loss=1.36 avg=1.13\n",
      "[1020 | 776.61] loss=1.24 avg=1.13\n",
      "[1030 | 784.11] loss=1.07 avg=1.13\n",
      "[1040 | 791.60] loss=0.97 avg=1.13\n",
      "[1050 | 799.10] loss=0.88 avg=1.13\n",
      "[1060 | 806.60] loss=0.89 avg=1.12\n",
      "[1070 | 814.09] loss=0.89 avg=1.12\n",
      "[1080 | 821.59] loss=1.14 avg=1.12\n",
      "[1090 | 829.08] loss=0.98 avg=1.12\n",
      "[1100 | 836.57] loss=1.02 avg=1.12\n",
      "[1110 | 844.08] loss=0.93 avg=1.11\n",
      "[1120 | 851.58] loss=1.10 avg=1.11\n",
      "[1130 | 859.08] loss=1.20 avg=1.11\n",
      "[1140 | 866.57] loss=1.29 avg=1.12\n",
      "[1150 | 874.07] loss=1.05 avg=1.12\n",
      "[1160 | 881.57] loss=1.22 avg=1.12\n",
      "[1170 | 889.06] loss=1.15 avg=1.12\n",
      "[1180 | 896.56] loss=1.04 avg=1.12\n",
      "[1190 | 904.05] loss=1.10 avg=1.12\n",
      "[1200 | 911.53] loss=1.04 avg=1.12\n",
      "[1210 | 919.03] loss=1.17 avg=1.12\n",
      "[1220 | 926.51] loss=0.93 avg=1.11\n",
      "[1230 | 933.99] loss=1.10 avg=1.11\n",
      "[1240 | 941.49] loss=1.00 avg=1.11\n",
      "[1250 | 949.02] loss=1.02 avg=1.11\n",
      "[1260 | 956.52] loss=1.49 avg=1.12\n",
      "[1270 | 964.02] loss=0.97 avg=1.11\n",
      "[1280 | 971.51] loss=1.10 avg=1.11\n",
      "[1290 | 979.02] loss=1.12 avg=1.11\n",
      "[1300 | 986.51] loss=1.15 avg=1.11\n",
      "[1310 | 994.00] loss=1.00 avg=1.11\n",
      "[1320 | 1001.49] loss=1.43 avg=1.12\n",
      "[1330 | 1008.99] loss=1.43 avg=1.12\n",
      "[1340 | 1016.49] loss=1.02 avg=1.12\n",
      "[1350 | 1023.98] loss=1.05 avg=1.12\n",
      "[1360 | 1031.46] loss=1.02 avg=1.12\n",
      "[1370 | 1038.97] loss=1.03 avg=1.12\n",
      "[1380 | 1046.47] loss=1.11 avg=1.12\n",
      "[1390 | 1053.95] loss=1.16 avg=1.12\n",
      "[1400 | 1061.44] loss=1.09 avg=1.12\n",
      "[1410 | 1068.93] loss=1.13 avg=1.12\n",
      "[1420 | 1076.43] loss=1.19 avg=1.12\n",
      "[1430 | 1083.92] loss=0.88 avg=1.11\n",
      "[1440 | 1091.39] loss=1.15 avg=1.12\n",
      "[1450 | 1098.88] loss=1.21 avg=1.12\n",
      "[1460 | 1106.37] loss=0.95 avg=1.11\n",
      "[1470 | 1113.86] loss=1.09 avg=1.11\n",
      "[1480 | 1121.35] loss=1.03 avg=1.11\n",
      "[1490 | 1128.88] loss=1.05 avg=1.11\n",
      "[1500 | 1136.37] loss=1.16 avg=1.11\n",
      "Saving checkpoint/squad_ht_0/model-1500\n",
      "[1510 | 1145.36] loss=1.07 avg=1.11\n",
      "[1520 | 1152.85] loss=0.99 avg=1.11\n",
      "[1530 | 1160.35] loss=1.09 avg=1.11\n",
      "[1540 | 1167.84] loss=1.13 avg=1.11\n",
      "[1550 | 1175.33] loss=1.09 avg=1.11\n",
      "[1560 | 1182.81] loss=0.97 avg=1.11\n",
      "[1570 | 1190.29] loss=1.09 avg=1.11\n",
      "[1580 | 1197.78] loss=1.12 avg=1.11\n",
      "[1590 | 1205.26] loss=0.92 avg=1.11\n",
      "[1600 | 1212.75] loss=1.18 avg=1.11\n",
      "[1610 | 1220.24] loss=1.02 avg=1.11\n",
      "[1620 | 1227.73] loss=1.17 avg=1.11\n",
      "[1630 | 1235.21] loss=1.17 avg=1.11\n",
      "[1640 | 1242.69] loss=0.92 avg=1.11\n",
      "[1650 | 1250.19] loss=1.25 avg=1.11\n",
      "[1660 | 1257.69] loss=0.94 avg=1.11\n",
      "[1670 | 1265.19] loss=0.99 avg=1.10\n",
      "[1680 | 1272.68] loss=0.89 avg=1.10\n",
      "[1690 | 1280.17] loss=0.89 avg=1.10\n",
      "[1700 | 1287.65] loss=0.99 avg=1.10\n",
      "[1710 | 1295.13] loss=1.19 avg=1.10\n",
      "[1720 | 1302.61] loss=1.22 avg=1.10\n",
      "[1730 | 1310.13] loss=1.05 avg=1.10\n",
      "[1740 | 1317.61] loss=0.83 avg=1.10\n",
      "[1750 | 1325.10] loss=0.91 avg=1.09\n",
      "[1760 | 1332.60] loss=1.09 avg=1.09\n",
      "[1770 | 1340.09] loss=1.15 avg=1.09\n",
      "[1780 | 1347.58] loss=1.28 avg=1.10\n",
      "[1790 | 1355.08] loss=1.27 avg=1.10\n",
      "[1800 | 1362.57] loss=1.04 avg=1.10\n",
      "[1810 | 1370.06] loss=0.99 avg=1.10\n",
      "[1820 | 1377.55] loss=1.06 avg=1.10\n",
      "[1830 | 1385.05] loss=1.00 avg=1.10\n",
      "[1840 | 1392.55] loss=1.28 avg=1.10\n",
      "[1850 | 1400.05] loss=1.31 avg=1.10\n",
      "[1860 | 1407.54] loss=1.11 avg=1.10\n",
      "[1870 | 1415.04] loss=0.93 avg=1.10\n",
      "[1880 | 1422.54] loss=0.93 avg=1.10\n",
      "[1890 | 1430.04] loss=0.91 avg=1.09\n",
      "[1900 | 1437.53] loss=1.20 avg=1.09\n",
      "[1910 | 1445.01] loss=1.06 avg=1.09\n",
      "[1920 | 1452.50] loss=0.93 avg=1.09\n",
      "[1930 | 1459.99] loss=1.15 avg=1.09\n",
      "[1940 | 1467.48] loss=1.29 avg=1.10\n",
      "[1950 | 1474.94] loss=1.20 avg=1.10\n",
      "[1960 | 1482.43] loss=1.08 avg=1.10\n",
      "[1970 | 1489.95] loss=1.23 avg=1.10\n",
      "[1980 | 1497.43] loss=1.15 avg=1.10\n",
      "[1990 | 1504.93] loss=0.90 avg=1.10\n",
      "[2000 | 1512.42] loss=0.93 avg=1.09\n",
      "Saving checkpoint/squad_ht_0/model-2000\n",
      "Loading checkpoint models/124M/model.ckpt\n",
      "INFO:tensorflow:Restoring parameters from models/124M/model.ckpt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [01:23<00:00, 83.04s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset has 16793911 tokens\n",
      "Training...\n",
      "[10 | 13.88] loss=1.19 avg=1.19\n",
      "[20 | 21.28] loss=1.09 avg=1.14\n",
      "[30 | 28.68] loss=1.54 avg=1.28\n",
      "[40 | 36.07] loss=1.11 avg=1.24\n",
      "[50 | 43.47] loss=0.95 avg=1.18\n",
      "[60 | 50.92] loss=1.16 avg=1.17\n",
      "[70 | 58.31] loss=1.38 avg=1.20\n",
      "[80 | 65.72] loss=0.99 avg=1.18\n",
      "[90 | 73.14] loss=1.47 avg=1.21\n",
      "[100 | 80.57] loss=1.26 avg=1.22\n",
      "[110 | 87.99] loss=1.19 avg=1.21\n",
      "[120 | 95.42] loss=1.37 avg=1.23\n",
      "[130 | 102.84] loss=1.11 avg=1.22\n",
      "[140 | 110.27] loss=1.13 avg=1.21\n",
      "[150 | 117.70] loss=1.13 avg=1.21\n",
      "[160 | 125.15] loss=1.26 avg=1.21\n",
      "[170 | 132.59] loss=1.02 avg=1.20\n",
      "[180 | 140.06] loss=1.04 avg=1.19\n",
      "[190 | 147.51] loss=1.31 avg=1.19\n",
      "[200 | 154.96] loss=1.15 avg=1.19\n",
      "[210 | 162.41] loss=0.99 avg=1.18\n",
      "[220 | 169.87] loss=1.10 avg=1.18\n",
      "[230 | 177.32] loss=1.07 avg=1.17\n",
      "[240 | 184.77] loss=0.98 avg=1.16\n",
      "[250 | 192.22] loss=1.08 avg=1.16\n",
      "[260 | 199.68] loss=1.36 avg=1.17\n",
      "[270 | 207.13] loss=1.15 avg=1.17\n",
      "[280 | 214.58] loss=1.16 avg=1.17\n",
      "[290 | 222.03] loss=0.97 avg=1.16\n",
      "[300 | 229.49] loss=1.17 avg=1.16\n",
      "[310 | 236.98] loss=1.28 avg=1.16\n",
      "[320 | 244.44] loss=1.23 avg=1.17\n",
      "[330 | 251.91] loss=1.08 avg=1.16\n",
      "[340 | 259.38] loss=1.34 avg=1.17\n",
      "[350 | 266.85] loss=1.25 avg=1.17\n",
      "[360 | 274.30] loss=1.11 avg=1.17\n",
      "[370 | 281.76] loss=0.95 avg=1.16\n",
      "[380 | 289.21] loss=1.20 avg=1.16\n",
      "[390 | 296.66] loss=1.08 avg=1.16\n",
      "[400 | 304.12] loss=1.04 avg=1.16\n",
      "[410 | 311.60] loss=1.15 avg=1.16\n",
      "[420 | 319.07] loss=1.07 avg=1.15\n",
      "[430 | 326.55] loss=1.29 avg=1.16\n",
      "[440 | 334.04] loss=1.43 avg=1.17\n",
      "[450 | 341.51] loss=1.19 avg=1.17\n",
      "[460 | 348.96] loss=1.17 avg=1.17\n",
      "[470 | 356.43] loss=1.27 avg=1.17\n",
      "[480 | 363.90] loss=1.13 avg=1.17\n",
      "[490 | 371.36] loss=1.31 avg=1.17\n",
      "[500 | 378.81] loss=1.06 avg=1.17\n",
      "Saving checkpoint/squad_ht_1/model-500\n",
      "[510 | 388.63] loss=1.12 avg=1.17\n",
      "[520 | 396.08] loss=1.31 avg=1.17\n",
      "[530 | 403.53] loss=1.25 avg=1.17\n",
      "[540 | 411.01] loss=1.12 avg=1.17\n",
      "[550 | 418.48] loss=1.37 avg=1.18\n",
      "[560 | 425.94] loss=0.86 avg=1.17\n",
      "[570 | 433.40] loss=1.23 avg=1.17\n",
      "[580 | 440.87] loss=0.99 avg=1.17\n",
      "[590 | 448.33] loss=1.26 avg=1.17\n",
      "[600 | 455.80] loss=1.52 avg=1.18\n",
      "[610 | 463.26] loss=1.12 avg=1.18\n",
      "[620 | 470.71] loss=1.22 avg=1.18\n",
      "[630 | 478.20] loss=1.12 avg=1.17\n",
      "[640 | 485.67] loss=1.32 avg=1.18\n",
      "[650 | 493.14] loss=1.14 avg=1.18\n",
      "[660 | 500.60] loss=0.80 avg=1.17\n",
      "[670 | 508.05] loss=1.07 avg=1.17\n",
      "[680 | 515.52] loss=1.20 avg=1.17\n",
      "[690 | 522.98] loss=1.18 avg=1.17\n",
      "[700 | 530.44] loss=1.02 avg=1.17\n",
      "[710 | 537.91] loss=1.02 avg=1.16\n",
      "[720 | 545.39] loss=1.32 avg=1.17\n",
      "[730 | 552.86] loss=1.05 avg=1.16\n",
      "[740 | 560.33] loss=0.85 avg=1.16\n",
      "[750 | 567.81] loss=1.25 avg=1.16\n",
      "[760 | 575.29] loss=0.95 avg=1.15\n",
      "[770 | 582.77] loss=0.98 avg=1.15\n",
      "[780 | 590.28] loss=1.05 avg=1.15\n",
      "[790 | 597.74] loss=0.99 avg=1.15\n",
      "[800 | 605.21] loss=1.24 avg=1.15\n",
      "[810 | 612.67] loss=1.19 avg=1.15\n",
      "[820 | 620.15] loss=1.03 avg=1.15\n",
      "[830 | 627.63] loss=1.05 avg=1.15\n",
      "[840 | 635.11] loss=1.04 avg=1.14\n",
      "[850 | 642.58] loss=1.06 avg=1.14\n",
      "[860 | 650.06] loss=1.17 avg=1.14\n",
      "[870 | 657.54] loss=1.27 avg=1.14\n",
      "[880 | 665.01] loss=1.10 avg=1.14\n",
      "[890 | 672.48] loss=1.04 avg=1.14\n",
      "[900 | 679.95] loss=1.09 avg=1.14\n",
      "[910 | 687.42] loss=1.07 avg=1.14\n",
      "[920 | 694.88] loss=0.99 avg=1.14\n",
      "[930 | 702.36] loss=1.35 avg=1.14\n",
      "[940 | 709.84] loss=1.11 avg=1.14\n",
      "[950 | 717.30] loss=1.12 avg=1.14\n",
      "[960 | 724.78] loss=1.28 avg=1.14\n",
      "[970 | 732.25] loss=1.08 avg=1.14\n",
      "[980 | 739.72] loss=0.96 avg=1.14\n",
      "[990 | 747.19] loss=1.16 avg=1.14\n",
      "[1000 | 754.67] loss=1.10 avg=1.14\n",
      "Saving checkpoint/squad_ht_1/model-1000\n",
      "[1010 | 763.63] loss=0.93 avg=1.13\n",
      "[1020 | 771.16] loss=1.07 avg=1.13\n",
      "[1030 | 778.62] loss=1.12 avg=1.13\n",
      "[1040 | 786.08] loss=1.15 avg=1.13\n",
      "[1050 | 793.53] loss=1.02 avg=1.13\n",
      "[1060 | 801.00] loss=1.22 avg=1.13\n",
      "[1070 | 808.48] loss=1.02 avg=1.13\n",
      "[1080 | 815.96] loss=1.39 avg=1.14\n",
      "[1090 | 823.42] loss=1.11 avg=1.14\n",
      "[1100 | 830.88] loss=0.97 avg=1.13\n",
      "[1110 | 838.36] loss=0.98 avg=1.13\n",
      "[1120 | 845.83] loss=1.13 avg=1.13\n",
      "[1130 | 853.31] loss=0.77 avg=1.13\n",
      "[1140 | 860.79] loss=1.26 avg=1.13\n",
      "[1150 | 868.27] loss=0.92 avg=1.12\n",
      "[1160 | 875.74] loss=1.04 avg=1.12\n",
      "[1170 | 883.20] loss=1.05 avg=1.12\n",
      "[1180 | 890.65] loss=1.24 avg=1.12\n",
      "[1190 | 898.11] loss=1.00 avg=1.12\n",
      "[1200 | 905.58] loss=1.02 avg=1.12\n",
      "[1210 | 913.06] loss=1.12 avg=1.12\n",
      "[1220 | 920.54] loss=1.01 avg=1.12\n",
      "[1230 | 928.01] loss=1.25 avg=1.12\n",
      "[1240 | 935.49] loss=0.87 avg=1.12\n",
      "[1250 | 942.95] loss=1.05 avg=1.12\n",
      "[1260 | 950.47] loss=1.17 avg=1.12\n",
      "[1270 | 957.93] loss=1.15 avg=1.12\n",
      "[1280 | 965.41] loss=0.85 avg=1.11\n",
      "[1290 | 972.87] loss=1.06 avg=1.11\n",
      "[1300 | 980.35] loss=1.05 avg=1.11\n",
      "[1310 | 987.82] loss=1.22 avg=1.11\n",
      "[1320 | 995.29] loss=1.19 avg=1.11\n",
      "[1330 | 1002.75] loss=0.94 avg=1.11\n",
      "[1340 | 1010.23] loss=0.92 avg=1.11\n",
      "[1350 | 1017.70] loss=0.96 avg=1.11\n",
      "[1360 | 1025.15] loss=1.03 avg=1.11\n",
      "[1370 | 1032.61] loss=0.96 avg=1.10\n",
      "[1380 | 1040.07] loss=1.14 avg=1.11\n",
      "[1390 | 1047.54] loss=1.06 avg=1.10\n",
      "[1400 | 1054.99] loss=1.30 avg=1.11\n",
      "[1410 | 1062.45] loss=1.16 avg=1.11\n",
      "[1420 | 1069.91] loss=1.01 avg=1.11\n",
      "[1430 | 1077.38] loss=1.28 avg=1.11\n",
      "[1440 | 1084.84] loss=0.88 avg=1.11\n",
      "[1450 | 1092.30] loss=0.93 avg=1.10\n",
      "[1460 | 1099.78] loss=1.12 avg=1.10\n",
      "[1470 | 1107.25] loss=1.27 avg=1.11\n",
      "[1480 | 1114.72] loss=1.10 avg=1.11\n",
      "[1490 | 1122.18] loss=1.12 avg=1.11\n",
      "[1500 | 1129.66] loss=1.08 avg=1.11\n",
      "Saving checkpoint/squad_ht_1/model-1500\n",
      "[1510 | 1138.62] loss=0.99 avg=1.10\n",
      "[1520 | 1146.07] loss=1.02 avg=1.10\n",
      "[1530 | 1153.53] loss=1.13 avg=1.10\n",
      "[1540 | 1161.00] loss=1.21 avg=1.10\n",
      "[1550 | 1168.48] loss=1.10 avg=1.10\n",
      "[1560 | 1175.93] loss=1.37 avg=1.11\n",
      "[1570 | 1183.38] loss=1.04 avg=1.11\n",
      "[1580 | 1190.85] loss=1.21 avg=1.11\n",
      "[1590 | 1198.33] loss=0.98 avg=1.11\n",
      "[1600 | 1205.80] loss=0.93 avg=1.10\n",
      "[1610 | 1213.25] loss=1.07 avg=1.10\n",
      "[1620 | 1220.71] loss=1.09 avg=1.10\n",
      "[1630 | 1228.17] loss=1.53 avg=1.11\n",
      "[1640 | 1235.64] loss=1.08 avg=1.11\n",
      "[1650 | 1243.11] loss=1.29 avg=1.11\n",
      "[1660 | 1250.58] loss=1.30 avg=1.11\n",
      "[1670 | 1258.04] loss=1.33 avg=1.12\n",
      "[1680 | 1265.50] loss=1.14 avg=1.12\n",
      "[1690 | 1272.96] loss=1.34 avg=1.12\n",
      "[1700 | 1280.43] loss=1.03 avg=1.12\n",
      "[1710 | 1287.90] loss=1.06 avg=1.12\n",
      "[1720 | 1295.36] loss=1.07 avg=1.12\n",
      "[1730 | 1302.83] loss=1.05 avg=1.12\n",
      "[1740 | 1310.33] loss=1.05 avg=1.12\n",
      "[1750 | 1317.79] loss=1.01 avg=1.11\n",
      "[1760 | 1325.25] loss=1.18 avg=1.12\n",
      "[1770 | 1332.72] loss=1.33 avg=1.12\n",
      "[1780 | 1340.19] loss=0.93 avg=1.12\n",
      "[1790 | 1347.66] loss=1.20 avg=1.12\n",
      "[1800 | 1355.11] loss=1.10 avg=1.12\n",
      "[1810 | 1362.57] loss=1.31 avg=1.12\n",
      "[1820 | 1370.03] loss=1.05 avg=1.12\n",
      "[1830 | 1377.51] loss=1.19 avg=1.12\n",
      "[1840 | 1384.96] loss=1.22 avg=1.12\n",
      "[1850 | 1392.43] loss=1.04 avg=1.12\n",
      "[1860 | 1399.89] loss=0.91 avg=1.12\n",
      "[1870 | 1407.35] loss=1.08 avg=1.12\n",
      "[1880 | 1414.80] loss=1.11 avg=1.12\n",
      "[1890 | 1422.25] loss=1.00 avg=1.11\n",
      "[1900 | 1429.72] loss=1.20 avg=1.12\n",
      "[1910 | 1437.19] loss=1.44 avg=1.12\n",
      "[1920 | 1444.65] loss=1.11 avg=1.12\n",
      "[1930 | 1452.13] loss=1.14 avg=1.12\n",
      "[1940 | 1459.60] loss=1.05 avg=1.12\n",
      "[1950 | 1467.07] loss=1.12 avg=1.12\n",
      "[1960 | 1474.54] loss=1.15 avg=1.12\n",
      "[1970 | 1482.01] loss=1.19 avg=1.12\n",
      "[1980 | 1489.47] loss=0.87 avg=1.12\n",
      "[1990 | 1496.95] loss=1.15 avg=1.12\n",
      "[2000 | 1504.41] loss=1.21 avg=1.12\n",
      "Saving checkpoint/squad_ht_1/model-2000\n",
      "======== SAMPLE 1 ========\n",
      " low-intensity musical instrumentation than music, which has traditionally been regarded as a skill. The sound of pianos is also an important part of musical training. One reason why pianos are the most widely recorded musical instrument in Europe may be that the sound they produce is also a dominant factor in many types of training. As a result, training is more demanding and stressful for players due to the ability to operate such a complex instrument complex training will entail a significant amount of learning that can be costly and time consuming. It also results in an injury risk that is higher than when playing a traditional solo piano. [QUESTION]: Do professional pianos require more stress or less? <|endoftext|>\n",
      "<|startoftext|>\n",
      "[CONTEXT]: Modern piano training may require several additional aspects than usual. The first is the use of the pianistic equipment that is used. The techniques are developed from an actual study of the piano and its parts. Many techniques, as well as many instrumental instruments are practiced by people who are not trained musicians. The use of the piano equipment or the piano parts is a significant skill in many situations in which the playing is very natural and a significant part of playing. It is also the major reason why many skills such as musical instrumentation or fingerwork are very important in modern life. The development of a piano may be a significant learning opportunity and the training is also stressful for the pianisticists who use such instruments. However, this learning opportunity may last a lifetime. The piano can play very complex musical instruments even by those who are not trained musicians. In order for a pianist to operate a traditional piano, one needs to be able to operate hundreds of different instruments with very complex tuning. It is not the only way to play, though, pianists should explore different musical styles, playing technique, and playing repertoire. [QUESTION]: How many different pianos use the piano for playing? <|endoftext|>\n",
      "<|startoftext|>\n",
      "[CONTEXT]: Modern piano training may require several additional aspects than usual. The first is the use of the pianistic equipment that is used. The techniques are developed from an actual study of the piano and its parts. Many techniques, as well as many instrumental instruments are practiced by people who are not trained musicians. The use of the piano equipment or the piano parts is a significant skill in many situations in which the playing is very natural and a significant part of playing. It is also the major reason why many skills such as musical instrumentation or fingerwork are very important in modern life. The development of a piano may be a significant learning opportunity and the training is also stressful for the pianisticists who use such instruments. However, this learning opportunity may last a lifetime. The piano can play very complex musical instruments even by those who are not trained musicians. In order for a pianist to operate a traditional piano, one needs to be able to operate hundreds of different instruments with very complex tuning. It is not the only way to play, though, pianists should explore different musical styles, playing technique, and playing repertoire. [QUESTION]: What type of instrument are two common examples of examples of? <|endoftext|>\n",
      "<|startoftext|>\n",
      "[CONTEXT]: Performance involves the performance of a performance piece (whether playing the piece as a live version of traditional playing technique or the piece or solo) as many times as possible. The purpose is to offer a context of play when performing, and the performances are typically a collection of a single piece (typically a piano, violin, mandolin, mandolin cello, recorder, or mandolin concertos, though sometimes many more). Many classical classical composers, including Chopin's cello, Mozart and Liszt, have used the idea (or technique) of the performance to provide classical music to their audiences. The purpose that the performance of a piece allows for the audience to hear is for the benefit of the performer for both the performance of a standard piece (the piece or solo) and the performance of the performance or solo performed as part of the performance itself (a solo and the piano). Some performing pieces may also offer a particular performance technique or a particular performance technique for the performance of a piece performed by one individual individual, or solo performances which are performed by groups of groups. Composers such as Bach, Chopin, Liszt, Mahler, and Mozart used the idea (or technique) of the performance to provide classical music to their audiences. These works were a form of play, but they did not offer real performance and a real understanding of the technique in order to provide a viable performance setting. Many performance techniques require a clear understanding of the performance to recognize the unique musical components of the piece, a skill that can benefit many different performers. Many classical pieces offer a specific performance technique in which a sequence of steps, each performed within a sequence, is known as the piece. An ensemble of musicians perform an entire solo piece, or solo\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2010 | 1527.31] loss=0.96 avg=1.12\n",
      "[2020 | 1534.77] loss=1.03 avg=1.12\n",
      "[2030 | 1542.21] loss=1.13 avg=1.12\n",
      "[2040 | 1549.65] loss=0.87 avg=1.11\n",
      "[2050 | 1557.09] loss=1.06 avg=1.11\n",
      "[2060 | 1564.53] loss=1.00 avg=1.11\n",
      "[2070 | 1571.97] loss=0.77 avg=1.11\n",
      "[2080 | 1579.42] loss=1.13 avg=1.11\n",
      "[2090 | 1586.86] loss=1.05 avg=1.11\n",
      "[2100 | 1594.30] loss=0.93 avg=1.10\n",
      "[2110 | 1601.75] loss=1.17 avg=1.11\n",
      "[2120 | 1609.21] loss=1.12 avg=1.11\n",
      "[2130 | 1616.67] loss=0.94 avg=1.10\n",
      "[2140 | 1624.14] loss=1.10 avg=1.10\n",
      "[2150 | 1631.60] loss=1.03 avg=1.10\n",
      "[2160 | 1639.06] loss=1.06 avg=1.10\n",
      "[2170 | 1646.52] loss=0.82 avg=1.10\n",
      "[2180 | 1653.98] loss=1.08 avg=1.10\n",
      "[2190 | 1661.44] loss=1.22 avg=1.10\n",
      "[2200 | 1668.91] loss=1.04 avg=1.10\n",
      "[2210 | 1676.42] loss=1.29 avg=1.10\n",
      "[2220 | 1683.88] loss=1.29 avg=1.10\n",
      "[2230 | 1691.34] loss=1.03 avg=1.10\n",
      "[2240 | 1698.81] loss=1.07 avg=1.10\n",
      "[2250 | 1706.28] loss=0.97 avg=1.10\n",
      "[2260 | 1713.74] loss=1.15 avg=1.10\n",
      "[2270 | 1721.20] loss=0.90 avg=1.10\n",
      "[2280 | 1728.67] loss=0.97 avg=1.10\n",
      "[2290 | 1736.13] loss=1.10 avg=1.10\n",
      "[2300 | 1743.58] loss=1.06 avg=1.10\n",
      "[2310 | 1751.03] loss=1.20 avg=1.10\n",
      "[2320 | 1758.49] loss=1.00 avg=1.10\n",
      "[2330 | 1765.96] loss=1.21 avg=1.10\n",
      "[2340 | 1773.43] loss=0.75 avg=1.10\n",
      "[2350 | 1780.90] loss=0.97 avg=1.09\n",
      "[2360 | 1788.36] loss=1.40 avg=1.10\n",
      "[2370 | 1795.80] loss=1.24 avg=1.10\n",
      "[2380 | 1803.25] loss=1.01 avg=1.10\n",
      "[2390 | 1810.71] loss=1.11 avg=1.10\n",
      "[2400 | 1818.16] loss=1.08 avg=1.10\n",
      "[2410 | 1825.62] loss=1.06 avg=1.10\n",
      "[2420 | 1833.07] loss=1.09 avg=1.10\n",
      "[2430 | 1840.53] loss=0.90 avg=1.10\n",
      "[2440 | 1848.00] loss=0.96 avg=1.09\n",
      "[2450 | 1855.49] loss=0.97 avg=1.09\n",
      "[2460 | 1862.94] loss=1.14 avg=1.09\n",
      "[2470 | 1870.41] loss=1.18 avg=1.09\n",
      "[2480 | 1877.88] loss=1.07 avg=1.09\n",
      "[2490 | 1885.34] loss=0.88 avg=1.09\n",
      "[2500 | 1892.81] loss=1.06 avg=1.09\n",
      "Saving checkpoint/squad_ht_1/model-2500\n",
      "[2510 | 1901.82] loss=0.84 avg=1.09\n",
      "[2520 | 1909.29] loss=0.92 avg=1.09\n",
      "[2530 | 1916.75] loss=0.93 avg=1.08\n",
      "[2540 | 1924.22] loss=1.12 avg=1.09\n",
      "[2550 | 1931.70] loss=1.03 avg=1.08\n",
      "[2560 | 1939.16] loss=1.20 avg=1.09\n",
      "[2570 | 1946.62] loss=1.21 avg=1.09\n",
      "[2580 | 1954.07] loss=0.80 avg=1.08\n",
      "[2590 | 1961.53] loss=1.07 avg=1.08\n",
      "[2600 | 1968.97] loss=1.08 avg=1.08\n",
      "[2610 | 1976.45] loss=1.08 avg=1.08\n",
      "[2620 | 1983.91] loss=1.08 avg=1.08\n",
      "[2630 | 1991.36] loss=1.31 avg=1.09\n",
      "[2640 | 1998.83] loss=1.07 avg=1.09\n",
      "[2650 | 2006.29] loss=1.18 avg=1.09\n",
      "[2660 | 2013.75] loss=1.05 avg=1.09\n",
      "[2670 | 2021.20] loss=1.06 avg=1.09\n",
      "[2680 | 2028.66] loss=1.13 avg=1.09\n",
      "[2690 | 2036.17] loss=0.87 avg=1.08\n",
      "[2700 | 2043.64] loss=0.81 avg=1.08\n",
      "[2710 | 2051.11] loss=1.03 avg=1.08\n",
      "[2720 | 2058.59] loss=1.24 avg=1.08\n",
      "[2730 | 2066.07] loss=1.05 avg=1.08\n",
      "[2740 | 2073.53] loss=1.03 avg=1.08\n",
      "[2750 | 2081.00] loss=1.07 avg=1.08\n",
      "[2760 | 2088.48] loss=0.81 avg=1.08\n",
      "[2770 | 2095.95] loss=0.98 avg=1.08\n",
      "[2780 | 2103.41] loss=0.90 avg=1.08\n",
      "[2790 | 2110.88] loss=1.01 avg=1.08\n",
      "[2800 | 2118.36] loss=1.01 avg=1.07\n",
      "[2810 | 2125.81] loss=1.11 avg=1.07\n",
      "[2820 | 2133.26] loss=1.11 avg=1.08\n",
      "[2830 | 2140.73] loss=1.08 avg=1.08\n",
      "[2840 | 2148.20] loss=0.82 avg=1.07\n",
      "[2850 | 2155.68] loss=1.07 avg=1.07\n",
      "[2860 | 2163.15] loss=0.97 avg=1.07\n",
      "[2870 | 2170.63] loss=1.11 avg=1.07\n",
      "[2880 | 2178.10] loss=1.07 avg=1.07\n",
      "[2890 | 2185.57] loss=1.09 avg=1.07\n",
      "[2900 | 2193.04] loss=0.90 avg=1.07\n",
      "[2910 | 2200.50] loss=1.04 avg=1.07\n",
      "[2920 | 2207.97] loss=0.81 avg=1.07\n",
      "[2930 | 2215.44] loss=0.82 avg=1.06\n",
      "[2940 | 2222.89] loss=1.00 avg=1.06\n",
      "[2950 | 2230.36] loss=0.87 avg=1.06\n",
      "[2960 | 2237.81] loss=0.96 avg=1.06\n",
      "[2970 | 2245.28] loss=0.85 avg=1.06\n",
      "[2980 | 2252.74] loss=0.92 avg=1.06\n",
      "[2990 | 2260.20] loss=0.98 avg=1.06\n",
      "[3000 | 2267.69] loss=1.20 avg=1.06\n",
      "Saving checkpoint/squad_ht_1/model-3000\n",
      "[3010 | 2276.65] loss=1.18 avg=1.06\n",
      "[3020 | 2284.11] loss=1.06 avg=1.06\n",
      "[3030 | 2291.57] loss=0.74 avg=1.06\n",
      "[3040 | 2299.05] loss=1.15 avg=1.06\n",
      "[3050 | 2306.52] loss=0.93 avg=1.06\n",
      "[3060 | 2313.98] loss=0.95 avg=1.05\n",
      "[3070 | 2321.44] loss=1.13 avg=1.05\n",
      "[3080 | 2328.90] loss=1.10 avg=1.06\n",
      "[3090 | 2336.35] loss=1.03 avg=1.06\n",
      "[3100 | 2343.81] loss=1.13 avg=1.06\n",
      "[3110 | 2351.26] loss=1.05 avg=1.06\n",
      "[3120 | 2358.72] loss=0.94 avg=1.05\n",
      "[3130 | 2366.19] loss=1.07 avg=1.05\n",
      "[3140 | 2373.65] loss=0.99 avg=1.05\n",
      "[3150 | 2381.11] loss=1.00 avg=1.05\n",
      "[50 | 43.94] loss=1.43 avg=1.24\n",
      "[60 | 51.35] loss=1.17 avg=1.23\n",
      "[70 | 58.75] loss=1.04 avg=1.20\n",
      "[80 | 66.16] loss=1.32 avg=1.22\n",
      "[2540 | 1923.17] loss=1.13 avg=1.16\n",
      "[3800 | 2867.51] loss=1.23 avg=1.13\n",
      "[3810 | 2874.99] loss=0.98 avg=1.13\n",
      "[3820 | 2882.47] loss=1.07 avg=1.13\n",
      "[3830 | 2889.94] loss=1.11 avg=1.13\n",
      "[3840 | 2897.42] loss=1.29 avg=1.13\n",
      "[3850 | 2904.88] loss=1.28 avg=1.13\n",
      "[3860 | 2912.36] loss=1.21 avg=1.13\n",
      "[3870 | 2919.84] loss=1.10 avg=1.13\n",
      "[3880 | 2927.32] loss=1.30 avg=1.14\n",
      "[3890 | 2934.79] loss=1.00 avg=1.13\n",
      "[3900 | 2942.27] loss=1.05 avg=1.13\n",
      "[3910 | 2949.75] loss=1.18 avg=1.13\n",
      "[3920 | 2957.23] loss=1.34 avg=1.14\n",
      "[3930 | 2964.70] loss=1.03 avg=1.13\n",
      "[3940 | 2972.18] loss=1.11 avg=1.13\n",
      "[3950 | 2979.65] loss=1.12 avg=1.13\n",
      "[3960 | 2987.13] loss=1.07 avg=1.13\n",
      "[3970 | 2994.61] loss=1.11 avg=1.13\n",
      "[3980 | 3002.09] loss=0.97 avg=1.13\n",
      "[3990 | 3009.56] loss=1.10 avg=1.13\n",
      "[4000 | 3017.04] loss=1.34 avg=1.13\n",
      "Saving checkpoint/squad_ht_3/model-4000\n",
      "Loading checkpoint models/124M/model.ckpt\n",
      "INFO:tensorflow:Restoring parameters from models/124M/model.ckpt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [01:23<00:00, 83.98s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset has 16793911 tokens\n",
      "Training...\n",
      "[10 | 14.00] loss=1.42 avg=1.42\n",
      "[20 | 21.41] loss=1.42 avg=1.42\n",
      "[30 | 28.81] loss=1.45 avg=1.43\n",
      "[40 | 36.22] loss=1.14 avg=1.36\n",
      "[50 | 43.62] loss=1.44 avg=1.37\n",
      "[60 | 51.02] loss=1.05 avg=1.32\n",
      "[70 | 58.43] loss=1.14 avg=1.29\n",
      "[80 | 65.84] loss=1.50 avg=1.32\n",
      "[90 | 73.26] loss=1.42 avg=1.33\n",
      "[100 | 80.68] loss=1.56 avg=1.35\n",
      "[110 | 88.11] loss=1.29 avg=1.35\n",
      "[120 | 95.55] loss=1.03 avg=1.32\n",
      "[130 | 102.98] loss=1.27 avg=1.32\n",
      "[140 | 110.40] loss=1.12 avg=1.30\n",
      "[150 | 117.83] loss=1.13 avg=1.29\n",
      "[160 | 125.30] loss=1.18 avg=1.28\n",
      "[170 | 132.72] loss=1.17 avg=1.28\n",
      "[180 | 140.15] loss=1.19 avg=1.27\n",
      "[190 | 147.60] loss=1.26 avg=1.27\n",
      "[200 | 155.05] loss=1.18 avg=1.26\n",
      "[210 | 162.50] loss=1.32 avg=1.27\n",
      "[220 | 169.96] loss=1.34 avg=1.27\n",
      "[230 | 177.41] loss=1.36 avg=1.27\n",
      "[240 | 184.84] loss=1.04 avg=1.26\n",
      "[250 | 192.29] loss=1.42 avg=1.27\n",
      "[260 | 199.75] loss=1.15 avg=1.27\n",
      "[270 | 207.22] loss=1.42 avg=1.27\n",
      "[280 | 214.69] loss=1.29 avg=1.27\n",
      "[290 | 222.15] loss=1.40 avg=1.28\n",
      "[300 | 229.61] loss=1.25 avg=1.28\n",
      "[310 | 237.07] loss=1.48 avg=1.28\n",
      "[320 | 244.54] loss=1.32 avg=1.29\n",
      "[330 | 252.00] loss=1.14 avg=1.28\n",
      "[340 | 259.46] loss=1.33 avg=1.28\n",
      "[350 | 266.93] loss=1.24 avg=1.28\n",
      "[360 | 274.39] loss=1.31 avg=1.28\n",
      "[370 | 281.84] loss=1.09 avg=1.28\n",
      "[380 | 289.29] loss=1.13 avg=1.27\n",
      "[390 | 296.76] loss=1.27 avg=1.27\n",
      "[400 | 304.25] loss=1.09 avg=1.27\n",
      "[410 | 311.71] loss=1.33 avg=1.27\n",
      "[420 | 319.17] loss=1.17 avg=1.26\n",
      "[430 | 326.63] loss=1.18 avg=1.26\n",
      "[440 | 334.08] loss=1.14 avg=1.26\n",
      "[450 | 341.52] loss=1.46 avg=1.26\n",
      "[460 | 348.99] loss=1.23 avg=1.26\n",
      "[470 | 356.46] loss=1.15 avg=1.26\n",
      "[480 | 363.93] loss=1.11 avg=1.26\n",
      "[490 | 371.39] loss=1.29 avg=1.26\n",
      "[500 | 378.85] loss=1.27 avg=1.26\n",
      "Saving checkpoint/squad_ht_4/model-500\n",
      "[510 | 388.33] loss=1.28 avg=1.26\n",
      "[520 | 395.80] loss=1.13 avg=1.26\n",
      "[530 | 403.27] loss=1.57 avg=1.26\n",
      "[540 | 410.73] loss=1.25 avg=1.26\n",
      "[550 | 418.20] loss=1.38 avg=1.27\n",
      "[560 | 425.66] loss=1.26 avg=1.27\n",
      "[570 | 433.12] loss=1.18 avg=1.26\n",
      "[580 | 440.59] loss=1.36 avg=1.27\n",
      "[590 | 448.05] loss=1.23 avg=1.26\n",
      "[600 | 455.51] loss=1.22 avg=1.26\n",
      "[610 | 462.98] loss=1.21 avg=1.26\n",
      "[620 | 470.44] loss=1.22 avg=1.26\n",
      "[630 | 477.91] loss=1.23 avg=1.26\n",
      "[640 | 485.43] loss=1.26 avg=1.26\n",
      "[650 | 492.90] loss=1.19 avg=1.26\n",
      "[660 | 500.37] loss=1.25 avg=1.26\n",
      "[670 | 507.83] loss=1.23 avg=1.26\n",
      "[680 | 515.32] loss=1.09 avg=1.25\n",
      "[690 | 522.78] loss=1.17 avg=1.25\n",
      "[700 | 530.23] loss=1.30 avg=1.25\n",
      "[710 | 537.71] loss=1.13 avg=1.25\n",
      "[720 | 545.17] loss=1.05 avg=1.25\n",
      "[730 | 552.63] loss=1.25 avg=1.25\n",
      "[740 | 560.10] loss=1.23 avg=1.25\n",
      "[750 | 567.54] loss=1.14 avg=1.25\n",
      "[760 | 574.99] loss=1.11 avg=1.24\n",
      "[770 | 582.45] loss=1.23 avg=1.24\n",
      "[780 | 589.91] loss=1.34 avg=1.24\n",
      "[790 | 597.36] loss=1.09 avg=1.24\n",
      "[800 | 604.81] loss=1.46 avg=1.25\n",
      "[810 | 612.28] loss=1.29 avg=1.25\n",
      "[820 | 619.74] loss=1.01 avg=1.24\n",
      "[830 | 627.20] loss=1.16 avg=1.24\n",
      "[840 | 634.68] loss=1.12 avg=1.24\n",
      "[850 | 642.14] loss=1.26 avg=1.24\n",
      "[860 | 649.61] loss=1.20 avg=1.24\n",
      "[870 | 657.08] loss=1.19 avg=1.24\n",
      "[880 | 664.57] loss=1.23 avg=1.24\n",
      "[890 | 672.04] loss=1.17 avg=1.24\n",
      "[900 | 679.51] loss=1.04 avg=1.23\n",
      "[910 | 686.99] loss=1.11 avg=1.23\n",
      "[920 | 694.47] loss=1.18 avg=1.23\n",
      "[930 | 701.92] loss=1.41 avg=1.23\n",
      "[940 | 709.38] loss=1.35 avg=1.24\n",
      "[950 | 716.83] loss=0.96 avg=1.23\n",
      "[960 | 724.27] loss=1.18 avg=1.23\n",
      "[970 | 731.74] loss=1.03 avg=1.23\n",
      "[980 | 739.19] loss=1.11 avg=1.22\n",
      "[990 | 746.64] loss=1.03 avg=1.22\n",
      "[1000 | 754.09] loss=0.99 avg=1.22\n",
      "Saving checkpoint/squad_ht_4/model-1000\n",
      "[1010 | 763.08] loss=1.44 avg=1.22\n",
      "[1020 | 770.54] loss=1.31 avg=1.22\n",
      "[1030 | 778.02] loss=1.19 avg=1.22\n",
      "[1040 | 785.48] loss=1.03 avg=1.22\n",
      "[1050 | 792.92] loss=1.30 avg=1.22\n",
      "[1060 | 800.38] loss=1.47 avg=1.22\n",
      "[1070 | 807.84] loss=1.52 avg=1.23\n",
      "[1080 | 815.31] loss=1.16 avg=1.23\n",
      "[1090 | 822.77] loss=0.99 avg=1.22\n",
      "[1100 | 830.21] loss=1.16 avg=1.22\n",
      "[1110 | 837.66] loss=0.97 avg=1.22\n",
      "[1120 | 845.16] loss=1.21 avg=1.22\n",
      "[1130 | 852.63] loss=1.68 avg=1.23\n",
      "[1140 | 860.10] loss=1.13 avg=1.22\n",
      "[1150 | 867.55] loss=1.00 avg=1.22\n",
      "[1160 | 875.02] loss=1.22 avg=1.22\n",
      "[1170 | 882.48] loss=1.07 avg=1.22\n",
      "[1180 | 889.92] loss=1.22 avg=1.22\n",
      "[1190 | 897.40] loss=1.17 avg=1.22\n",
      "[1200 | 904.87] loss=1.32 avg=1.22\n",
      "[1210 | 912.33] loss=1.24 avg=1.22\n",
      "[1220 | 919.80] loss=1.36 avg=1.22\n",
      "[1230 | 927.27] loss=1.35 avg=1.22\n",
      "[1240 | 934.74] loss=1.30 avg=1.23\n",
      "[1250 | 942.20] loss=0.87 avg=1.22\n",
      "[1260 | 949.67] loss=1.44 avg=1.22\n",
      "[1270 | 957.14] loss=1.49 avg=1.23\n",
      "[1280 | 964.61] loss=1.22 avg=1.23\n",
      "[1290 | 972.08] loss=1.22 avg=1.23\n",
      "[1300 | 979.56] loss=1.18 avg=1.23\n",
      "[1310 | 987.02] loss=1.16 avg=1.23\n",
      "[1320 | 994.49] loss=1.29 avg=1.23\n",
      "[1330 | 1001.96] loss=1.41 avg=1.23\n",
      "[1340 | 1009.44] loss=1.05 avg=1.23\n",
      "[1350 | 1016.90] loss=0.82 avg=1.22\n",
      "[1360 | 1024.39] loss=0.91 avg=1.22\n",
      "[1370 | 1031.87] loss=1.25 avg=1.22\n",
      "[1380 | 1039.34] loss=1.08 avg=1.22\n",
      "[1390 | 1046.80] loss=0.91 avg=1.21\n",
      "[1400 | 1054.28] loss=1.06 avg=1.21\n",
      "[1410 | 1061.75] loss=1.06 avg=1.21\n",
      "[1420 | 1069.21] loss=1.20 avg=1.21\n",
      "[1430 | 1076.68] loss=1.07 avg=1.21\n",
      "[1440 | 1084.16] loss=1.03 avg=1.20\n",
      "[1450 | 1091.63] loss=1.32 avg=1.20\n",
      "[1460 | 1099.09] loss=1.19 avg=1.20\n",
      "[1470 | 1106.56] loss=1.06 avg=1.20\n",
      "[1480 | 1114.04] loss=1.30 avg=1.20\n",
      "[1490 | 1121.51] loss=1.38 avg=1.21\n",
      "[1500 | 1128.98] loss=1.20 avg=1.21\n",
      "Saving checkpoint/squad_ht_4/model-1500\n",
      "[1510 | 1137.99] loss=1.14 avg=1.20\n",
      "[1520 | 1145.46] loss=1.28 avg=1.21\n",
      "[1530 | 1152.94] loss=1.43 avg=1.21\n",
      "[1540 | 1160.40] loss=1.25 avg=1.21\n",
      "[1550 | 1167.84] loss=1.41 avg=1.21\n",
      "[1560 | 1175.33] loss=1.18 avg=1.21\n",
      "[1570 | 1182.80] loss=1.33 avg=1.21\n",
      "[1580 | 1190.28] loss=1.12 avg=1.21\n",
      "[1590 | 1197.74] loss=1.06 avg=1.21\n",
      "[1600 | 1205.22] loss=0.98 avg=1.21\n",
      "[1610 | 1212.69] loss=1.16 avg=1.21\n",
      "[1620 | 1220.16] loss=1.32 avg=1.21\n",
      "[1630 | 1227.61] loss=1.09 avg=1.21\n",
      "[1640 | 1235.07] loss=1.09 avg=1.20\n",
      "[1650 | 1242.54] loss=1.41 avg=1.21\n",
      "[1660 | 1250.01] loss=1.14 avg=1.21\n",
      "[1670 | 1257.49] loss=1.12 avg=1.21\n",
      "[1680 | 1264.96] loss=1.07 avg=1.20\n",
      "[1690 | 1272.43] loss=1.34 avg=1.21\n",
      "[1700 | 1279.90] loss=1.08 avg=1.20\n",
      "[1710 | 1287.36] loss=1.20 avg=1.20\n",
      "[1720 | 1294.83] loss=1.13 avg=1.20\n",
      "[1730 | 1302.30] loss=1.23 avg=1.20\n",
      "[1740 | 1309.77] loss=1.25 avg=1.20\n",
      "[1750 | 1317.23] loss=1.25 avg=1.20\n",
      "[1760 | 1324.70] loss=1.24 avg=1.20\n",
      "[1770 | 1332.16] loss=1.12 avg=1.20\n",
      "[1780 | 1339.64] loss=1.16 avg=1.20\n",
      "[1790 | 1347.11] loss=1.11 avg=1.20\n",
      "[1800 | 1354.59] loss=1.00 avg=1.20\n",
      "[1810 | 1362.06] loss=1.38 avg=1.20\n",
      "[1820 | 1369.53] loss=1.16 avg=1.20\n",
      "[1830 | 1377.00] loss=1.40 avg=1.20\n",
      "[1840 | 1384.48] loss=1.15 avg=1.20\n",
      "[1850 | 1391.94] loss=1.28 avg=1.20\n",
      "[1860 | 1399.40] loss=1.41 avg=1.21\n",
      "[1870 | 1406.87] loss=1.01 avg=1.20\n",
      "[1880 | 1414.35] loss=1.30 avg=1.20\n",
      "[1890 | 1421.82] loss=1.18 avg=1.20\n",
      "[1900 | 1429.29] loss=1.32 avg=1.21\n",
      "[1910 | 1436.75] loss=1.54 avg=1.21\n",
      "[1920 | 1444.22] loss=1.56 avg=1.21\n",
      "[1930 | 1451.69] loss=1.44 avg=1.22\n",
      "[1940 | 1459.13] loss=1.31 avg=1.22\n",
      "[1950 | 1466.59] loss=1.19 avg=1.22\n",
      "[1960 | 1474.06] loss=1.35 avg=1.22\n",
      "[1970 | 1481.53] loss=1.07 avg=1.22\n",
      "[1980 | 1489.00] loss=1.15 avg=1.22\n",
      "[1990 | 1496.47] loss=1.07 avg=1.21\n",
      "[2000 | 1503.92] loss=1.27 avg=1.22\n",
      "Saving checkpoint/squad_ht_4/model-2000\n",
      "Loading checkpoint models/124M/model.ckpt\n",
      "INFO:tensorflow:Restoring parameters from models/124M/model.ckpt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [01:23<00:00, 83.15s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset has 16793911 tokens\n",
      "Training...\n",
      "[10 | 14.04] loss=1.39 avg=1.39\n",
      "[20 | 21.45] loss=1.53 avg=1.46\n",
      "[30 | 28.84] loss=1.28 avg=1.40\n",
      "[40 | 36.26] loss=1.46 avg=1.41\n",
      "[50 | 43.67] loss=1.38 avg=1.41\n",
      "[60 | 51.09] loss=1.48 avg=1.42\n",
      "[70 | 58.52] loss=1.65 avg=1.45\n",
      "[80 | 65.95] loss=1.25 avg=1.43\n",
      "[90 | 73.39] loss=1.38 avg=1.42\n",
      "[100 | 80.82] loss=1.26 avg=1.40\n",
      "[110 | 88.24] loss=1.19 avg=1.38\n",
      "[120 | 95.69] loss=1.22 avg=1.37\n",
      "[130 | 103.14] loss=1.27 avg=1.36\n",
      "[140 | 110.60] loss=1.29 avg=1.36\n",
      "[150 | 118.04] loss=1.28 avg=1.35\n",
      "[160 | 125.50] loss=1.13 avg=1.33\n",
      "[170 | 132.96] loss=1.12 avg=1.32\n",
      "[180 | 140.46] loss=1.17 avg=1.31\n",
      "[190 | 147.92] loss=1.11 avg=1.30\n",
      "[200 | 155.39] loss=1.25 avg=1.30\n",
      "[210 | 162.87] loss=1.20 avg=1.29\n",
      "[220 | 170.35] loss=1.16 avg=1.29\n",
      "[230 | 177.82] loss=1.11 avg=1.28\n",
      "[240 | 185.29] loss=1.25 avg=1.28\n",
      "[250 | 192.76] loss=1.19 avg=1.27\n",
      "[260 | 200.23] loss=1.23 avg=1.27\n",
      "[270 | 207.71] loss=1.25 avg=1.27\n",
      "[280 | 215.19] loss=1.20 avg=1.27\n",
      "[290 | 222.67] loss=1.13 avg=1.26\n",
      "[300 | 230.16] loss=1.37 avg=1.27\n",
      "[310 | 237.64] loss=1.18 avg=1.26\n",
      "[320 | 245.12] loss=1.28 avg=1.26\n",
      "[330 | 252.60] loss=0.99 avg=1.25\n",
      "[340 | 260.08] loss=1.01 avg=1.24\n",
      "[350 | 267.57] loss=1.08 avg=1.24\n",
      "[360 | 275.06] loss=1.10 avg=1.23\n",
      "[370 | 282.54] loss=0.98 avg=1.23\n",
      "[380 | 290.03] loss=1.56 avg=1.24\n",
      "[390 | 297.51] loss=1.31 avg=1.24\n",
      "[400 | 304.99] loss=0.88 avg=1.23\n",
      "[410 | 312.48] loss=1.05 avg=1.22\n",
      "[420 | 320.00] loss=1.20 avg=1.22\n",
      "[430 | 327.48] loss=1.32 avg=1.22\n",
      "[440 | 334.97] loss=1.18 avg=1.22\n",
      "[450 | 342.46] loss=1.18 avg=1.22\n",
      "[460 | 349.95] loss=1.35 avg=1.23\n",
      "[470 | 357.44] loss=1.21 avg=1.23\n",
      "[480 | 364.93] loss=1.12 avg=1.22\n",
      "[490 | 372.40] loss=1.34 avg=1.23\n",
      "[500 | 379.87] loss=1.34 avg=1.23\n",
      "Saving checkpoint/squad_ht_5/model-500\n",
      "[510 | 389.38] loss=1.23 avg=1.23\n",
      "[520 | 396.87] loss=1.04 avg=1.22\n",
      "[530 | 404.36] loss=1.25 avg=1.22\n",
      "[540 | 411.85] loss=1.15 avg=1.22\n",
      "[550 | 419.33] loss=1.14 avg=1.22\n",
      "[560 | 426.81] loss=1.01 avg=1.22\n",
      "[570 | 434.29] loss=1.19 avg=1.22\n",
      "[580 | 441.78] loss=1.10 avg=1.21\n",
      "[590 | 449.27] loss=1.28 avg=1.21\n",
      "[600 | 456.76] loss=1.57 avg=1.22\n",
      "[610 | 464.23] loss=1.40 avg=1.23\n",
      "[620 | 471.72] loss=1.26 avg=1.23\n",
      "[630 | 479.18] loss=1.40 avg=1.23\n",
      "[640 | 486.66] loss=1.20 avg=1.23\n",
      "[650 | 494.14] loss=1.43 avg=1.23\n",
      "[660 | 501.64] loss=1.21 avg=1.23\n",
      "[670 | 509.11] loss=1.35 avg=1.24\n",
      "[680 | 516.58] loss=1.23 avg=1.24\n",
      "[690 | 524.07] loss=1.30 avg=1.24\n",
      "[700 | 531.54] loss=1.28 avg=1.24\n",
      "[710 | 539.02] loss=1.17 avg=1.24\n",
      "[720 | 546.50] loss=1.35 avg=1.24\n",
      "[730 | 553.97] loss=1.33 avg=1.24\n",
      "[740 | 561.45] loss=1.23 avg=1.24\n",
      "[750 | 568.92] loss=1.06 avg=1.24\n",
      "[760 | 576.41] loss=1.25 avg=1.24\n",
      "[770 | 583.91] loss=1.42 avg=1.24\n",
      "[780 | 591.39] loss=1.10 avg=1.24\n",
      "[790 | 598.86] loss=1.10 avg=1.24\n",
      "[800 | 606.33] loss=1.29 avg=1.24\n",
      "[810 | 613.81] loss=1.34 avg=1.24\n",
      "[820 | 621.29] loss=1.25 avg=1.24\n",
      "[830 | 628.77] loss=1.17 avg=1.24\n",
      "[840 | 636.27] loss=1.13 avg=1.24\n",
      "[850 | 643.76] loss=1.28 avg=1.24\n",
      "[860 | 651.25] loss=1.30 avg=1.24\n",
      "[870 | 658.73] loss=1.04 avg=1.23\n",
      "[880 | 666.23] loss=1.22 avg=1.23\n",
      "[890 | 673.71] loss=1.37 avg=1.24\n",
      "[900 | 681.23] loss=1.40 avg=1.24\n",
      "[910 | 688.70] loss=1.18 avg=1.24\n",
      "[920 | 696.18] loss=1.20 avg=1.24\n",
      "[930 | 703.67] loss=1.09 avg=1.23\n",
      "[940 | 711.14] loss=1.18 avg=1.23\n",
      "[950 | 718.62] loss=1.08 avg=1.23\n",
      "[960 | 726.12] loss=1.34 avg=1.23\n",
      "[970 | 733.60] loss=1.33 avg=1.23\n",
      "[980 | 741.08] loss=1.08 avg=1.23\n",
      "[990 | 748.56] loss=1.30 avg=1.23\n",
      "[1000 | 756.04] loss=1.20 avg=1.23\n",
      "Saving checkpoint/squad_ht_5/model-1000\n",
      "[1010 | 765.00] loss=1.28 avg=1.23\n",
      "[1020 | 772.49] loss=1.09 avg=1.23\n",
      "[1030 | 779.98] loss=0.96 avg=1.23\n",
      "[1040 | 787.47] loss=1.16 avg=1.23\n",
      "[1050 | 794.96] loss=1.38 avg=1.23\n",
      "[1060 | 802.45] loss=1.12 avg=1.23\n",
      "[1070 | 809.92] loss=1.33 avg=1.23\n",
      "[1080 | 817.40] loss=1.26 avg=1.23\n",
      "[1090 | 824.89] loss=1.06 avg=1.23\n",
      "[1100 | 832.37] loss=1.20 avg=1.23\n",
      "[1110 | 839.86] loss=1.15 avg=1.22\n",
      "[1120 | 847.35] loss=1.06 avg=1.22\n",
      "[1130 | 854.85] loss=1.24 avg=1.22\n",
      "[1140 | 862.34] loss=1.21 avg=1.22\n",
      "[1150 | 869.83] loss=1.00 avg=1.22\n",
      "[1160 | 877.32] loss=0.93 avg=1.21\n",
      "[1170 | 884.82] loss=1.59 avg=1.22\n",
      "[1180 | 892.31] loss=1.02 avg=1.22\n",
      "[1190 | 899.79] loss=1.38 avg=1.22\n",
      "[1200 | 907.27] loss=1.01 avg=1.22\n",
      "[1210 | 914.75] loss=1.27 avg=1.22\n",
      "[1220 | 922.24] loss=1.19 avg=1.22\n",
      "[1230 | 929.72] loss=1.10 avg=1.22\n",
      "[1240 | 937.21] loss=1.15 avg=1.21\n",
      "[1250 | 944.69] loss=1.37 avg=1.22\n",
      "[1260 | 952.17] loss=0.93 avg=1.21\n",
      "[1270 | 959.66] loss=1.21 avg=1.21\n",
      "[1280 | 967.14] loss=1.19 avg=1.21\n",
      "[1290 | 974.62] loss=1.09 avg=1.21\n",
      "[1300 | 982.09] loss=1.18 avg=1.21\n",
      "[1310 | 989.58] loss=1.11 avg=1.21\n",
      "[1320 | 997.07] loss=1.33 avg=1.21\n",
      "[1330 | 1004.56] loss=1.02 avg=1.21\n",
      "[1340 | 1012.05] loss=1.16 avg=1.21\n",
      "[1350 | 1019.54] loss=1.29 avg=1.21\n",
      "[1360 | 1027.02] loss=0.96 avg=1.21\n",
      "[1370 | 1034.53] loss=1.37 avg=1.21\n",
      "[1380 | 1042.00] loss=1.45 avg=1.21\n",
      "[1390 | 1049.49] loss=1.12 avg=1.21\n",
      "[1400 | 1056.96] loss=1.09 avg=1.21\n",
      "[1410 | 1064.45] loss=1.41 avg=1.21\n",
      "[1420 | 1071.94] loss=1.26 avg=1.21\n",
      "[1430 | 1079.41] loss=0.95 avg=1.21\n",
      "[1440 | 1086.89] loss=1.14 avg=1.21\n",
      "[1450 | 1094.38] loss=1.44 avg=1.21\n",
      "[1460 | 1101.86] loss=1.13 avg=1.21\n",
      "[1470 | 1109.34] loss=1.48 avg=1.21\n",
      "[1480 | 1116.82] loss=1.07 avg=1.21\n",
      "[1490 | 1124.31] loss=1.26 avg=1.21\n",
      "[1500 | 1131.79] loss=1.30 avg=1.21\n",
      "Saving checkpoint/squad_ht_5/model-1500\n",
      "[1510 | 1140.76] loss=1.07 avg=1.21\n",
      "[1520 | 1148.24] loss=1.06 avg=1.21\n",
      "[1530 | 1155.73] loss=1.37 avg=1.21\n",
      "[1540 | 1163.20] loss=1.21 avg=1.21\n",
      "[1550 | 1170.68] loss=1.01 avg=1.21\n",
      "[1560 | 1178.16] loss=0.91 avg=1.20\n",
      "[1570 | 1185.64] loss=1.39 avg=1.21\n",
      "[1580 | 1193.12] loss=1.16 avg=1.21\n",
      "[1590 | 1200.60] loss=1.19 avg=1.21\n",
      "[1600 | 1208.08] loss=1.29 avg=1.21\n",
      "[1610 | 1215.59] loss=1.81 avg=1.21\n",
      "[1620 | 1223.07] loss=1.42 avg=1.22\n",
      "[1630 | 1230.54] loss=1.19 avg=1.22\n",
      "[1640 | 1238.03] loss=1.04 avg=1.21\n",
      "[1650 | 1245.53] loss=1.16 avg=1.21\n",
      "[1660 | 1253.01] loss=1.32 avg=1.22\n",
      "[1670 | 1260.50] loss=1.43 avg=1.22\n",
      "[1680 | 1267.97] loss=1.23 avg=1.22\n",
      "[1690 | 1275.44] loss=1.21 avg=1.22\n",
      "[1700 | 1282.91] loss=1.38 avg=1.22\n",
      "[1710 | 1290.40] loss=1.28 avg=1.22\n",
      "[1720 | 1297.88] loss=1.50 avg=1.22\n",
      "[1730 | 1305.37] loss=1.30 avg=1.22\n",
      "[1740 | 1312.86] loss=1.07 avg=1.22\n",
      "[1750 | 1320.34] loss=1.21 avg=1.22\n",
      "[1760 | 1327.81] loss=1.18 avg=1.22\n",
      "[1770 | 1335.30] loss=1.38 avg=1.22\n",
      "[1780 | 1342.76] loss=1.16 avg=1.22\n",
      "[1790 | 1350.25] loss=1.28 avg=1.22\n",
      "[1800 | 1357.73] loss=1.16 avg=1.22\n",
      "[1810 | 1365.22] loss=1.37 avg=1.23\n",
      "[1820 | 1372.70] loss=1.26 avg=1.23\n",
      "[1830 | 1380.19] loss=1.06 avg=1.22\n",
      "[1840 | 1387.66] loss=1.26 avg=1.22\n",
      "[1850 | 1395.16] loss=1.26 avg=1.22\n",
      "[1860 | 1402.63] loss=0.95 avg=1.22\n",
      "[1870 | 1410.10] loss=1.10 avg=1.22\n",
      "[1880 | 1417.57] loss=1.16 avg=1.22\n",
      "[1890 | 1425.04] loss=1.11 avg=1.22\n",
      "[1900 | 1432.52] loss=1.03 avg=1.22\n",
      "[1910 | 1440.00] loss=1.23 avg=1.22\n",
      "[1920 | 1447.49] loss=1.10 avg=1.21\n",
      "[1930 | 1454.98] loss=1.15 avg=1.21\n",
      "[1940 | 1462.47] loss=1.31 avg=1.21\n",
      "[1950 | 1469.95] loss=0.99 avg=1.21\n",
      "[1960 | 1477.44] loss=1.27 avg=1.21\n",
      "[1970 | 1484.94] loss=1.22 avg=1.21\n",
      "[1980 | 1492.43] loss=1.11 avg=1.21\n",
      "[1990 | 1499.91] loss=1.23 avg=1.21\n",
      "[2000 | 1507.41] loss=1.13 avg=1.21\n",
      "Saving checkpoint/squad_ht_5/model-2000\n",
      "======== SAMPLE 1 ========\n",
      "]: [INFO] [STDERR] at cpw.mods.fml.relauncher.FMLModContainer.func_71344_a(FMLModContainer.java:89) [relauncher.Class:?] at net.minecraft.client.renderer.block.model.ModelBlockDefinition$MissingVariants.func_188008_h(ModelBlockDefinition.java:56) [ModelBlockDefinition.class:?] at net.minecraft.client.renderer.block.model.ModelBakery.func_177592_c(ModelBakery.java:334) [bakery.class:?] at net.minecraft.client.renderer.block.model.Model istg [0]=(?=16;)?[Squadlib][Squadlib]: Sent event FMLModStateMachine to mod SentEvent FMLModStateMachine 2013-02-10 17:44:36 [FINEST] [ForgeModLoader] [ItemTracker] Adding item net.minecraft.item.ItemBlock(2219) owned by mod net.minecraft.item.ItemBlock(2218) owned by mod rftools 2013-02-10 17:44:36 [FINEST] [ForgeModLoader] [ItemTracker] Adding item net.minecraft.item.ItemBlock(2219) owned by mod net.minecraft.item.ItemBlock(2218) owned by mod rftools 2013-02-10 17:44:36 [FINEST] [ForgeModLoader] [ItemTracker] Adding item net.minecraft.item.ItemBlock(2219) owned by mod net.minecraft.item.ItemBlock(2218) owned by mod rftools 2013-02-10 17:44:36 [FINEST] [ForgeModLoader] [ItemTracker] Adding item net.minecraft.item.ItemBlock(2219) owned by mod net.minecraft.item.ItemBlock(2218) owned by mod rftools 2013-02-10 17:44:36 [FINEST] [ForgeModLoader] [ItemTracker] Adding item net.minecraft.item.ItemBlock(2219) owned by mod net.minecraft.item.ItemBlock(2218) owned by mod rftools 2013-02-10 17:44:36 [FINEST] [ForgeModLoader] [ItemTracker] Adding item net.minecraft.item.ItemBlock(2219) owned by mod net.minecraft.item.ItemBlock(2218) owned by mod rftools 2013-02-10 17:44:36 [FINEST] [ForgeModLoader] [ItemTracker] Adding item net.minecraft.item.ItemBlock(2219) owned by mod net.minecraft.item.ItemBlock(2218) owned by mod rftools 2013-02-10 17:44:36 [FINEST] [ForgeModLoader] [ItemTracker] Adding item net.minecraft.inventory.Slot(2218) owned by mod net.minecraft.inventory.Slot[2218]: Sent event FMLModStateMachine to mod SentEvent FMLModStateMachine 2013-02-10 17:44:36 [FINEST] [notenoughkeys] Sending event FMLModStateMachine to mod notenoughkeys 2013-02-10 17:44:36 [FINEST] [NotEnoughKeys] Sent event FMLModStateMachine to mod NotEnoughKeys 2013-02-10 17:44:36 [FINEST] [NotEnoughItems] Sending event FMLModStateMachine to mod notenoughitems 2013-02-10 17:44:36 [FINEST] [NotEnoughItems] Sent event FMLModStateMachine to mod NotEnoughItems 2014-02-10 17:44:36 [FINEST] [NotEnoughItems] Sending event FMLModStateMachine to mod NotEnoughItems 2014-02-10 17:44:36 [FINEST] [NotEnoughItems] Sent event FMLModStateMachine to mod NotEnoughItems 2014-02-10 17:44:36 [FINEST] [notenoughkeys] Sending event FMLModStateMachine to mod notenoughkeypacks 2013-02-10 17:44:36 [FINEST] [NotEnoughKeys] Sent event FMLModStateMachine to mod notenoughkeys 2014-02-10 17:44:36 [FINEST] [NotEnoughItems] Sending event FMLModStateMachine to mod NOTEnoughKeys 2014-02-10 17:44:36 [FINEST] [NotEnoughItems] Sent event FMLModStateMachine to mod NotEnoughItems 2014-02-10 17:44:36 [FINEST] [notenoughkeys] Sending event FMLModStateMachine to mod notenoughkeys 2014-02-10 17:44:36 [FINEST] [NotEnoughKeys] Sent event FMLModStateMachine to mod notenoughkeys 2014-02-10 17:44:36 [FINEST] [notenoughkeys] Sending event FMLModStateMachine to mod notenoughkeys 2014-02-10 17:44:36 [FINEST] [NotEnoughKeys] Sent event FMLModStateMachine to mod\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2010 | 1530.20] loss=1.08 avg=1.21\n",
      "[2020 | 1537.68] loss=1.10 avg=1.21\n",
      "[2030 | 1545.12] loss=1.14 avg=1.21\n",
      "[2040 | 1552.57] loss=1.02 avg=1.21\n",
      "[2050 | 1560.02] loss=1.08 avg=1.20\n",
      "[2060 | 1567.49] loss=1.25 avg=1.20\n",
      "[2070 | 1574.97] loss=1.18 avg=1.20\n",
      "[2080 | 1582.43] loss=1.19 avg=1.20\n",
      "[2090 | 1589.90] loss=1.30 avg=1.20\n",
      "[2100 | 1597.36] loss=1.00 avg=1.20\n",
      "[2110 | 1604.82] loss=1.30 avg=1.20\n",
      "[2120 | 1612.29] loss=1.27 avg=1.20\n",
      "[2130 | 1619.78] loss=1.15 avg=1.20\n",
      "[2140 | 1627.25] loss=1.27 avg=1.20\n",
      "[2150 | 1634.72] loss=1.14 avg=1.20\n",
      "[2160 | 1642.19] loss=1.10 avg=1.20\n",
      "[2170 | 1649.67] loss=1.27 avg=1.20\n",
      "[2180 | 1657.15] loss=1.06 avg=1.20\n",
      "[2190 | 1664.61] loss=1.03 avg=1.20\n",
      "[2200 | 1672.08] loss=1.15 avg=1.20\n",
      "[2210 | 1679.55] loss=1.17 avg=1.20\n",
      "[2220 | 1687.02] loss=1.44 avg=1.20\n",
      "[2230 | 1694.49] loss=1.05 avg=1.20\n",
      "[2240 | 1701.96] loss=1.05 avg=1.20\n",
      "[2250 | 1709.43] loss=1.11 avg=1.20\n",
      "[2260 | 1716.91] loss=1.11 avg=1.20\n",
      "[2270 | 1724.38] loss=1.32 avg=1.20\n",
      "[2280 | 1731.85] loss=1.40 avg=1.20\n",
      "[2290 | 1739.31] loss=1.44 avg=1.20\n",
      "[2300 | 1746.78] loss=1.03 avg=1.20\n",
      "[2310 | 1754.28] loss=1.20 avg=1.20\n",
      "[2320 | 1761.75] loss=1.36 avg=1.20\n",
      "[2330 | 1769.22] loss=1.21 avg=1.20\n",
      "[2340 | 1776.68] loss=1.26 avg=1.20\n",
      "[2350 | 1784.16] loss=1.18 avg=1.20\n",
      "[2360 | 1791.64] loss=1.08 avg=1.20\n",
      "[2370 | 1799.13] loss=1.11 avg=1.20\n",
      "[2380 | 1806.61] loss=1.34 avg=1.20\n",
      "[2390 | 1814.09] loss=1.21 avg=1.20\n",
      "[2400 | 1821.58] loss=1.13 avg=1.20\n",
      "[2410 | 1829.06] loss=1.21 avg=1.20\n",
      "[2420 | 1836.54] loss=1.10 avg=1.20\n",
      "[2430 | 1844.03] loss=1.07 avg=1.20\n",
      "[2440 | 1851.52] loss=1.11 avg=1.20\n",
      "[2450 | 1859.00] loss=1.36 avg=1.20\n",
      "[2460 | 1866.49] loss=1.15 avg=1.20\n",
      "[2470 | 1873.97] loss=1.25 avg=1.20\n",
      "[2480 | 1881.47] loss=1.35 avg=1.20\n",
      "[2490 | 1888.95] loss=1.14 avg=1.20\n",
      "[2500 | 1896.44] loss=1.29 avg=1.20\n",
      "Saving checkpoint/squad_ht_5/model-2500\n",
      "[2510 | 1905.46] loss=1.29 avg=1.20\n",
      "[2520 | 1912.95] loss=1.18 avg=1.20\n",
      "[2530 | 1920.43] loss=1.01 avg=1.20\n",
      "[2540 | 1927.91] loss=1.22 avg=1.20\n",
      "[2550 | 1935.42] loss=1.12 avg=1.20\n",
      "[2560 | 1942.90] loss=1.05 avg=1.20\n",
      "[2570 | 1950.39] loss=1.09 avg=1.20\n",
      "[2580 | 1957.87] loss=1.30 avg=1.20\n",
      "[2590 | 1965.36] loss=0.95 avg=1.19\n",
      "[2600 | 1972.85] loss=1.22 avg=1.20\n",
      "[2610 | 1980.33] loss=1.36 avg=1.20\n",
      "[2620 | 1987.82] loss=1.29 avg=1.20\n",
      "[2630 | 1995.31] loss=1.33 avg=1.20\n",
      "[2640 | 2002.79] loss=1.02 avg=1.20\n",
      "[2650 | 2010.27] loss=1.20 avg=1.20\n",
      "[2660 | 2017.76] loss=1.29 avg=1.20\n",
      "[2670 | 2025.26] loss=0.98 avg=1.20\n",
      "[2680 | 2032.75] loss=0.98 avg=1.19\n",
      "[2690 | 2040.23] loss=1.12 avg=1.19\n",
      "[2700 | 2047.72] loss=1.16 avg=1.19\n",
      "[2710 | 2055.21] loss=1.41 avg=1.20\n",
      "[2720 | 2062.69] loss=0.93 avg=1.19\n",
      "[2730 | 2070.17] loss=1.51 avg=1.20\n",
      "[2740 | 2077.66] loss=1.07 avg=1.19\n",
      "[2750 | 2085.15] loss=1.58 avg=1.20\n",
      "[2760 | 2092.64] loss=1.22 avg=1.20\n",
      "[2770 | 2100.13] loss=0.93 avg=1.20\n",
      "[2780 | 2107.61] loss=0.99 avg=1.19\n",
      "[2790 | 2115.12] loss=1.12 avg=1.19\n",
      "[2800 | 2122.61] loss=1.39 avg=1.19\n",
      "[2810 | 2130.10] loss=1.02 avg=1.19\n",
      "[2820 | 2137.58] loss=0.80 avg=1.19\n",
      "[2830 | 2145.06] loss=1.29 avg=1.19\n",
      "[2840 | 2152.54] loss=1.04 avg=1.19\n",
      "[2850 | 2160.03] loss=1.40 avg=1.19\n",
      "[2860 | 2167.51] loss=1.17 avg=1.19\n",
      "[2870 | 2175.00] loss=1.08 avg=1.19\n",
      "[2880 | 2182.48] loss=1.17 avg=1.19\n",
      "[2890 | 2189.96] loss=1.11 avg=1.19\n",
      "[2900 | 2197.45] loss=1.28 avg=1.19\n",
      "[2910 | 2204.94] loss=1.16 avg=1.19\n",
      "[2920 | 2212.43] loss=1.37 avg=1.19\n",
      "[2930 | 2219.91] loss=1.33 avg=1.19\n",
      "[2940 | 2227.40] loss=1.35 avg=1.19\n",
      "[2950 | 2234.88] loss=1.30 avg=1.20\n",
      "[2960 | 2242.36] loss=1.06 avg=1.19\n",
      "[2970 | 2249.85] loss=1.00 avg=1.19\n",
      "[2980 | 2257.32] loss=1.20 avg=1.19\n",
      "[2990 | 2264.81] loss=1.13 avg=1.19\n",
      "[3000 | 2272.29] loss=0.94 avg=1.19\n",
      "Saving checkpoint/squad_ht_5/model-3000\n",
      "[3010 | 2281.27] loss=1.39 avg=1.19\n",
      "[3020 | 2288.76] loss=0.87 avg=1.19\n",
      "[3030 | 2296.28] loss=1.40 avg=1.19\n",
      "[3040 | 2303.76] loss=1.04 avg=1.19\n",
      "[3050 | 2311.25] loss=1.25 avg=1.19\n",
      "[3060 | 2318.74] loss=1.10 avg=1.19\n",
      "[3070 | 2326.22] loss=1.16 avg=1.19\n",
      "[3080 | 2333.71] loss=1.38 avg=1.19\n",
      "[3090 | 2341.19] loss=1.19 avg=1.19\n",
      "[3100 | 2348.67] loss=1.10 avg=1.19\n",
      "[3110 | 2356.13] loss=1.30 avg=1.19\n",
      "[3120 | 2363.62] loss=1.50 avg=1.19\n",
      "[3130 | 2371.10] loss=1.24 avg=1.19\n",
      "[3140 | 2378.58] loss=1.15 avg=1.19\n",
      "[3150 | 2386.07] loss=1.13 avg=1.19\n",
      "[3160 | 2393.56] loss=1.19 avg=1.19\n",
      "[3170 | 2401.05] loss=1.43 avg=1.19\n",
      "[3180 | 2408.53] loss=1.09 avg=1.19\n",
      "[3190 | 2416.01] loss=1.46 avg=1.20\n",
      "[3200 | 2423.49] loss=1.09 avg=1.20\n",
      "[3210 | 2430.96] loss=1.24 avg=1.20\n",
      "[3220 | 2438.43] loss=1.06 avg=1.19\n",
      "[3230 | 2445.92] loss=1.03 avg=1.19\n",
      "[3240 | 2453.40] loss=1.11 avg=1.19\n",
      "[3250 | 2460.89] loss=1.16 avg=1.19\n",
      "[3260 | 2468.37] loss=1.26 avg=1.19\n",
      "[3270 | 2475.87] loss=1.01 avg=1.19\n",
      "[3280 | 2483.35] loss=1.55 avg=1.19\n",
      "[3290 | 2490.83] loss=0.92 avg=1.19\n",
      "[3300 | 2498.32] loss=1.05 avg=1.19\n",
      "[3310 | 2505.81] loss=1.23 avg=1.19\n",
      "[3320 | 2513.27] loss=1.17 avg=1.19\n",
      "[3330 | 2520.76] loss=1.23 avg=1.19\n",
      "[3340 | 2528.25] loss=0.90 avg=1.19\n",
      "[3350 | 2535.74] loss=0.88 avg=1.18\n",
      "[3360 | 2543.21] loss=1.15 avg=1.18\n",
      "[3370 | 2550.69] loss=1.26 avg=1.18\n",
      "[3380 | 2558.18] loss=1.11 avg=1.18\n",
      "[3390 | 2565.65] loss=1.15 avg=1.18\n",
      "[3400 | 2573.13] loss=1.11 avg=1.18\n",
      "[3410 | 2580.60] loss=1.04 avg=1.18\n",
      "[3420 | 2588.08] loss=0.96 avg=1.18\n",
      "[3430 | 2595.57] loss=1.37 avg=1.18\n",
      "[3440 | 2603.05] loss=1.20 avg=1.18\n",
      "[3450 | 2610.55] loss=0.93 avg=1.18\n",
      "[3460 | 2618.03] loss=1.14 avg=1.18\n",
      "[3470 | 2625.51] loss=1.26 avg=1.18\n",
      "[3480 | 2632.99] loss=1.31 avg=1.18\n",
      "[3490 | 2640.48] loss=1.26 avg=1.18\n",
      "[3500 | 2647.97] loss=0.94 avg=1.18\n",
      "Saving checkpoint/squad_ht_5/model-3500\n",
      "[3510 | 2657.02] loss=1.39 avg=1.18\n",
      "[3520 | 2664.50] loss=0.99 avg=1.18\n",
      "[3530 | 2671.99] loss=0.96 avg=1.18\n",
      "[3540 | 2679.48] loss=1.36 avg=1.18\n",
      "[3550 | 2686.98] loss=1.12 avg=1.18\n",
      "[3560 | 2694.46] loss=1.50 avg=1.18\n",
      "[3570 | 2701.95] loss=1.23 avg=1.18\n",
      "[3580 | 2709.44] loss=1.19 avg=1.18\n",
      "[3590 | 2716.92] loss=1.04 avg=1.18\n",
      "[3600 | 2724.40] loss=1.51 avg=1.18\n",
      "[3610 | 2731.88] loss=1.10 avg=1.18\n",
      "[3620 | 2739.36] loss=1.26 avg=1.18\n",
      "[3630 | 2746.86] loss=1.13 avg=1.18\n",
      "[3640 | 2754.34] loss=1.12 avg=1.18\n",
      "[3650 | 2761.83] loss=1.23 avg=1.18\n",
      "[3660 | 2769.31] loss=1.44 avg=1.19\n",
      "[3670 | 2776.80] loss=1.45 avg=1.19\n",
      "[3680 | 2784.28] loss=1.19 avg=1.19\n",
      "[3690 | 2791.76] loss=1.15 avg=1.19\n",
      "[3700 | 2799.25] loss=1.37 avg=1.19\n",
      "[3710 | 2806.74] loss=1.38 avg=1.19\n",
      "[3720 | 2814.21] loss=1.13 avg=1.19\n",
      "[3730 | 2821.70] loss=1.30 avg=1.19\n",
      "[3740 | 2829.18] loss=1.24 avg=1.19\n",
      "[3750 | 2836.69] loss=1.24 avg=1.19\n",
      "[3760 | 2844.18] loss=1.12 avg=1.19\n",
      "[3770 | 2851.67] loss=1.25 avg=1.19\n",
      "[3780 | 2859.15] loss=1.11 avg=1.19\n",
      "[3790 | 2866.63] loss=1.48 avg=1.20\n",
      "[3800 | 2874.09] loss=1.13 avg=1.19\n",
      "[3810 | 2881.58] loss=1.33 avg=1.20\n",
      "[3820 | 2889.06] loss=1.11 avg=1.20\n",
      "[3830 | 2896.54] loss=1.15 avg=1.19\n",
      "[3840 | 2904.01] loss=1.09 avg=1.19\n",
      "[3850 | 2911.48] loss=1.01 avg=1.19\n",
      "[3860 | 2918.95] loss=1.01 avg=1.19\n",
      "[3870 | 2926.43] loss=1.21 avg=1.19\n",
      "[3880 | 2933.91] loss=1.17 avg=1.19\n",
      "[3890 | 2941.40] loss=1.04 avg=1.19\n",
      "[3900 | 2948.88] loss=1.23 avg=1.19\n",
      "[3910 | 2956.38] loss=1.55 avg=1.19\n",
      "[3920 | 2963.87] loss=1.20 avg=1.19\n",
      "[3930 | 2971.35] loss=1.21 avg=1.19\n",
      "[3940 | 2978.83] loss=1.42 avg=1.19\n",
      "[3950 | 2986.33] loss=1.01 avg=1.19\n",
      "[3960 | 2993.82] loss=1.03 avg=1.19\n",
      "[3970 | 3001.31] loss=1.25 avg=1.19\n",
      "[3980 | 3008.79] loss=1.07 avg=1.19\n",
      "[3990 | 3016.32] loss=1.06 avg=1.19\n",
      "[4000 | 3023.81] loss=1.29 avg=1.19\n",
      "Saving checkpoint/squad_ht_5/model-4000\n",
      "Loading checkpoint models/124M/model.ckpt\n",
      "INFO:tensorflow:Restoring parameters from models/124M/model.ckpt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [01:23<00:00, 83.76s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset has 16793911 tokens\n",
      "Training...\n",
      "[10 | 13.97] loss=1.01 avg=1.01\n",
      "[20 | 21.38] loss=0.98 avg=0.99\n",
      "[30 | 28.78] loss=1.27 avg=1.09\n",
      "[40 | 36.19] loss=1.33 avg=1.15\n",
      "[50 | 43.59] loss=1.26 avg=1.17\n",
      "[60 | 51.00] loss=1.30 avg=1.19\n",
      "[70 | 58.42] loss=1.26 avg=1.20\n",
      "[80 | 65.84] loss=1.25 avg=1.21\n",
      "[90 | 73.28] loss=1.23 avg=1.21\n",
      "[100 | 80.70] loss=1.20 avg=1.21\n",
      "[110 | 88.12] loss=1.24 avg=1.21\n",
      "[120 | 95.54] loss=1.36 avg=1.22\n",
      "[130 | 102.97] loss=1.31 avg=1.23\n",
      "[140 | 110.42] loss=1.29 avg=1.24\n",
      "[150 | 117.86] loss=1.15 avg=1.23\n",
      "[160 | 125.30] loss=1.36 avg=1.24\n",
      "[170 | 132.74] loss=1.03 avg=1.23\n",
      "[180 | 140.18] loss=1.36 avg=1.23\n",
      "[190 | 147.62] loss=1.28 avg=1.24\n",
      "[200 | 155.07] loss=1.15 avg=1.23\n",
      "[210 | 162.52] loss=1.09 avg=1.22\n",
      "[220 | 169.97] loss=1.19 avg=1.22\n",
      "[230 | 177.42] loss=1.35 avg=1.23\n",
      "[240 | 184.88] loss=1.04 avg=1.22\n",
      "[250 | 192.35] loss=1.18 avg=1.22\n",
      "[260 | 199.81] loss=1.06 avg=1.21\n",
      "[270 | 207.27] loss=1.01 avg=1.20\n",
      "[280 | 214.74] loss=1.20 avg=1.20\n",
      "[290 | 222.18] loss=1.17 avg=1.20\n",
      "[300 | 229.64] loss=0.92 avg=1.19\n",
      "[310 | 237.09] loss=0.95 avg=1.18\n",
      "[320 | 244.55] loss=1.25 avg=1.18\n",
      "[330 | 252.07] loss=1.16 avg=1.18\n",
      "[340 | 259.54] loss=1.20 avg=1.18\n",
      "[350 | 266.99] loss=1.16 avg=1.18\n",
      "[360 | 274.45] loss=1.29 avg=1.19\n",
      "[370 | 281.92] loss=1.24 avg=1.19\n",
      "[380 | 289.40] loss=1.17 avg=1.19\n",
      "[390 | 296.86] loss=1.25 avg=1.19\n",
      "[400 | 304.33] loss=1.26 avg=1.19\n",
      "[410 | 311.80] loss=0.92 avg=1.18\n",
      "[420 | 319.26] loss=1.11 avg=1.18\n",
      "[430 | 326.72] loss=1.16 avg=1.18\n",
      "[440 | 334.19] loss=1.05 avg=1.18\n",
      "[450 | 341.66] loss=1.05 avg=1.17\n",
      "[460 | 349.13] loss=1.04 avg=1.17\n",
      "[470 | 356.62] loss=1.43 avg=1.18\n",
      "[480 | 364.10] loss=1.15 avg=1.18\n",
      "[490 | 371.57] loss=1.22 avg=1.18\n",
      "[500 | 379.05] loss=1.05 avg=1.17\n",
      "Saving checkpoint/squad_ht_6/model-500\n",
      "[510 | 388.88] loss=1.21 avg=1.17\n",
      "[520 | 396.36] loss=1.09 avg=1.17\n",
      "[530 | 403.83] loss=1.22 avg=1.17\n",
      "[540 | 411.30] loss=1.33 avg=1.18\n",
      "[550 | 418.76] loss=1.01 avg=1.17\n",
      "[560 | 426.22] loss=1.11 avg=1.17\n",
      "[570 | 433.71] loss=1.30 avg=1.18\n",
      "[580 | 441.18] loss=1.01 avg=1.17\n",
      "[590 | 448.64] loss=1.13 avg=1.17\n",
      "[600 | 456.10] loss=1.20 avg=1.17\n",
      "[610 | 463.59] loss=0.98 avg=1.17\n",
      "[620 | 471.07] loss=1.35 avg=1.17\n",
      "[630 | 478.55] loss=1.13 avg=1.17\n",
      "[640 | 486.02] loss=1.17 avg=1.17\n",
      "[650 | 493.48] loss=1.10 avg=1.17\n",
      "[660 | 500.95] loss=0.78 avg=1.16\n",
      "[670 | 508.41] loss=1.17 avg=1.16\n",
      "[680 | 515.87] loss=1.07 avg=1.16\n"
     ]
    }
   ],
   "source": [
    "for idx, combination in enumerate(grid): \n",
    "    \n",
    "    if(idx>0):\n",
    "        while(os.path.isfile('results/Hypertuning/SQUAD/done.txt') != True):\n",
    "            print(\"Done file does not exist\")\n",
    "            Event().wait(2.0) \n",
    "            \n",
    "    if(os.path.isfile('results/Hypertuning/SQUAD/done.txt')):\n",
    "        os.remove('results/Hypertuning/SQUAD/done.txt')\n",
    "    \n",
    "    tf.reset_default_graph()\n",
    "\n",
    "    sess = gpt2.start_tf_sess()\n",
    "    \n",
    "    run_iter_name=\"squad_ht_\"+str(idx)\n",
    "    \n",
    "    gpt2.finetune(sess,\n",
    "                  dataset= \"data/squad_train.txt\", \n",
    "                  model_name='124M', \n",
    "                  restore_from='fresh', \n",
    "                  run_name= run_iter_name, \n",
    "                  print_every=10, \n",
    "                  sample_every=2000, \n",
    "                  save_every=500,\n",
    "                  #batch_size = combination['batch_size'],\n",
    "                  learning_rate= combination['learning_rate'],\n",
    "                  optimizer = combination['optimizer'],\n",
    "                  steps = combination['steps']\n",
    "             )\n",
    "    \n",
    "    \n",
    "    results_file = 'results/Hypertuning/SQUAD/'+run_iter_name+'.txt'\n",
    "\n",
    "    #create results file and write out the hyperparams used in this iteration\n",
    "    with open(results_file, 'w') as f:\n",
    "        f.write(\"{}\\n\".format(combination))\n",
    "    f.close()\n",
    "    \n",
    "    with open('results/Hypertuning/SQUAD/done.txt', 'w') as f:\n",
    "        f.write(\"Done\\n\".format(combination))\n",
    "    f.close()\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
